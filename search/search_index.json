{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83c\udfa5 Video Sync \u00b6 A Python tool to synchronize NSP data with camera recordings. Welcome to the official documentation for video-sync . This tool allows you to synchronize Neural Signal Processing (NSP) data with camera recordings. It processes NEV and NS5 files , aligns timestamps with camera JSON metadata , slices video based on valid frames, and synchronizes audio with video. \ud83d\udcd6 Documentation Overview \u00b6 Installation Configuration Usage Guide Features EMU Camera Serials License","title":"Home"},{"location":"#video-sync","text":"A Python tool to synchronize NSP data with camera recordings. Welcome to the official documentation for video-sync . This tool allows you to synchronize Neural Signal Processing (NSP) data with camera recordings. It processes NEV and NS5 files , aligns timestamps with camera JSON metadata , slices video based on valid frames, and synchronizes audio with video.","title":"\ud83c\udfa5 Video Sync"},{"location":"#documentation-overview","text":"Installation Configuration Usage Guide Features EMU Camera Serials License","title":"\ud83d\udcd6 Documentation Overview"},{"location":"configuration/","text":"\u2699\ufe0f Configuration \u00b6 Before running video-sync , you need to configure it using a YAML configuration file. A sample configuration file is provided: cp config.example.yaml config.yaml \ud83d\udce5 Download config.example.yaml","title":"Configuration"},{"location":"configuration/#configuration","text":"Before running video-sync , you need to configure it using a YAML configuration file. A sample configuration file is provided: cp config.example.yaml config.yaml \ud83d\udce5 Download config.example.yaml","title":"\u2699\ufe0f Configuration"},{"location":"emu-cameras/","text":"\ud83d\udcf7 EMU Camera Serials \u00b6 List of supported EMU camera serial numbers: Camera Serial Position 18486634 F1 23512908 F2 18486644 F3 18486638 B1 23512014 B2 23512906 R1 23512012 R2 23505577 R3 Use these serials when mapping cameras for video synchronization.","title":"EMU Camera Serials"},{"location":"emu-cameras/#emu-camera-serials","text":"List of supported EMU camera serial numbers: Camera Serial Position 18486634 F1 23512908 F2 18486644 F3 18486638 B1 23512014 B2 23512906 R1 23512012 R2 23505577 R3 Use these serials when mapping cameras for video synchronization.","title":"\ud83d\udcf7 EMU Camera Serials"},{"location":"features/","text":"\ud83c\udfd7\ufe0f Features \u00b6 video-sync provides the following features: \u2714\ufe0f Synchronizes NEV and NS5 files with camera recordings \u2714\ufe0f Slices video based on valid frames \u2714\ufe0f Aligns audio with video for precise synchronization \u2714\ufe0f Supports configurable processing options \u2714\ufe0f Efficient and scalable for large datasets","title":"Features"},{"location":"features/#features","text":"video-sync provides the following features: \u2714\ufe0f Synchronizes NEV and NS5 files with camera recordings \u2714\ufe0f Slices video based on valid frames \u2714\ufe0f Aligns audio with video for precise synchronization \u2714\ufe0f Supports configurable processing options \u2714\ufe0f Efficient and scalable for large datasets","title":"\ud83c\udfd7\ufe0f Features"},{"location":"installation/","text":"\ud83d\udce5 Installation \u00b6 Prerequisites \u00b6 Before installing video-sync , ensure you have the following dependencies: Conda (for environment management) FFmpeg (for video and audio processing) Installing FFmpeg on Linux \u00b6 For Ubuntu/Debian : sudo apt update sudo apt install ffmpeg For RHEL(with EPEL enabled) sudo yum install epel-release sudo yum install ffmpeg Installing video-sync \u00b6 Clone the repository and set up the environment: git clone git@github.com:BCM-Neurosurgery/video-sync.git cd video-sync Create and activate the Conda environment, then install dependencies: conda env create -f environment.yml conda activate videosync pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"\ud83d\udce5 Installation"},{"location":"installation/#prerequisites","text":"Before installing video-sync , ensure you have the following dependencies: Conda (for environment management) FFmpeg (for video and audio processing)","title":"Prerequisites"},{"location":"installation/#installing-ffmpeg-on-linux","text":"For Ubuntu/Debian : sudo apt update sudo apt install ffmpeg For RHEL(with EPEL enabled) sudo yum install epel-release sudo yum install ffmpeg","title":"Installing FFmpeg on Linux"},{"location":"installation/#installing-video-sync","text":"Clone the repository and set up the environment: git clone git@github.com:BCM-Neurosurgery/video-sync.git cd video-sync Create and activate the Conda environment, then install dependencies: conda env create -f environment.yml conda activate videosync pip install .","title":"Installing video-sync"},{"location":"license/","text":"\ud83d\udcdc License \u00b6 This project is licensed under the BSD-3-Clause License .","title":"License"},{"location":"license/#license","text":"This project is licensed under the BSD-3-Clause License .","title":"\ud83d\udcdc License"},{"location":"usage/","text":"\ud83d\ude80 Usage Guide \u00b6 Running video-sync \u00b6 Activate the Conda environment and run: conda activate videosync stitch-videos --config path/to/config.yaml Example Command stitch-videos --config config.yaml","title":"Usage"},{"location":"usage/#usage-guide","text":"","title":"\ud83d\ude80 Usage Guide"},{"location":"usage/#running-video-sync","text":"Activate the Conda environment and run: conda activate videosync stitch-videos --config path/to/config.yaml Example Command stitch-videos --config config.yaml","title":"Running video-sync"},{"location":"api/","text":"\ud83d\udee0 API Reference \u00b6 This section provides documentation for all scripts used in video-sync . DataPoool Module Nev Module Nsx Module Nsx Module Video Module Videojson Module Pathutils Module Utils Module","title":"Overview"},{"location":"api/#api-reference","text":"This section provides documentation for all scripts used in video-sync . DataPoool Module Nev Module Nsx Module Nsx Module Video Module Videojson Module Pathutils Module Utils Module","title":"\ud83d\udee0 API Reference"},{"location":"api/datapool/","text":"\ud83d\udcc4 DataPool API Documentation \u00b6 This section provides detailed documentation for pyvideosync.data_pool . \ud83d\udccc DataPool Class \u00b6 Manages NSP and video data for integrity verification and statistics. Attributes: nsp_dir ( str ) \u2013 Directory containing NSP files. cam_recording_dir ( str ) \u2013 Directory containing camera recordings. nev_pool ( NevPool ) \u2013 Stores NEV files. nsx_pool ( NsxPool ) \u2013 Stores NS5/NS3 files. video_pool ( VideoPool ) \u2013 Stores video files. video_json_pool ( VideoJsonPool ) \u2013 Stores video metadata. video_file_pool ( VideoFilesPool ) \u2013 Stores all video-related files. Source code in pyvideosync/data_pool.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class DataPool : \"\"\"Manages NSP and video data for integrity verification and statistics. Attributes: nsp_dir (str): Directory containing NSP files. cam_recording_dir (str): Directory containing camera recordings. nev_pool (NevPool): Stores NEV files. nsx_pool (NsxPool): Stores NS5/NS3 files. video_pool (VideoPool): Stores video files. video_json_pool (VideoJsonPool): Stores video metadata. video_file_pool (VideoFilesPool): Stores all video-related files. \"\"\" def __init__ ( self , nsp_dir : str , cam_recording_dir : str ) -> None : \"\"\"Initializes the DataPool class. Args: nsp_dir (str): Path to the NSP directory. cam_recording_dir (str): Path to the camera recording directory. \"\"\" self . nsp_dir = nsp_dir self . cam_recording_dir = cam_recording_dir self . nev_pool = NevPool () self . nsx_pool = NsxPool () self . video_pool = VideoPool () self . video_json_pool = VideoJsonPool () self . video_file_pool = VideoFilesPool () self . init_pools () def init_pools ( self ): \"\"\"Initializes the pools by: 1. Populating NEV and NSX pools with corresponding files. 2. Grouping the files in the video pool by timestamp. \"\"\" for file_path in Path ( self . nsp_dir ) . iterdir (): if file_path . suffix == \".nev\" : self . nev_pool . add_file ( file_path . name ) elif file_path . suffix in { \".ns5\" , \".ns3\" }: self . nsx_pool . add_file ( file_path . name ) for datefolder_path in Path ( self . cam_recording_dir ) . iterdir (): if datefolder_path . is_dir (): for file_path in datefolder_path . iterdir (): self . video_file_pool . add_file ( str ( file_path . resolve ())) def verify_integrity ( self ) -> bool : \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one of each required file. Required files: - One file matching pattern `*NSP-1.nev` - One file matching pattern `*NSP-1.ns3` - One file matching pattern `*NSP-1.ns5` - One file matching pattern `*NSP-2.nev` Returns: bool: True if exactly one of each required file is found, otherwise False. \"\"\" required_files = { \"*NSP-1.nev\" : 0 , \"*NSP-1.ns3\" : 0 , \"*NSP-1.ns5\" : 0 , \"*NSP-2.nev\" : 0 , } for file in os . listdir ( self . nsp_dir ): for pattern in required_files . keys (): if fnmatch . fnmatch ( file , pattern ): required_files [ pattern ] += 1 return all ( count == 1 for count in required_files . values ()) def get_nsp1_nev_path ( self ) -> str : \"\"\"Finds the NSP-1 NEV file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.nev\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\" def get_nsp1_ns5_path ( self ) -> str : \"\"\"Finds the NSP-1 NS5 file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.ns5\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\" def get_video_file_pool ( self ) -> \"VideoFilesPool\" : \"\"\"Retrieves the video file pool. Returns: VideoFilesPool: The video file pool object. \"\"\" return self . video_file_pool cam_recording_dir = cam_recording_dir instance-attribute \u00b6 nev_pool = NevPool () instance-attribute \u00b6 nsp_dir = nsp_dir instance-attribute \u00b6 nsx_pool = NsxPool () instance-attribute \u00b6 video_file_pool = VideoFilesPool () instance-attribute \u00b6 video_json_pool = VideoJsonPool () instance-attribute \u00b6 video_pool = VideoPool () instance-attribute \u00b6 __init__ ( nsp_dir : str , cam_recording_dir : str ) -> None \u00b6 Initializes the DataPool class. Parameters: nsp_dir ( str ) \u2013 Path to the NSP directory. cam_recording_dir ( str ) \u2013 Path to the camera recording directory. Source code in pyvideosync/data_pool.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , nsp_dir : str , cam_recording_dir : str ) -> None : \"\"\"Initializes the DataPool class. Args: nsp_dir (str): Path to the NSP directory. cam_recording_dir (str): Path to the camera recording directory. \"\"\" self . nsp_dir = nsp_dir self . cam_recording_dir = cam_recording_dir self . nev_pool = NevPool () self . nsx_pool = NsxPool () self . video_pool = VideoPool () self . video_json_pool = VideoJsonPool () self . video_file_pool = VideoFilesPool () self . init_pools () get_nsp1_nev_path () -> str \u00b6 Finds the NSP-1 NEV file path. Returns: str ( str ) \u2013 The full path of the matching file if found, otherwise an empty string. Source code in pyvideosync/data_pool.py 81 82 83 84 85 86 87 88 89 90 91 92 93 def get_nsp1_nev_path ( self ) -> str : \"\"\"Finds the NSP-1 NEV file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.nev\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\" get_nsp1_ns5_path () -> str \u00b6 Finds the NSP-1 NS5 file path. Returns: str ( str ) \u2013 The full path of the matching file if found, otherwise an empty string. Source code in pyvideosync/data_pool.py 95 96 97 98 99 100 101 102 103 104 105 106 107 def get_nsp1_ns5_path ( self ) -> str : \"\"\"Finds the NSP-1 NS5 file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.ns5\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\" get_video_file_pool () -> 'VideoFilesPool' \u00b6 Retrieves the video file pool. Returns: VideoFilesPool ( 'VideoFilesPool' ) \u2013 The video file pool object. Source code in pyvideosync/data_pool.py 109 110 111 112 113 114 115 def get_video_file_pool ( self ) -> \"VideoFilesPool\" : \"\"\"Retrieves the video file pool. Returns: VideoFilesPool: The video file pool object. \"\"\" return self . video_file_pool init_pools () \u00b6 Initializes the pools by: Populating NEV and NSX pools with corresponding files. Grouping the files in the video pool by timestamp. Source code in pyvideosync/data_pool.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def init_pools ( self ): \"\"\"Initializes the pools by: 1. Populating NEV and NSX pools with corresponding files. 2. Grouping the files in the video pool by timestamp. \"\"\" for file_path in Path ( self . nsp_dir ) . iterdir (): if file_path . suffix == \".nev\" : self . nev_pool . add_file ( file_path . name ) elif file_path . suffix in { \".ns5\" , \".ns3\" }: self . nsx_pool . add_file ( file_path . name ) for datefolder_path in Path ( self . cam_recording_dir ) . iterdir (): if datefolder_path . is_dir (): for file_path in datefolder_path . iterdir (): self . video_file_pool . add_file ( str ( file_path . resolve ())) verify_integrity () -> bool \u00b6 Verifies the integrity of the directory by ensuring it contains exactly one of each required file. Required files One file matching pattern *NSP-1.nev One file matching pattern *NSP-1.ns3 One file matching pattern *NSP-1.ns5 One file matching pattern *NSP-2.nev Returns: bool ( bool ) \u2013 True if exactly one of each required file is found, otherwise False. Source code in pyvideosync/data_pool.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def verify_integrity ( self ) -> bool : \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one of each required file. Required files: - One file matching pattern `*NSP-1.nev` - One file matching pattern `*NSP-1.ns3` - One file matching pattern `*NSP-1.ns5` - One file matching pattern `*NSP-2.nev` Returns: bool: True if exactly one of each required file is found, otherwise False. \"\"\" required_files = { \"*NSP-1.nev\" : 0 , \"*NSP-1.ns3\" : 0 , \"*NSP-1.ns5\" : 0 , \"*NSP-2.nev\" : 0 , } for file in os . listdir ( self . nsp_dir ): for pattern in required_files . keys (): if fnmatch . fnmatch ( file , pattern ): required_files [ pattern ] += 1 return all ( count == 1 for count in required_files . values ()) \ud83d\udccc Supporting Classes \u00b6 NevPool \u00b6 Stores NEV files grouped by suffix. Source code in pyvideosync/data_pool.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class NevPool : \"\"\"Stores NEV files grouped by suffix.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds a NEV file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file ) files = defaultdict ( list ) instance-attribute \u00b6 __init__ () -> None \u00b6 Source code in pyvideosync/data_pool.py 121 122 def __init__ ( self ) -> None : self . files = defaultdict ( list ) add_file ( file : str ) \u00b6 Adds a NEV file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 124 125 126 127 128 129 130 131 def add_file ( self , file : str ): \"\"\"Adds a NEV file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file ) NsxPool \u00b6 Stores NS5 and NS3 files grouped by suffix. Source code in pyvideosync/data_pool.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class NsxPool : \"\"\"Stores NS5 and NS3 files grouped by suffix.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds an NS5/NS3 file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file ) files = defaultdict ( list ) instance-attribute \u00b6 __init__ () -> None \u00b6 Source code in pyvideosync/data_pool.py 137 138 def __init__ ( self ) -> None : self . files = defaultdict ( list ) add_file ( file : str ) \u00b6 Adds an NS5/NS3 file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 140 141 142 143 144 145 146 147 def add_file ( self , file : str ): \"\"\"Adds an NS5/NS3 file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file ) VideoPool \u00b6 Stores video files grouped by timestamp. Source code in pyvideosync/data_pool.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class VideoPool : \"\"\"Stores video files grouped by timestamp.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds a video file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file ) files = defaultdict ( list ) instance-attribute \u00b6 __init__ () -> None \u00b6 Source code in pyvideosync/data_pool.py 153 154 def __init__ ( self ) -> None : self . files = defaultdict ( list ) add_file ( file : str ) \u00b6 Adds a video file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 156 157 158 159 160 161 162 163 def add_file ( self , file : str ): \"\"\"Adds a video file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file ) VideoJsonPool \u00b6 Stores video metadata JSON files grouped by timestamp. Source code in pyvideosync/data_pool.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class VideoJsonPool : \"\"\"Stores video metadata JSON files grouped by timestamp.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file ) def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists all groups of video metadata files. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : files for timestamp , files in self . files . items ()} files = defaultdict ( list ) instance-attribute \u00b6 __init__ () -> None \u00b6 Source code in pyvideosync/data_pool.py 169 170 def __init__ ( self ) -> None : self . files = defaultdict ( list ) add_file ( file : str ) \u00b6 Source code in pyvideosync/data_pool.py 172 173 174 def add_file ( self , file : str ): timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file ) list_groups () -> dict [ str , list [ str ]] \u00b6 Lists all groups of video metadata files. Returns: dict [ str , list [ str ]] \u2013 dict[str, list[str]]: A dictionary where keys are timestamps (str) dict [ str , list [ str ]] \u2013 and values are lists of file names (str). Source code in pyvideosync/data_pool.py 176 177 178 179 180 181 182 183 def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists all groups of video metadata files. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : files for timestamp , files in self . files . items ()} VideoFilesPool \u00b6 Stores all video-related files grouped by timestamp. Source code in pyvideosync/data_pool.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class VideoFilesPool : \"\"\"Stores all video-related files grouped by timestamp.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds a video-related file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = extract_timestamp ( file ) self . files [ timestamp ] . append ( file ) def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists groups of files sorted by timestamp. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : self . files [ timestamp ] for timestamp in sorted ( self . files )} def find_one_random_json ( self ) -> str | None : \"\"\"Finds a random JSON file in the pool. Returns: str: A JSON file name if found, otherwise None. \"\"\" for files in self . files . values (): for file in files : if file . endswith ( \".json\" ): return file return None def get_unique_cam_serials ( self ) -> set [ str ]: \"\"\" Returns a set of all unique camera serial numbers found in the filenames. Returns: set[str]: A set of unique camera serial numbers. \"\"\" serials = set () for files in self . files . values (): for file in files : if file . endswith ( \".mp4\" ): serial = extract_cam_serial ( file ) if serial : serials . add ( serial ) return serials files = defaultdict ( list ) instance-attribute \u00b6 __init__ () -> None \u00b6 Source code in pyvideosync/data_pool.py 189 190 def __init__ ( self ) -> None : self . files = defaultdict ( list ) add_file ( file : str ) \u00b6 Adds a video-related file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 192 193 194 195 196 197 198 199 def add_file ( self , file : str ): \"\"\"Adds a video-related file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = extract_timestamp ( file ) self . files [ timestamp ] . append ( file ) find_one_random_json () -> str | None \u00b6 Finds a random JSON file in the pool. Returns: str ( str | None ) \u2013 A JSON file name if found, otherwise None. Source code in pyvideosync/data_pool.py 210 211 212 213 214 215 216 217 218 219 220 def find_one_random_json ( self ) -> str | None : \"\"\"Finds a random JSON file in the pool. Returns: str: A JSON file name if found, otherwise None. \"\"\" for files in self . files . values (): for file in files : if file . endswith ( \".json\" ): return file return None get_unique_cam_serials () -> set [ str ] \u00b6 Returns a set of all unique camera serial numbers found in the filenames. Returns: set [ str ] \u2013 set[str]: A set of unique camera serial numbers. Source code in pyvideosync/data_pool.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def get_unique_cam_serials ( self ) -> set [ str ]: \"\"\" Returns a set of all unique camera serial numbers found in the filenames. Returns: set[str]: A set of unique camera serial numbers. \"\"\" serials = set () for files in self . files . values (): for file in files : if file . endswith ( \".mp4\" ): serial = extract_cam_serial ( file ) if serial : serials . add ( serial ) return serials list_groups () -> dict [ str , list [ str ]] \u00b6 Lists groups of files sorted by timestamp. Returns: dict [ str , list [ str ]] \u2013 dict[str, list[str]]: A dictionary where keys are timestamps (str) dict [ str , list [ str ]] \u2013 and values are lists of file names (str). Source code in pyvideosync/data_pool.py 201 202 203 204 205 206 207 208 def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists groups of files sorted by timestamp. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : self . files [ timestamp ] for timestamp in sorted ( self . files )}","title":"DataPool"},{"location":"api/datapool/#datapool-api-documentation","text":"This section provides detailed documentation for pyvideosync.data_pool .","title":"\ud83d\udcc4 DataPool API Documentation"},{"location":"api/datapool/#datapool-class","text":"Manages NSP and video data for integrity verification and statistics. Attributes: nsp_dir ( str ) \u2013 Directory containing NSP files. cam_recording_dir ( str ) \u2013 Directory containing camera recordings. nev_pool ( NevPool ) \u2013 Stores NEV files. nsx_pool ( NsxPool ) \u2013 Stores NS5/NS3 files. video_pool ( VideoPool ) \u2013 Stores video files. video_json_pool ( VideoJsonPool ) \u2013 Stores video metadata. video_file_pool ( VideoFilesPool ) \u2013 Stores all video-related files. Source code in pyvideosync/data_pool.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 class DataPool : \"\"\"Manages NSP and video data for integrity verification and statistics. Attributes: nsp_dir (str): Directory containing NSP files. cam_recording_dir (str): Directory containing camera recordings. nev_pool (NevPool): Stores NEV files. nsx_pool (NsxPool): Stores NS5/NS3 files. video_pool (VideoPool): Stores video files. video_json_pool (VideoJsonPool): Stores video metadata. video_file_pool (VideoFilesPool): Stores all video-related files. \"\"\" def __init__ ( self , nsp_dir : str , cam_recording_dir : str ) -> None : \"\"\"Initializes the DataPool class. Args: nsp_dir (str): Path to the NSP directory. cam_recording_dir (str): Path to the camera recording directory. \"\"\" self . nsp_dir = nsp_dir self . cam_recording_dir = cam_recording_dir self . nev_pool = NevPool () self . nsx_pool = NsxPool () self . video_pool = VideoPool () self . video_json_pool = VideoJsonPool () self . video_file_pool = VideoFilesPool () self . init_pools () def init_pools ( self ): \"\"\"Initializes the pools by: 1. Populating NEV and NSX pools with corresponding files. 2. Grouping the files in the video pool by timestamp. \"\"\" for file_path in Path ( self . nsp_dir ) . iterdir (): if file_path . suffix == \".nev\" : self . nev_pool . add_file ( file_path . name ) elif file_path . suffix in { \".ns5\" , \".ns3\" }: self . nsx_pool . add_file ( file_path . name ) for datefolder_path in Path ( self . cam_recording_dir ) . iterdir (): if datefolder_path . is_dir (): for file_path in datefolder_path . iterdir (): self . video_file_pool . add_file ( str ( file_path . resolve ())) def verify_integrity ( self ) -> bool : \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one of each required file. Required files: - One file matching pattern `*NSP-1.nev` - One file matching pattern `*NSP-1.ns3` - One file matching pattern `*NSP-1.ns5` - One file matching pattern `*NSP-2.nev` Returns: bool: True if exactly one of each required file is found, otherwise False. \"\"\" required_files = { \"*NSP-1.nev\" : 0 , \"*NSP-1.ns3\" : 0 , \"*NSP-1.ns5\" : 0 , \"*NSP-2.nev\" : 0 , } for file in os . listdir ( self . nsp_dir ): for pattern in required_files . keys (): if fnmatch . fnmatch ( file , pattern ): required_files [ pattern ] += 1 return all ( count == 1 for count in required_files . values ()) def get_nsp1_nev_path ( self ) -> str : \"\"\"Finds the NSP-1 NEV file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.nev\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\" def get_nsp1_ns5_path ( self ) -> str : \"\"\"Finds the NSP-1 NS5 file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.ns5\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\" def get_video_file_pool ( self ) -> \"VideoFilesPool\" : \"\"\"Retrieves the video file pool. Returns: VideoFilesPool: The video file pool object. \"\"\" return self . video_file_pool","title":"\ud83d\udccc DataPool Class"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.cam_recording_dir","text":"","title":"cam_recording_dir"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nev_pool","text":"","title":"nev_pool"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nsp_dir","text":"","title":"nsp_dir"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nsx_pool","text":"","title":"nsx_pool"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_file_pool","text":"","title":"video_file_pool"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_json_pool","text":"","title":"video_json_pool"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_pool","text":"","title":"video_pool"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.__init__","text":"Initializes the DataPool class. Parameters: nsp_dir ( str ) \u2013 Path to the NSP directory. cam_recording_dir ( str ) \u2013 Path to the camera recording directory. Source code in pyvideosync/data_pool.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , nsp_dir : str , cam_recording_dir : str ) -> None : \"\"\"Initializes the DataPool class. Args: nsp_dir (str): Path to the NSP directory. cam_recording_dir (str): Path to the camera recording directory. \"\"\" self . nsp_dir = nsp_dir self . cam_recording_dir = cam_recording_dir self . nev_pool = NevPool () self . nsx_pool = NsxPool () self . video_pool = VideoPool () self . video_json_pool = VideoJsonPool () self . video_file_pool = VideoFilesPool () self . init_pools ()","title":"__init__"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_nsp1_nev_path","text":"Finds the NSP-1 NEV file path. Returns: str ( str ) \u2013 The full path of the matching file if found, otherwise an empty string. Source code in pyvideosync/data_pool.py 81 82 83 84 85 86 87 88 89 90 91 92 93 def get_nsp1_nev_path ( self ) -> str : \"\"\"Finds the NSP-1 NEV file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.nev\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\"","title":"get_nsp1_nev_path"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_nsp1_ns5_path","text":"Finds the NSP-1 NS5 file path. Returns: str ( str ) \u2013 The full path of the matching file if found, otherwise an empty string. Source code in pyvideosync/data_pool.py 95 96 97 98 99 100 101 102 103 104 105 106 107 def get_nsp1_ns5_path ( self ) -> str : \"\"\"Finds the NSP-1 NS5 file path. Returns: str: The full path of the matching file if found, otherwise an empty string. \"\"\" pattern = \"*NSP-1.ns5\" for file in os . listdir ( self . nsp_dir ): if fnmatch . fnmatch ( file , pattern ): return os . path . join ( self . nsp_dir , file ) return \"\"","title":"get_nsp1_ns5_path"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_video_file_pool","text":"Retrieves the video file pool. Returns: VideoFilesPool ( 'VideoFilesPool' ) \u2013 The video file pool object. Source code in pyvideosync/data_pool.py 109 110 111 112 113 114 115 def get_video_file_pool ( self ) -> \"VideoFilesPool\" : \"\"\"Retrieves the video file pool. Returns: VideoFilesPool: The video file pool object. \"\"\" return self . video_file_pool","title":"get_video_file_pool"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.init_pools","text":"Initializes the pools by: Populating NEV and NSX pools with corresponding files. Grouping the files in the video pool by timestamp. Source code in pyvideosync/data_pool.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def init_pools ( self ): \"\"\"Initializes the pools by: 1. Populating NEV and NSX pools with corresponding files. 2. Grouping the files in the video pool by timestamp. \"\"\" for file_path in Path ( self . nsp_dir ) . iterdir (): if file_path . suffix == \".nev\" : self . nev_pool . add_file ( file_path . name ) elif file_path . suffix in { \".ns5\" , \".ns3\" }: self . nsx_pool . add_file ( file_path . name ) for datefolder_path in Path ( self . cam_recording_dir ) . iterdir (): if datefolder_path . is_dir (): for file_path in datefolder_path . iterdir (): self . video_file_pool . add_file ( str ( file_path . resolve ()))","title":"init_pools"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.verify_integrity","text":"Verifies the integrity of the directory by ensuring it contains exactly one of each required file. Required files One file matching pattern *NSP-1.nev One file matching pattern *NSP-1.ns3 One file matching pattern *NSP-1.ns5 One file matching pattern *NSP-2.nev Returns: bool ( bool ) \u2013 True if exactly one of each required file is found, otherwise False. Source code in pyvideosync/data_pool.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def verify_integrity ( self ) -> bool : \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one of each required file. Required files: - One file matching pattern `*NSP-1.nev` - One file matching pattern `*NSP-1.ns3` - One file matching pattern `*NSP-1.ns5` - One file matching pattern `*NSP-2.nev` Returns: bool: True if exactly one of each required file is found, otherwise False. \"\"\" required_files = { \"*NSP-1.nev\" : 0 , \"*NSP-1.ns3\" : 0 , \"*NSP-1.ns5\" : 0 , \"*NSP-2.nev\" : 0 , } for file in os . listdir ( self . nsp_dir ): for pattern in required_files . keys (): if fnmatch . fnmatch ( file , pattern ): required_files [ pattern ] += 1 return all ( count == 1 for count in required_files . values ())","title":"verify_integrity"},{"location":"api/datapool/#supporting-classes","text":"","title":"\ud83d\udccc Supporting Classes"},{"location":"api/datapool/#nevpool","text":"Stores NEV files grouped by suffix. Source code in pyvideosync/data_pool.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 class NevPool : \"\"\"Stores NEV files grouped by suffix.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds a NEV file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file )","title":"NevPool"},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.files","text":"","title":"files"},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.__init__","text":"Source code in pyvideosync/data_pool.py 121 122 def __init__ ( self ) -> None : self . files = defaultdict ( list )","title":"__init__"},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.add_file","text":"Adds a NEV file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 124 125 126 127 128 129 130 131 def add_file ( self , file : str ): \"\"\"Adds a NEV file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file )","title":"add_file"},{"location":"api/datapool/#nsxpool","text":"Stores NS5 and NS3 files grouped by suffix. Source code in pyvideosync/data_pool.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class NsxPool : \"\"\"Stores NS5 and NS3 files grouped by suffix.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds an NS5/NS3 file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file )","title":"NsxPool"},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.files","text":"","title":"files"},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.__init__","text":"Source code in pyvideosync/data_pool.py 137 138 def __init__ ( self ) -> None : self . files = defaultdict ( list )","title":"__init__"},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.add_file","text":"Adds an NS5/NS3 file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 140 141 142 143 144 145 146 147 def add_file ( self , file : str ): \"\"\"Adds an NS5/NS3 file to the pool. Args: file (str): File name to be added. \"\"\" suffix = file . split ( \"-\" )[ - 1 ] self . files [ suffix ] . append ( file )","title":"add_file"},{"location":"api/datapool/#videopool","text":"Stores video files grouped by timestamp. Source code in pyvideosync/data_pool.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 class VideoPool : \"\"\"Stores video files grouped by timestamp.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds a video file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file )","title":"VideoPool"},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.files","text":"","title":"files"},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.__init__","text":"Source code in pyvideosync/data_pool.py 153 154 def __init__ ( self ) -> None : self . files = defaultdict ( list )","title":"__init__"},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.add_file","text":"Adds a video file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 156 157 158 159 160 161 162 163 def add_file ( self , file : str ): \"\"\"Adds a video file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file )","title":"add_file"},{"location":"api/datapool/#videojsonpool","text":"Stores video metadata JSON files grouped by timestamp. Source code in pyvideosync/data_pool.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 class VideoJsonPool : \"\"\"Stores video metadata JSON files grouped by timestamp.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file ) def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists all groups of video metadata files. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : files for timestamp , files in self . files . items ()}","title":"VideoJsonPool"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.files","text":"","title":"files"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.__init__","text":"Source code in pyvideosync/data_pool.py 169 170 def __init__ ( self ) -> None : self . files = defaultdict ( list )","title":"__init__"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.add_file","text":"Source code in pyvideosync/data_pool.py 172 173 174 def add_file ( self , file : str ): timestamp = file . split ( \"_\" )[ - 1 ] . split ( \".\" )[ 0 ] self . files [ timestamp ] . append ( file )","title":"add_file"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.list_groups","text":"Lists all groups of video metadata files. Returns: dict [ str , list [ str ]] \u2013 dict[str, list[str]]: A dictionary where keys are timestamps (str) dict [ str , list [ str ]] \u2013 and values are lists of file names (str). Source code in pyvideosync/data_pool.py 176 177 178 179 180 181 182 183 def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists all groups of video metadata files. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : files for timestamp , files in self . files . items ()}","title":"list_groups"},{"location":"api/datapool/#videofilespool","text":"Stores all video-related files grouped by timestamp. Source code in pyvideosync/data_pool.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 class VideoFilesPool : \"\"\"Stores all video-related files grouped by timestamp.\"\"\" def __init__ ( self ) -> None : self . files = defaultdict ( list ) def add_file ( self , file : str ): \"\"\"Adds a video-related file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = extract_timestamp ( file ) self . files [ timestamp ] . append ( file ) def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists groups of files sorted by timestamp. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : self . files [ timestamp ] for timestamp in sorted ( self . files )} def find_one_random_json ( self ) -> str | None : \"\"\"Finds a random JSON file in the pool. Returns: str: A JSON file name if found, otherwise None. \"\"\" for files in self . files . values (): for file in files : if file . endswith ( \".json\" ): return file return None def get_unique_cam_serials ( self ) -> set [ str ]: \"\"\" Returns a set of all unique camera serial numbers found in the filenames. Returns: set[str]: A set of unique camera serial numbers. \"\"\" serials = set () for files in self . files . values (): for file in files : if file . endswith ( \".mp4\" ): serial = extract_cam_serial ( file ) if serial : serials . add ( serial ) return serials","title":"VideoFilesPool"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.files","text":"","title":"files"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.__init__","text":"Source code in pyvideosync/data_pool.py 189 190 def __init__ ( self ) -> None : self . files = defaultdict ( list )","title":"__init__"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.add_file","text":"Adds a video-related file to the pool. Parameters: file ( str ) \u2013 File name to be added. Source code in pyvideosync/data_pool.py 192 193 194 195 196 197 198 199 def add_file ( self , file : str ): \"\"\"Adds a video-related file to the pool. Args: file (str): File name to be added. \"\"\" timestamp = extract_timestamp ( file ) self . files [ timestamp ] . append ( file )","title":"add_file"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.find_one_random_json","text":"Finds a random JSON file in the pool. Returns: str ( str | None ) \u2013 A JSON file name if found, otherwise None. Source code in pyvideosync/data_pool.py 210 211 212 213 214 215 216 217 218 219 220 def find_one_random_json ( self ) -> str | None : \"\"\"Finds a random JSON file in the pool. Returns: str: A JSON file name if found, otherwise None. \"\"\" for files in self . files . values (): for file in files : if file . endswith ( \".json\" ): return file return None","title":"find_one_random_json"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.get_unique_cam_serials","text":"Returns a set of all unique camera serial numbers found in the filenames. Returns: set [ str ] \u2013 set[str]: A set of unique camera serial numbers. Source code in pyvideosync/data_pool.py 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def get_unique_cam_serials ( self ) -> set [ str ]: \"\"\" Returns a set of all unique camera serial numbers found in the filenames. Returns: set[str]: A set of unique camera serial numbers. \"\"\" serials = set () for files in self . files . values (): for file in files : if file . endswith ( \".mp4\" ): serial = extract_cam_serial ( file ) if serial : serials . add ( serial ) return serials","title":"get_unique_cam_serials"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.list_groups","text":"Lists groups of files sorted by timestamp. Returns: dict [ str , list [ str ]] \u2013 dict[str, list[str]]: A dictionary where keys are timestamps (str) dict [ str , list [ str ]] \u2013 and values are lists of file names (str). Source code in pyvideosync/data_pool.py 201 202 203 204 205 206 207 208 def list_groups ( self ) -> dict [ str , list [ str ]]: \"\"\"Lists groups of files sorted by timestamp. Returns: dict[str, list[str]]: A dictionary where keys are timestamps (str) and values are lists of file names (str). \"\"\" return { timestamp : self . files [ timestamp ] for timestamp in sorted ( self . files )}","title":"list_groups"},{"location":"api/nev/","text":"","title":"Nev"},{"location":"api/nsx/","text":"","title":"Nsx"},{"location":"api/pathutils/","text":"","title":"Pathutils"},{"location":"api/utils/","text":"","title":"Utils"},{"location":"api/video/","text":"","title":"Video"},{"location":"api/videojson/","text":"","title":"Videojson"},{"location":"developer-guide/","text":"\ud83d\udee0\ufe0f Developer Guide \u00b6 Welcome to the Developer Guide for video-sync . This section provides a deep dive into the internal workings of the project, covering: Program Flow : How the tool processes and synchronizes data. Edge Cases : Real-world scenarios and solutions. Contributing : How developers can contribute to the project.","title":"Overview"},{"location":"developer-guide/#developer-guide","text":"Welcome to the Developer Guide for video-sync . This section provides a deep dive into the internal workings of the project, covering: Program Flow : How the tool processes and synchronizes data. Edge Cases : Real-world scenarios and solutions. Contributing : How developers can contribute to the project.","title":"\ud83d\udee0\ufe0f Developer Guide"},{"location":"developer-guide/contributing/","text":"\ud83e\udd1d Contributing Guide \u00b6 Want to contribute? Follow these steps:","title":"Contributing"},{"location":"developer-guide/contributing/#contributing-guide","text":"Want to contribute? Follow these steps:","title":"\ud83e\udd1d Contributing Guide"},{"location":"developer-guide/edge-cases/","text":"Edge Cases & Real Examples \u00b6 This section highlights real-world edge cases encountered when synchronizing video and neural data. Each example provides a problem description, a sample JSON snippet, analysis, and a possible solution. 1. Frame Counter Anomalies \u00b6 The Problem \u00b6 In some cases, the JSON metadata file may show frame counters that jump unexpectedly. However, the total frame count remains consistent across multiple verification methods, suggesting that frames are not actually lost. Example JSON (Incorrect Frame Sequence) \u00b6 { \"frames\" : [ [ 19393 , 19393 , 19392 , 19393 , 19392 ], [ 19400 , 19400 , 19399 , 19400 , 19399 ] ] } In this case, the frame IDs jump from 19392 to 19399 , which may indicate a counter issue. Observations \u00b6 The JSON metadata suggests that frames skip from 19392 \u2192 19399. However, when checking the total number of frames in the JSON: >>> len ( yfb_json [ \"frame_id\" ]) 18000 it still returns 18000 frames, meaning that the frames exist but their numbering is inconsistent. Similarly, probing the corresponding MP4 video file also returns 18000 frames, further confirming that no frames are missing. Conclusion \u00b6 This issue is not a missing frame problem, but rather a frame counter anomaly. Download Reference JSON File \u00b6 You can download the example JSON file here: YFB_20240505_133351.json 2. Chunk Serial Discontinuities in JSON \u00b6 The Problem \u00b6 When analyzing chunk serial data in the JSON files, discontinuities can occur in different forms. These discontinuities can affect downstream processing, requiring careful identification and handling. The types of discontinuities observed in the dataset are: Type I Discontinuity: The number drops to zero and then increases to a value greater than 1. Type II Discontinuity: The number resets from zero to 1. Type III Discontinuity: The difference between consecutive numbers is greater than 1. Type IV Discontinuity: The number hits -1. These discontinuities can impact the integrity of data streams, requiring careful detection and mitigation strategies. Example JSON \u00b6 Type I discontinuity [ 20323583 , 20323583 , 20323583 , 20323583 , 20323583 ], [ 0 , 0 , 0 , 0 , 0 ], [ 20323585 , 20323585 , 20323585 , 20323585 , 20323585 ], Type II discontinuity [ 20332543 , 20332543 , 20332543 , 20332543 , 20332543 ], [ 0 , 0 , 0 , 0 , 0 ], [ 1 , 1 , 1 , 1 , 1 ], ... [ 127 , 127 , 127 , 127 , 127 ], [ 0 , 0 , 0 , 0 , 0 ], [ 20332673 , 20332673 , 20332673 , 20332673 , 20332673 ], Observation \u00b6 Type I discontinuities dominate the dataset, with a structured gap pattern of 128 occurring repeatedly. Type II discontinuities are rare but show a similar gap pattern to Type I. The chunk serial usually drops to 0, and gradually increases to 127, before dropping to 0 again and resuming from where it left off as if all those data were not missing. Type III and Type IV discontinuities are absent, suggesting no extreme jumps or invalid serial numbers. Conclusion \u00b6 The analysis highlights a strong pattern of Type I discontinuities, suggesting a structured reset mechanism rather than random errors. The gaps of 128 could indicate an intentional segmentation in the data stream, possibly due to a recording or transmission mechanism. Extra attention is required when handling chunk serial stream in JSON files. Download Example Data \u00b6 2024-07-19_09:10:54_chunk_discontinuities.json YFC_20240719_091054.json Jupyter Notebook to Reproduce Results","title":"Edge Cases"},{"location":"developer-guide/edge-cases/#edge-cases-real-examples","text":"This section highlights real-world edge cases encountered when synchronizing video and neural data. Each example provides a problem description, a sample JSON snippet, analysis, and a possible solution.","title":"Edge Cases &amp; Real Examples"},{"location":"developer-guide/edge-cases/#1-frame-counter-anomalies","text":"","title":"1. Frame Counter Anomalies"},{"location":"developer-guide/edge-cases/#the-problem","text":"In some cases, the JSON metadata file may show frame counters that jump unexpectedly. However, the total frame count remains consistent across multiple verification methods, suggesting that frames are not actually lost.","title":"The Problem"},{"location":"developer-guide/edge-cases/#example-json-incorrect-frame-sequence","text":"{ \"frames\" : [ [ 19393 , 19393 , 19392 , 19393 , 19392 ], [ 19400 , 19400 , 19399 , 19400 , 19399 ] ] } In this case, the frame IDs jump from 19392 to 19399 , which may indicate a counter issue.","title":"Example JSON (Incorrect Frame Sequence)"},{"location":"developer-guide/edge-cases/#observations","text":"The JSON metadata suggests that frames skip from 19392 \u2192 19399. However, when checking the total number of frames in the JSON: >>> len ( yfb_json [ \"frame_id\" ]) 18000 it still returns 18000 frames, meaning that the frames exist but their numbering is inconsistent. Similarly, probing the corresponding MP4 video file also returns 18000 frames, further confirming that no frames are missing.","title":"Observations"},{"location":"developer-guide/edge-cases/#conclusion","text":"This issue is not a missing frame problem, but rather a frame counter anomaly.","title":"Conclusion"},{"location":"developer-guide/edge-cases/#download-reference-json-file","text":"You can download the example JSON file here: YFB_20240505_133351.json","title":"Download Reference JSON File"},{"location":"developer-guide/edge-cases/#2-chunk-serial-discontinuities-in-json","text":"","title":"2. Chunk Serial Discontinuities in JSON"},{"location":"developer-guide/edge-cases/#the-problem_1","text":"When analyzing chunk serial data in the JSON files, discontinuities can occur in different forms. These discontinuities can affect downstream processing, requiring careful identification and handling. The types of discontinuities observed in the dataset are: Type I Discontinuity: The number drops to zero and then increases to a value greater than 1. Type II Discontinuity: The number resets from zero to 1. Type III Discontinuity: The difference between consecutive numbers is greater than 1. Type IV Discontinuity: The number hits -1. These discontinuities can impact the integrity of data streams, requiring careful detection and mitigation strategies.","title":"The Problem"},{"location":"developer-guide/edge-cases/#example-json","text":"Type I discontinuity [ 20323583 , 20323583 , 20323583 , 20323583 , 20323583 ], [ 0 , 0 , 0 , 0 , 0 ], [ 20323585 , 20323585 , 20323585 , 20323585 , 20323585 ], Type II discontinuity [ 20332543 , 20332543 , 20332543 , 20332543 , 20332543 ], [ 0 , 0 , 0 , 0 , 0 ], [ 1 , 1 , 1 , 1 , 1 ], ... [ 127 , 127 , 127 , 127 , 127 ], [ 0 , 0 , 0 , 0 , 0 ], [ 20332673 , 20332673 , 20332673 , 20332673 , 20332673 ],","title":"Example JSON"},{"location":"developer-guide/edge-cases/#observation","text":"Type I discontinuities dominate the dataset, with a structured gap pattern of 128 occurring repeatedly. Type II discontinuities are rare but show a similar gap pattern to Type I. The chunk serial usually drops to 0, and gradually increases to 127, before dropping to 0 again and resuming from where it left off as if all those data were not missing. Type III and Type IV discontinuities are absent, suggesting no extreme jumps or invalid serial numbers.","title":"Observation"},{"location":"developer-guide/edge-cases/#conclusion_1","text":"The analysis highlights a strong pattern of Type I discontinuities, suggesting a structured reset mechanism rather than random errors. The gaps of 128 could indicate an intentional segmentation in the data stream, possibly due to a recording or transmission mechanism. Extra attention is required when handling chunk serial stream in JSON files.","title":"Conclusion"},{"location":"developer-guide/edge-cases/#download-example-data","text":"2024-07-19_09:10:54_chunk_discontinuities.json YFC_20240719_091054.json Jupyter Notebook to Reproduce Results","title":"Download Example Data"},{"location":"developer-guide/program-flow/","text":"\ud83d\udd04 Program Flow \u00b6 This section explains the data processing flow of video-sync . 1. Configuration \u00b6 In main.py, we load and validate the configuration using the PathUtils class. timestamp = get_current_ts () pathutils = PathUtils ( config_path , timestamp ) logger = configure_logging ( pathutils . output_dir ) if not pathutils . is_config_valid (): logger . error ( \"Config not valid, exiting to inital screen...\" ) return This ensures the configuration is valid before proceeding with synchronization. 2. Data Integrity Check \u00b6 The DataPool class ensures all required neural and camera files are present. If files are missing, the process stops. datapool = DataPool ( pathutils . nsp_dir , pathutils . cam_recording_dir ) if not datapool . verify_integrity (): logger . error ( \"File integrity check failed: Missing or duplicate NSP files detected. \" \"Please verify the directory structure and try again. Returning to the initial screen.\" ) return 3. Extracting Neural Data (NEV) \u00b6 This step extracts and reconstructs chunk serial data in the format of a dataframe from the stitched NSP-1 .nev file, which contains event-based neural data. The chunk serial values are reconstructed by combining five split chunks of serial communication sent from an Arduino. # 1. Get NEV serial start and end nsp1_nev_path = datapool . get_nsp1_nev_path () nev = Nev ( nsp1_nev_path ) nev_chunk_serial_df = nev . get_chunk_serial_df () logger . info ( f \"NEV dataframe \\n : { nev_chunk_serial_df } \" ) nev_start_serial , nev_end_serial = get_column_min_max ( nev_chunk_serial_df , \"chunk_serial\" ) logger . info ( f \"Start serial: { nev_start_serial } , End serial: { nev_end_serial } \" ) The script first retrieves the NSP-1 .nev file path and initializes an instance of the Nev class to parse its contents. It then calls get_chunk_serial_df() , which reconstructs the chunk serials by combining the five split parts. This reconstructed DataFrame provides a sequential timeline of neural events, essential for aligning with video data. To establish the valid time range for synchronization, the script determines the earliest and latest chunk serial values from the NEV data using get_column_min_max() . These values define the window in which video frames should be extracted later to ensure proper alignment. 4. Identifying Relevant Video and Metadata Files for Synchronization \u00b6 To ensure proper alignment between neural and video data, the script identifies which camera recordings overlap with the neural event (NEV) time range. Each video recording has an associated JSON metadata file containing start and end chunk serials, which are extracted and compared against the NEV serial range. To optimize performance, the script first checks if previously computed timestamps exist in timestamps.json . If found, these timestamps are used directly to skip redundant processing. Otherwise, the script iterates through all available JSON metadata files, extracting their chunk serials and determining whether they overlap with the neural recording. If a video\u2019s serial range falls within the NEV range, its timestamp is added to the processing list. Once all relevant timestamps are identified, they are saved to timestamps.json for future runs and sorted to maintain chronological order. This approach ensures that only the necessary video files are processed, reducing computational overhead while maintaining precise synchronization. # 3. load camera serials from the config file camera_serials = pathutils . cam_serial logger . info ( f \"Camera serials loaded from config: { camera_serials } \" ) # 4. Go through all JSON files and find the ones that # are within the NEV serial range # read timestamps if available timestamps_path = os . path . join ( pathutils . output_dir , \"timestamps.json\" ) timestamps = load_timestamps ( timestamps_path , logger ) if timestamps : logger . info ( f \"Loaded timestamps: { timestamps } \" ) else : logger . info ( \"No timestamps found\" ) timestamps = [] for timestamp , camera_file_group in camera_files . items (): json_path = get_json_file ( camera_file_group , pathutils ) if json_path is None : logger . error ( f \"No JSON file found in group { timestamp } \" ) continue videojson = Videojson ( json_path ) start_serial , end_serial = videojson . get_min_max_chunk_serial () if start_serial is None or end_serial is None : logger . error ( f \"No chunk serials found in JSON file: { json_path } \" ) continue if end_serial < nev_start_serial : logger . info ( f \"No overlap found: { timestamp } \" ) continue elif start_serial <= nev_end_serial : logger . info ( f \"Overlap found, timestamp: { timestamp } \" ) timestamps . append ( timestamp ) else : logger . info ( f \"Break: { timestamp } \" ) break logger . info ( f \"timestamps: { timestamps } \" ) save_timestamps ( timestamps_path , timestamps ) sorted_timestamps = sort_timestamps ( timestamps ) 5. Processing Videos for Synchronization \u00b6 Once the relevant timestamps are identified, this part of the script processes video files to align them with neural data. The workflow can be divided into two phases: Before Processing Videos \u2192 Extract and merge neural and video data. After Processing Videos \u2192 Generate subclips, add synchronized audio, and export the final video. Before Processing Videos: Extracting and Merging Data \u00b6 The script iterates over each camera serial number and processes the corresponding video recordings. For each timestamp, it loads the associated camera metadata JSON file, extracts frame information, and filters the frames that overlap with the NEV chunk serial range. # 5. Go through the timestamps and process the videos for camera_serial in camera_serials : all_merged_list = [] for i , timestamp in enumerate ( sorted_timestamps ): camera_file_group = camera_files [ timestamp ] json_path = get_json_file ( camera_file_group , pathutils ) if json_path is None : logger . error ( f \"No JSON file found in group { timestamp } \" ) continue videojson = Videojson ( json_path ) camera_df = videojson . get_camera_df ( camera_serial ) camera_df [ \"frame_ids_relative\" ] = ( camera_df [ \"frame_ids_reconstructed\" ] - camera_df [ \"frame_ids_reconstructed\" ] . iloc [ 0 ] + 1 ) camera_df = camera_df . loc [ ( camera_df [ \"chunk_serial_data\" ] >= nev_start_serial ) & ( camera_df [ \"chunk_serial_data\" ] <= nev_end_serial ) ] The filtered camera frame data is then merged with the NEV chunk serials on serial to create a synchronized dataset. Next, the script extracts continuous neural/audio signals (NS5 data) for the same time window and merges them with the existing dataset. This results in a combined DataFrame containing timestamped neural/audio data, camera frame IDs, and amplitudes from the NS5 file. Note: the audio signal and the neural data are all arrays in the same format in NS5, so they can be processed in the same way. chunk_serial_joined = nev_chunk_serial_df . merge ( camera_df , left_on = \"chunk_serial\" , right_on = \"chunk_serial_data\" , how = \"inner\" , ) logger . info ( \"Processing ns5 filtered channel df...\" ) ns5_slice = ns5 . get_filtered_channel_df ( pathutils . ns5_channel , chunk_serial_joined . iloc [ 0 ][ \"TimeStamps\" ], chunk_serial_joined . iloc [ - 1 ][ \"TimeStamps\" ], ) logger . info ( \"Merging ns5 and chunk serial df...\" ) all_merged = ns5_slice . merge ( chunk_serial_joined , left_on = \"TimeStamp\" , right_on = \"TimeStamps\" , how = \"left\" , ) all_merged = all_merged [ [ \"TimeStamp\" , \"Amplitude\" , \"chunk_serial\" , \"frame_id\" , \"frame_ids_reconstructed\" , \"frame_ids_relative\" , ] ] If a matching MP4 file is found for the timestamp, it is added to the processing list. After iterating through all timestamps, the merged data for the camera serial is stored and logged for validation. mp4_path = get_mp4_file ( camera_file_group , camera_serial , pathutils ) if mp4_path is None : logger . error ( f \"No MP4 file found in group { timestamp } \" ) continue all_merged [ \"mp4_file\" ] = mp4_path all_merged_list . append ( all_merged ) if not all_merged_list : logger . warning ( f \"No valid merged data for { camera_serial } \" ) continue all_merged_df = pd . concat ( all_merged_list , ignore_index = True ) logger . info ( f \"Final merged DataFrame for { camera_serial } head: \\n { all_merged_df . head () } \" ) logger . info ( f \"Final merged DataFrame for { camera_serial } tail: \\n { all_merged_df . tail () } \" ) After Processing Videos: Synchronizing and Exporting \u00b6 With the synchronized DataFrame ready, the script processes the corresponding video files. It iterates through each unique MP4 file and extracts relevant frames based on the filtered timestamps. Using make_synced_subclip_ffmpeg() , the script generates subclips, attaching the audio data at 30 kHz. for camera_serial in camera_serials : ... # process the videos video_output_dir = os . path . join ( pathutils . output_dir , camera_serial ) os . makedirs ( video_output_dir , exist_ok = True ) video_output_path = os . path . join ( video_output_dir , \"output.mp4\" ) subclip_paths = [] for mp4_path in all_merged_df [ \"mp4_file\" ] . unique (): df_sub = all_merged_df [ all_merged_df [ \"mp4_file\" ] == mp4_path ] # Build a subclip from the relevant frames, attach audio subclip = make_synced_subclip_ffmpeg ( df_sub , mp4_path , fps_audio = 30000 , # 30kHz out_dir = os . path . join ( pathutils . output_dir , camera_serial ), ) subclip_paths . append ( subclip ) If multiple subclips are generated, they are concatenated into a single video using ffmpeg_concat_mp4s() . Finally, the fully synchronized video is saved to the output directory, completing the alignment process. This ensures that the final exported video is precisely synchronized with continuous audio amplitude signals. # Now 'subclip_paths' has each final MP4 subclip # If we have only one, just rename or copy it if len ( subclip_paths ) == 1 : final_path = subclip_paths [ 0 ] else : final_path = os . path . join ( pathutils . output_dir , camera_serial , f \"stitched_ { camera_serial } .mp4\" ) ffmpeg_concat_mp4s ( subclip_paths , final_path ) logger . info ( f \"Saved { camera_serial } to { video_output_path } \" )","title":"Program Flow"},{"location":"developer-guide/program-flow/#program-flow","text":"This section explains the data processing flow of video-sync .","title":"\ud83d\udd04 Program Flow"},{"location":"developer-guide/program-flow/#1-configuration","text":"In main.py, we load and validate the configuration using the PathUtils class. timestamp = get_current_ts () pathutils = PathUtils ( config_path , timestamp ) logger = configure_logging ( pathutils . output_dir ) if not pathutils . is_config_valid (): logger . error ( \"Config not valid, exiting to inital screen...\" ) return This ensures the configuration is valid before proceeding with synchronization.","title":"1. Configuration"},{"location":"developer-guide/program-flow/#2-data-integrity-check","text":"The DataPool class ensures all required neural and camera files are present. If files are missing, the process stops. datapool = DataPool ( pathutils . nsp_dir , pathutils . cam_recording_dir ) if not datapool . verify_integrity (): logger . error ( \"File integrity check failed: Missing or duplicate NSP files detected. \" \"Please verify the directory structure and try again. Returning to the initial screen.\" ) return","title":"2. Data Integrity Check"},{"location":"developer-guide/program-flow/#3-extracting-neural-data-nev","text":"This step extracts and reconstructs chunk serial data in the format of a dataframe from the stitched NSP-1 .nev file, which contains event-based neural data. The chunk serial values are reconstructed by combining five split chunks of serial communication sent from an Arduino. # 1. Get NEV serial start and end nsp1_nev_path = datapool . get_nsp1_nev_path () nev = Nev ( nsp1_nev_path ) nev_chunk_serial_df = nev . get_chunk_serial_df () logger . info ( f \"NEV dataframe \\n : { nev_chunk_serial_df } \" ) nev_start_serial , nev_end_serial = get_column_min_max ( nev_chunk_serial_df , \"chunk_serial\" ) logger . info ( f \"Start serial: { nev_start_serial } , End serial: { nev_end_serial } \" ) The script first retrieves the NSP-1 .nev file path and initializes an instance of the Nev class to parse its contents. It then calls get_chunk_serial_df() , which reconstructs the chunk serials by combining the five split parts. This reconstructed DataFrame provides a sequential timeline of neural events, essential for aligning with video data. To establish the valid time range for synchronization, the script determines the earliest and latest chunk serial values from the NEV data using get_column_min_max() . These values define the window in which video frames should be extracted later to ensure proper alignment.","title":"3. Extracting Neural Data (NEV)"},{"location":"developer-guide/program-flow/#4-identifying-relevant-video-and-metadata-files-for-synchronization","text":"To ensure proper alignment between neural and video data, the script identifies which camera recordings overlap with the neural event (NEV) time range. Each video recording has an associated JSON metadata file containing start and end chunk serials, which are extracted and compared against the NEV serial range. To optimize performance, the script first checks if previously computed timestamps exist in timestamps.json . If found, these timestamps are used directly to skip redundant processing. Otherwise, the script iterates through all available JSON metadata files, extracting their chunk serials and determining whether they overlap with the neural recording. If a video\u2019s serial range falls within the NEV range, its timestamp is added to the processing list. Once all relevant timestamps are identified, they are saved to timestamps.json for future runs and sorted to maintain chronological order. This approach ensures that only the necessary video files are processed, reducing computational overhead while maintaining precise synchronization. # 3. load camera serials from the config file camera_serials = pathutils . cam_serial logger . info ( f \"Camera serials loaded from config: { camera_serials } \" ) # 4. Go through all JSON files and find the ones that # are within the NEV serial range # read timestamps if available timestamps_path = os . path . join ( pathutils . output_dir , \"timestamps.json\" ) timestamps = load_timestamps ( timestamps_path , logger ) if timestamps : logger . info ( f \"Loaded timestamps: { timestamps } \" ) else : logger . info ( \"No timestamps found\" ) timestamps = [] for timestamp , camera_file_group in camera_files . items (): json_path = get_json_file ( camera_file_group , pathutils ) if json_path is None : logger . error ( f \"No JSON file found in group { timestamp } \" ) continue videojson = Videojson ( json_path ) start_serial , end_serial = videojson . get_min_max_chunk_serial () if start_serial is None or end_serial is None : logger . error ( f \"No chunk serials found in JSON file: { json_path } \" ) continue if end_serial < nev_start_serial : logger . info ( f \"No overlap found: { timestamp } \" ) continue elif start_serial <= nev_end_serial : logger . info ( f \"Overlap found, timestamp: { timestamp } \" ) timestamps . append ( timestamp ) else : logger . info ( f \"Break: { timestamp } \" ) break logger . info ( f \"timestamps: { timestamps } \" ) save_timestamps ( timestamps_path , timestamps ) sorted_timestamps = sort_timestamps ( timestamps )","title":"4. Identifying Relevant Video and Metadata Files for Synchronization"},{"location":"developer-guide/program-flow/#5-processing-videos-for-synchronization","text":"Once the relevant timestamps are identified, this part of the script processes video files to align them with neural data. The workflow can be divided into two phases: Before Processing Videos \u2192 Extract and merge neural and video data. After Processing Videos \u2192 Generate subclips, add synchronized audio, and export the final video.","title":"5. Processing Videos for Synchronization"},{"location":"developer-guide/program-flow/#before-processing-videos-extracting-and-merging-data","text":"The script iterates over each camera serial number and processes the corresponding video recordings. For each timestamp, it loads the associated camera metadata JSON file, extracts frame information, and filters the frames that overlap with the NEV chunk serial range. # 5. Go through the timestamps and process the videos for camera_serial in camera_serials : all_merged_list = [] for i , timestamp in enumerate ( sorted_timestamps ): camera_file_group = camera_files [ timestamp ] json_path = get_json_file ( camera_file_group , pathutils ) if json_path is None : logger . error ( f \"No JSON file found in group { timestamp } \" ) continue videojson = Videojson ( json_path ) camera_df = videojson . get_camera_df ( camera_serial ) camera_df [ \"frame_ids_relative\" ] = ( camera_df [ \"frame_ids_reconstructed\" ] - camera_df [ \"frame_ids_reconstructed\" ] . iloc [ 0 ] + 1 ) camera_df = camera_df . loc [ ( camera_df [ \"chunk_serial_data\" ] >= nev_start_serial ) & ( camera_df [ \"chunk_serial_data\" ] <= nev_end_serial ) ] The filtered camera frame data is then merged with the NEV chunk serials on serial to create a synchronized dataset. Next, the script extracts continuous neural/audio signals (NS5 data) for the same time window and merges them with the existing dataset. This results in a combined DataFrame containing timestamped neural/audio data, camera frame IDs, and amplitudes from the NS5 file. Note: the audio signal and the neural data are all arrays in the same format in NS5, so they can be processed in the same way. chunk_serial_joined = nev_chunk_serial_df . merge ( camera_df , left_on = \"chunk_serial\" , right_on = \"chunk_serial_data\" , how = \"inner\" , ) logger . info ( \"Processing ns5 filtered channel df...\" ) ns5_slice = ns5 . get_filtered_channel_df ( pathutils . ns5_channel , chunk_serial_joined . iloc [ 0 ][ \"TimeStamps\" ], chunk_serial_joined . iloc [ - 1 ][ \"TimeStamps\" ], ) logger . info ( \"Merging ns5 and chunk serial df...\" ) all_merged = ns5_slice . merge ( chunk_serial_joined , left_on = \"TimeStamp\" , right_on = \"TimeStamps\" , how = \"left\" , ) all_merged = all_merged [ [ \"TimeStamp\" , \"Amplitude\" , \"chunk_serial\" , \"frame_id\" , \"frame_ids_reconstructed\" , \"frame_ids_relative\" , ] ] If a matching MP4 file is found for the timestamp, it is added to the processing list. After iterating through all timestamps, the merged data for the camera serial is stored and logged for validation. mp4_path = get_mp4_file ( camera_file_group , camera_serial , pathutils ) if mp4_path is None : logger . error ( f \"No MP4 file found in group { timestamp } \" ) continue all_merged [ \"mp4_file\" ] = mp4_path all_merged_list . append ( all_merged ) if not all_merged_list : logger . warning ( f \"No valid merged data for { camera_serial } \" ) continue all_merged_df = pd . concat ( all_merged_list , ignore_index = True ) logger . info ( f \"Final merged DataFrame for { camera_serial } head: \\n { all_merged_df . head () } \" ) logger . info ( f \"Final merged DataFrame for { camera_serial } tail: \\n { all_merged_df . tail () } \" )","title":"Before Processing Videos: Extracting and Merging Data"},{"location":"developer-guide/program-flow/#after-processing-videos-synchronizing-and-exporting","text":"With the synchronized DataFrame ready, the script processes the corresponding video files. It iterates through each unique MP4 file and extracts relevant frames based on the filtered timestamps. Using make_synced_subclip_ffmpeg() , the script generates subclips, attaching the audio data at 30 kHz. for camera_serial in camera_serials : ... # process the videos video_output_dir = os . path . join ( pathutils . output_dir , camera_serial ) os . makedirs ( video_output_dir , exist_ok = True ) video_output_path = os . path . join ( video_output_dir , \"output.mp4\" ) subclip_paths = [] for mp4_path in all_merged_df [ \"mp4_file\" ] . unique (): df_sub = all_merged_df [ all_merged_df [ \"mp4_file\" ] == mp4_path ] # Build a subclip from the relevant frames, attach audio subclip = make_synced_subclip_ffmpeg ( df_sub , mp4_path , fps_audio = 30000 , # 30kHz out_dir = os . path . join ( pathutils . output_dir , camera_serial ), ) subclip_paths . append ( subclip ) If multiple subclips are generated, they are concatenated into a single video using ffmpeg_concat_mp4s() . Finally, the fully synchronized video is saved to the output directory, completing the alignment process. This ensures that the final exported video is precisely synchronized with continuous audio amplitude signals. # Now 'subclip_paths' has each final MP4 subclip # If we have only one, just rename or copy it if len ( subclip_paths ) == 1 : final_path = subclip_paths [ 0 ] else : final_path = os . path . join ( pathutils . output_dir , camera_serial , f \"stitched_ { camera_serial } .mp4\" ) ffmpeg_concat_mp4s ( subclip_paths , final_path ) logger . info ( f \"Saved { camera_serial } to { video_output_path } \" )","title":"After Processing Videos: Synchronizing and Exporting"}]}