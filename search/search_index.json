{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfa5 Video Sync","text":"<p>A Python tool to synchronize NSP data with camera recordings.</p> <p>Welcome to the official documentation for <code>video-sync</code>. This tool allows you to synchronize Neural Signal Processing (NSP) data with camera recordings. It processes NEV and NS5 files, aligns timestamps with camera JSON metadata, slices video based on valid frames, and synchronizes audio with video.</p>"},{"location":"#documentation-overview","title":"\ud83d\udcd6 Documentation Overview","text":"<ul> <li>Installation</li> <li>Configuration</li> <li>Usage Guide</li> <li>Features</li> <li>EMU Camera Serials</li> <li>License</li> </ul>"},{"location":"configuration/","title":"\u2699\ufe0f Configuration","text":"<p>Before running <code>video-sync</code>, you need to configure it using a YAML configuration file.</p> <p>A sample configuration file is provided in the repo and is also available to download below. Rename the example template to <code>config.yaml</code> and replace the paths to your desired paths</p> <p>The following command renames the example config file to <code>config.yaml</code> <pre><code>cp config.example.yaml config.yaml\n</code></pre></p> <p>\ud83d\udce5 Download config.example.yaml</p>"},{"location":"emu-cameras/","title":"\ud83d\udcf7 EMU Camera Serials","text":"<p>List of supported EMU camera serial numbers:</p> Camera Serial Position 18486634 F1 23512908 F2 18486644 F3 18486638 B1 23512014 B2 23512906 R1 23512012 R2 23505577 R3 <p>Use these serials when mapping cameras for video synchronization.</p>"},{"location":"features/","title":"\ud83c\udfd7\ufe0f Features","text":"<p><code>video-sync</code> provides the following features:</p> <p>\u2714\ufe0f Synchronizes NEV and NS5 files with camera recordings \u2714\ufe0f Slices video based on valid frames \u2714\ufe0f Aligns audio with video for precise synchronization \u2714\ufe0f Supports configurable processing options \u2714\ufe0f Efficient and scalable for large datasets  </p>"},{"location":"installation/","title":"\ud83d\udce5 Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>video-sync</code>, ensure you have the following dependencies:</p> <ul> <li>Conda (for environment management)</li> <li>FFmpeg (for video and audio processing)</li> </ul>"},{"location":"installation/#installing-miniconda","title":"Installing Miniconda","text":"<p>Miniconda is the light-weight version of Conda, it's recommended for all OS. Visit the official Miniconda download page to download the installer for your OS.</p>"},{"location":"installation/#installing-ffmpeg","title":"Installing FFmpeg","text":"<p>For Ubuntu/Debian: <pre><code>sudo apt update\nsudo apt install ffmpeg\n</code></pre></p> <p>For RHEL (with EPEL enabled) <pre><code>sudo yum install epel-release\nsudo yum install ffmpeg\n</code></pre></p> <p>For MacOS</p> <p>Ensure Homebrew is installed, then run: <pre><code>brew install ffmpeg\n</code></pre></p> <p>For Windows</p> <p>Use Chocolatey (recommended): <pre><code>choco install ffmpeg\n</code></pre></p>"},{"location":"installation/#installing-video-sync","title":"Installing video-sync","text":"<p>Clone the repository:</p> <pre><code>git clone git@github.com:BCM-Neurosurgery/video-sync.git\ncd video-sync\n</code></pre> <p>Note: if you don't have permission, reach out to Yewen</p> <p>For EMU tasks, check out branch <code>stitch-videos-to-neuro</code> <pre><code>git checkout stitch-videos-to-neuro\n</code></pre></p> <p>For TRD tasks, checkout branch <code>trd</code> <pre><code>git checkout trd\n</code></pre></p> <p>Create and activate the Conda environment, then install dependencies:</p> <pre><code>conda env create -f environment.yml\nconda activate videosync\npip install .\n</code></pre>"},{"location":"license/","title":"\ud83d\udcdc License","text":"<p>This project is licensed under the BSD-3-Clause License.  </p>"},{"location":"usage/","title":"\ud83d\ude80 Usage Guide","text":""},{"location":"usage/#running-video-sync","title":"Running <code>video-sync</code>","text":"<p>Make sure the conda environment <code>videosync</code> is activated and run <pre><code>stitch-videos --config path/to/config.yaml\n</code></pre></p> <p>Example Command</p> <pre><code>stitch-videos --config /home/auto/CODE/utils/video-sync/Testing/YFITesting/config.yaml\n</code></pre>"},{"location":"api/","title":"\ud83d\udee0 API Reference","text":"<p>This section provides documentation for all scripts used in <code>video-sync</code>.</p> <ul> <li>DataPoool Module</li> <li>Nev Module</li> <li>Nsx Module</li> <li>Nsx Module</li> <li>Video Module</li> <li>Videojson Module</li> <li>Pathutils Module</li> <li>Utils Module</li> </ul>"},{"location":"api/datapool/","title":"\ud83d\udcc4 DataPool API Documentation","text":"<p>This section provides detailed documentation for <code>pyvideosync.data_pool</code>.</p>"},{"location":"api/datapool/#datapool-class","title":"\ud83d\udccc DataPool Class","text":"<p>Manages NSP and video data for integrity verification and statistics.</p> <p>Attributes:</p> Name Type Description <code>nsp_dir</code> <code>str</code> <p>Directory containing NSP files.</p> <code>cam_recording_dir</code> <code>str</code> <p>Directory containing camera recordings.</p> <code>nev_pool</code> <code>NevPool</code> <p>Stores NEV files.</p> <code>nsx_pool</code> <code>NsxPool</code> <p>Stores NS5/NS3 files.</p> <code>video_pool</code> <code>VideoPool</code> <p>Stores video files.</p> <code>video_json_pool</code> <code>VideoJsonPool</code> <p>Stores video metadata.</p> <code>video_file_pool</code> <code>VideoFilesPool</code> <p>Stores all video-related files.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class DataPool:\n    \"\"\"Manages NSP and video data for integrity verification and statistics.\n\n    Attributes:\n        nsp_dir (str): Directory containing NSP files.\n        cam_recording_dir (str): Directory containing camera recordings.\n        nev_pool (NevPool): Stores NEV files.\n        nsx_pool (NsxPool): Stores NS5/NS3 files.\n        video_pool (VideoPool): Stores video files.\n        video_json_pool (VideoJsonPool): Stores video metadata.\n        video_file_pool (VideoFilesPool): Stores all video-related files.\n    \"\"\"\n\n    def __init__(self, nsp_dir: str, cam_recording_dir: str) -&gt; None:\n        \"\"\"Initializes the DataPool class.\n\n        Args:\n            nsp_dir (str): Path to the NSP directory.\n            cam_recording_dir (str): Path to the camera recording directory.\n        \"\"\"\n        self.nsp_dir = nsp_dir\n        self.cam_recording_dir = cam_recording_dir\n        self.nev_pool = NevPool()\n        self.nsx_pool = NsxPool()\n        self.video_pool = VideoPool()\n        self.video_json_pool = VideoJsonPool()\n        self.video_file_pool = VideoFilesPool()\n        self.init_pools()\n\n    def init_pools(self):\n        \"\"\"Initializes the pools by:\n\n        Grouping the files in the video pool by timestamp.\n        \"\"\"\n        for taskfolder_path in Path(self.cam_recording_dir).iterdir():\n            if taskfolder_path.is_dir():\n                for datefolder_path in taskfolder_path.iterdir():\n                    if datefolder_path.is_dir():\n                        for file_path in datefolder_path.iterdir():\n                            self.video_file_pool.add_file(str(file_path.resolve()))\n\n    def verify_integrity(self) -&gt; bool:\n        \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one `.nev` file and one `.ns5` file.\n\n        Returns:\n            bool: True if exactly one `.nev` file and exactly one `.ns5` file are found, otherwise False.\n        \"\"\"\n        nev_count = 0\n        ns5_count = 0\n\n        for file in os.listdir(self.nsp_dir):\n            if fnmatch.fnmatch(file, \"*.nev\"):\n                nev_count += 1\n            elif fnmatch.fnmatch(file, \"*.ns5\"):\n                ns5_count += 1\n\n        return nev_count == 1 and ns5_count == 1\n\n    def get_nev_path(self) -&gt; str:\n        \"\"\"Finds the first NEV file in the directory.\n\n        Returns:\n            str: The full path of the first matching `.nev` file if found, otherwise an empty string.\n        \"\"\"\n        for file in os.listdir(self.nsp_dir):\n            if fnmatch.fnmatch(file, \"*.nev\"):\n                return os.path.join(self.nsp_dir, file)\n\n        return \"\"\n\n    def get_ns5_path(self) -&gt; str:\n        \"\"\"Finds the first NS5 file in the directory.\n\n        Returns:\n            str: The full path of the first matching `.ns5` file if found, otherwise an empty string.\n        \"\"\"\n        for file in os.listdir(self.nsp_dir):\n            if fnmatch.fnmatch(file, \"*.ns5\"):\n                return os.path.join(self.nsp_dir, file)\n\n        return \"\"\n\n    def get_video_file_pool(self) -&gt; \"VideoFilesPool\":\n        \"\"\"Retrieves the video file pool.\n\n        Returns:\n            VideoFilesPool: The video file pool object.\n        \"\"\"\n        return self.video_file_pool\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.cam_recording_dir","title":"<code>cam_recording_dir = cam_recording_dir</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nev_pool","title":"<code>nev_pool = NevPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nsp_dir","title":"<code>nsp_dir = nsp_dir</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nsx_pool","title":"<code>nsx_pool = NsxPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_file_pool","title":"<code>video_file_pool = VideoFilesPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_json_pool","title":"<code>video_json_pool = VideoJsonPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_pool","title":"<code>video_pool = VideoPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.__init__","title":"<code>__init__(nsp_dir: str, cam_recording_dir: str) -&gt; None</code>","text":"<p>Initializes the DataPool class.</p> <p>Parameters:</p> Name Type Description Default <code>nsp_dir</code> <code>str</code> <p>Path to the NSP directory.</p> required <code>cam_recording_dir</code> <code>str</code> <p>Path to the camera recording directory.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self, nsp_dir: str, cam_recording_dir: str) -&gt; None:\n    \"\"\"Initializes the DataPool class.\n\n    Args:\n        nsp_dir (str): Path to the NSP directory.\n        cam_recording_dir (str): Path to the camera recording directory.\n    \"\"\"\n    self.nsp_dir = nsp_dir\n    self.cam_recording_dir = cam_recording_dir\n    self.nev_pool = NevPool()\n    self.nsx_pool = NsxPool()\n    self.video_pool = VideoPool()\n    self.video_json_pool = VideoJsonPool()\n    self.video_file_pool = VideoFilesPool()\n    self.init_pools()\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_nev_path","title":"<code>get_nev_path() -&gt; str</code>","text":"<p>Finds the first NEV file in the directory.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full path of the first matching <code>.nev</code> file if found, otherwise an empty string.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_nev_path(self) -&gt; str:\n    \"\"\"Finds the first NEV file in the directory.\n\n    Returns:\n        str: The full path of the first matching `.nev` file if found, otherwise an empty string.\n    \"\"\"\n    for file in os.listdir(self.nsp_dir):\n        if fnmatch.fnmatch(file, \"*.nev\"):\n            return os.path.join(self.nsp_dir, file)\n\n    return \"\"\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_ns5_path","title":"<code>get_ns5_path() -&gt; str</code>","text":"<p>Finds the first NS5 file in the directory.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full path of the first matching <code>.ns5</code> file if found, otherwise an empty string.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_ns5_path(self) -&gt; str:\n    \"\"\"Finds the first NS5 file in the directory.\n\n    Returns:\n        str: The full path of the first matching `.ns5` file if found, otherwise an empty string.\n    \"\"\"\n    for file in os.listdir(self.nsp_dir):\n        if fnmatch.fnmatch(file, \"*.ns5\"):\n            return os.path.join(self.nsp_dir, file)\n\n    return \"\"\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_video_file_pool","title":"<code>get_video_file_pool() -&gt; 'VideoFilesPool'</code>","text":"<p>Retrieves the video file pool.</p> <p>Returns:</p> Name Type Description <code>VideoFilesPool</code> <code>'VideoFilesPool'</code> <p>The video file pool object.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_video_file_pool(self) -&gt; \"VideoFilesPool\":\n    \"\"\"Retrieves the video file pool.\n\n    Returns:\n        VideoFilesPool: The video file pool object.\n    \"\"\"\n    return self.video_file_pool\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.init_pools","title":"<code>init_pools()</code>","text":"<p>Initializes the pools by:</p> <p>Grouping the files in the video pool by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def init_pools(self):\n    \"\"\"Initializes the pools by:\n\n    Grouping the files in the video pool by timestamp.\n    \"\"\"\n    for taskfolder_path in Path(self.cam_recording_dir).iterdir():\n        if taskfolder_path.is_dir():\n            for datefolder_path in taskfolder_path.iterdir():\n                if datefolder_path.is_dir():\n                    for file_path in datefolder_path.iterdir():\n                        self.video_file_pool.add_file(str(file_path.resolve()))\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.verify_integrity","title":"<code>verify_integrity() -&gt; bool</code>","text":"<p>Verifies the integrity of the directory by ensuring it contains exactly one <code>.nev</code> file and one <code>.ns5</code> file.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if exactly one <code>.nev</code> file and exactly one <code>.ns5</code> file are found, otherwise False.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def verify_integrity(self) -&gt; bool:\n    \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one `.nev` file and one `.ns5` file.\n\n    Returns:\n        bool: True if exactly one `.nev` file and exactly one `.ns5` file are found, otherwise False.\n    \"\"\"\n    nev_count = 0\n    ns5_count = 0\n\n    for file in os.listdir(self.nsp_dir):\n        if fnmatch.fnmatch(file, \"*.nev\"):\n            nev_count += 1\n        elif fnmatch.fnmatch(file, \"*.ns5\"):\n            ns5_count += 1\n\n    return nev_count == 1 and ns5_count == 1\n</code></pre>"},{"location":"api/datapool/#supporting-classes","title":"\ud83d\udccc Supporting Classes","text":""},{"location":"api/datapool/#nevpool","title":"NevPool","text":"<p>Stores NEV files grouped by suffix.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class NevPool:\n    \"\"\"Stores NEV files grouped by suffix.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds a NEV file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        suffix = file.split(\"-\")[-1]\n        self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a NEV file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds a NEV file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    suffix = file.split(\"-\")[-1]\n    self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#nsxpool","title":"NsxPool","text":"<p>Stores NS5 and NS3 files grouped by suffix.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class NsxPool:\n    \"\"\"Stores NS5 and NS3 files grouped by suffix.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds an NS5/NS3 file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        suffix = file.split(\"-\")[-1]\n        self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds an NS5/NS3 file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds an NS5/NS3 file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    suffix = file.split(\"-\")[-1]\n    self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#videopool","title":"VideoPool","text":"<p>Stores video files grouped by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class VideoPool:\n    \"\"\"Stores video files grouped by timestamp.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds a video file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        timestamp = file.split(\"_\")[-1].split(\".\")[0]\n        self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a video file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds a video file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    timestamp = file.split(\"_\")[-1].split(\".\")[0]\n    self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#videojsonpool","title":"VideoJsonPool","text":"<p>Stores video metadata JSON files grouped by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class VideoJsonPool:\n    \"\"\"Stores video metadata JSON files grouped by timestamp.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"\n        Adds a file to the internal dictionary, grouping by its timestamp.\n\n        The timestamp is extracted from the file name as the portion after the last underscore (`_`)\n        and before the file extension (`.`).\n\n        Example:\n            Given file names:\n            - \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n            - \"utsw_TRD011_day_1_20240716_152736.json\"\n\n            The extracted timestamps would be:\n            - \"154737\" from \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n            - \"152736\" from \"utsw_TRD011_day_1_20240716_152736.json\"\n\n            These files are then grouped by their extracted timestamp.\n\n        Args:\n            file (str): The file name including its extension.\n\n        \"\"\"\n        timestamp = file.split(\"_\")[-1].split(\".\")[0]\n        self.files[timestamp].append(file)\n\n    def list_groups(self) -&gt; dict[str, list[str]]:\n        \"\"\"Lists all groups of video metadata files.\n\n        Returns:\n            dict[str, list[str]]: A dictionary where keys are timestamps (str)\n            and values are lists of file names (str).\n        \"\"\"\n        return {timestamp: files for timestamp, files in self.files.items()}\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a file to the internal dictionary, grouping by its timestamp.</p> <p>The timestamp is extracted from the file name as the portion after the last underscore (<code>_</code>) and before the file extension (<code>.</code>).</p> Example <p>Given file names: - \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\" - \"utsw_TRD011_day_1_20240716_152736.json\"</p> <p>The extracted timestamps would be: - \"154737\" from \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\" - \"152736\" from \"utsw_TRD011_day_1_20240716_152736.json\"</p> <p>These files are then grouped by their extracted timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file name including its extension.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"\n    Adds a file to the internal dictionary, grouping by its timestamp.\n\n    The timestamp is extracted from the file name as the portion after the last underscore (`_`)\n    and before the file extension (`.`).\n\n    Example:\n        Given file names:\n        - \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n        - \"utsw_TRD011_day_1_20240716_152736.json\"\n\n        The extracted timestamps would be:\n        - \"154737\" from \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n        - \"152736\" from \"utsw_TRD011_day_1_20240716_152736.json\"\n\n        These files are then grouped by their extracted timestamp.\n\n    Args:\n        file (str): The file name including its extension.\n\n    \"\"\"\n    timestamp = file.split(\"_\")[-1].split(\".\")[0]\n    self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.list_groups","title":"<code>list_groups() -&gt; dict[str, list[str]]</code>","text":"<p>Lists all groups of video metadata files.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: A dictionary where keys are timestamps (str)</p> <code>dict[str, list[str]]</code> <p>and values are lists of file names (str).</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def list_groups(self) -&gt; dict[str, list[str]]:\n    \"\"\"Lists all groups of video metadata files.\n\n    Returns:\n        dict[str, list[str]]: A dictionary where keys are timestamps (str)\n        and values are lists of file names (str).\n    \"\"\"\n    return {timestamp: files for timestamp, files in self.files.items()}\n</code></pre>"},{"location":"api/datapool/#videofilespool","title":"VideoFilesPool","text":"<p>Stores all video-related files grouped by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class VideoFilesPool:\n    \"\"\"Stores all video-related files grouped by timestamp.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds a video-related file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        timestamp = extract_timestamp(file)\n        self.files[timestamp].append(file)\n\n    def list_groups(self) -&gt; dict[str, list[str]]:\n        \"\"\"Lists groups of files sorted by timestamp.\n\n        Returns:\n            dict[str, list[str]]: A dictionary where keys are timestamps (str)\n            and values are lists of file names (str).\n        \"\"\"\n        return {timestamp: self.files[timestamp] for timestamp in sorted(self.files)}\n\n    def find_one_random_json(self) -&gt; str | None:\n        \"\"\"Finds a random JSON file in the pool.\n\n        Returns:\n            str: A JSON file name if found, otherwise None.\n        \"\"\"\n        for files in self.files.values():\n            for file in files:\n                if file.endswith(\".json\"):\n                    return file\n        return None\n\n    def get_unique_cam_serials(self) -&gt; set[str]:\n        \"\"\"\n        Returns a set of all unique camera serial numbers found in the filenames.\n\n        Returns:\n            set[str]: A set of unique camera serial numbers.\n        \"\"\"\n        serials = set()\n        for files in self.files.values():\n            for file in files:\n                if file.endswith(\".mp4\"):\n                    serial = extract_cam_serial(file)\n                if serial:\n                    serials.add(serial)\n        return serials\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a video-related file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds a video-related file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    timestamp = extract_timestamp(file)\n    self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.find_one_random_json","title":"<code>find_one_random_json() -&gt; str | None</code>","text":"<p>Finds a random JSON file in the pool.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>A JSON file name if found, otherwise None.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def find_one_random_json(self) -&gt; str | None:\n    \"\"\"Finds a random JSON file in the pool.\n\n    Returns:\n        str: A JSON file name if found, otherwise None.\n    \"\"\"\n    for files in self.files.values():\n        for file in files:\n            if file.endswith(\".json\"):\n                return file\n    return None\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.get_unique_cam_serials","title":"<code>get_unique_cam_serials() -&gt; set[str]</code>","text":"<p>Returns a set of all unique camera serial numbers found in the filenames.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: A set of unique camera serial numbers.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_unique_cam_serials(self) -&gt; set[str]:\n    \"\"\"\n    Returns a set of all unique camera serial numbers found in the filenames.\n\n    Returns:\n        set[str]: A set of unique camera serial numbers.\n    \"\"\"\n    serials = set()\n    for files in self.files.values():\n        for file in files:\n            if file.endswith(\".mp4\"):\n                serial = extract_cam_serial(file)\n            if serial:\n                serials.add(serial)\n    return serials\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.list_groups","title":"<code>list_groups() -&gt; dict[str, list[str]]</code>","text":"<p>Lists groups of files sorted by timestamp.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: A dictionary where keys are timestamps (str)</p> <code>dict[str, list[str]]</code> <p>and values are lists of file names (str).</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def list_groups(self) -&gt; dict[str, list[str]]:\n    \"\"\"Lists groups of files sorted by timestamp.\n\n    Returns:\n        dict[str, list[str]]: A dictionary where keys are timestamps (str)\n        and values are lists of file names (str).\n    \"\"\"\n    return {timestamp: self.files[timestamp] for timestamp in sorted(self.files)}\n</code></pre>"},{"location":"developer-guide/","title":"\ud83d\udee0\ufe0f Developer Guide","text":"<p>Welcome to the Developer Guide for <code>video-sync</code>. This section provides a deep dive into the internal workings of the project, covering:</p> <ul> <li>Program Flow: How the tool processes and synchronizes data.</li> <li>Contributing: How developers can contribute to the project.</li> </ul>"},{"location":"developer-guide/contributing/","title":"\ud83e\udd1d Contributing Guide","text":"<p>We welcome contributions to improve video-sync! Follow these steps to contribute:</p> <p>Note: This guide assumes basic familiarity with Git, Python, and command-line environments. </p>"},{"location":"developer-guide/contributing/#1-fork-clone-the-repository","title":"1: Fork &amp; Clone the Repository","text":"<p>Click the Fork button at the top right of the GitHub repository</p> <p>Clone your fork <pre><code>git clone https://github.com/YOUR_USERNAME/video-sync.git\ncd video-sync\n</code></pre></p> <p>Set the upstream remote</p> <pre><code>git remote add upstream https://github.com/bcm-neurosurgery/video-sync.git\n</code></pre>"},{"location":"developer-guide/contributing/#2-set-up-your-environment","title":"2: Set Up Your Environment","text":"<p>Please refer to installations guide</p> <p>Verify installation <pre><code>stitch-videos --help\n</code></pre></p>"},{"location":"developer-guide/contributing/#3-create-a-branch","title":"3: Create a Branch","text":"<p>Before making any changes, create a new branch:</p> <pre><code>git checkout -b feature/my-awesome-feature\n</code></pre> <p>For bug fixes, use:</p> <pre><code>git checkout -b fix/issue-123\n</code></pre> <p>Replace my-awesome-feature or issue-123 with a meaningful name.</p>"},{"location":"developer-guide/contributing/#4-make-changes-test","title":"4: Make Changes &amp; Test","text":"<p>Write your code and make changes. Run tests to ensure everything works:</p> <p>Format your code with: <pre><code>black .\n</code></pre></p>"},{"location":"developer-guide/contributing/#5-commit-and-push","title":"5: Commit and Push","text":"<p>Stage and commit your changes</p> <pre><code>git add .\ngit commit -m \"Add feature: my awesome feature\"\n</code></pre> <p>Push your branch to GitHub</p> <pre><code>git push origin feature/my-awesome-feature\n</code></pre>"},{"location":"developer-guide/contributing/#6-submit-a-pull-request","title":"6: Submit a Pull Request","text":"<p>Go to your fork on GitHub. Click the \"New Pull Request\" button. Select your branch and compare it with the main branch. Write a clear description of your changes. Click \"Create Pull Request\".</p> <p>Yewen will review your code and provide feedback if needed.</p>"},{"location":"developer-guide/contributing/#7-keep-your-fork-updated","title":"7: Keep Your Fork Updated","text":"<p>To avoid merge conflicts, keep your local repository updated with the latest changes:</p> <pre><code>git checkout main\ngit pull upstream main\ngit checkout feature/my-awesome-feature\ngit merge main\n</code></pre>"},{"location":"developer-guide/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<ul> <li>Follow PEP 8 for Python code style.</li> <li>Write clear commit messages.</li> <li>Keep pull requests focused\u2014one feature or bug fix per PR.</li> <li>Add comments and docstrings where necessary.</li> <li>If modifying behavior, update documentation and tests.</li> </ul> <p>Happy coding! Thank you for improving video-sync!</p>"},{"location":"developer-guide/program-flow/","title":"\ud83d\udd04 Program Flow","text":"<p>This section explains the data processing flow of <code>video-sync</code>.</p>"},{"location":"developer-guide/program-flow/#1-configuration","title":"1. Configuration","text":"<p>In main.py, we load and validate the configuration using the <code>PathUtils</code> class.</p> <pre><code>timestamp = get_current_ts()\n\npathutils = PathUtils(config_path, timestamp)\nlogger = configure_logging(pathutils.output_dir)\n\nif not pathutils.is_config_valid():\n    logger.error(\"Config not valid, exiting to inital screen...\")\n    return\n</code></pre> <p>This ensures the configuration is valid before proceeding with synchronization.</p>"},{"location":"developer-guide/program-flow/#2-data-integrity-check","title":"2. Data Integrity Check","text":"<p>The DataPool class ensures all required neural and camera files are present.</p> <p>If files are missing, the process stops.</p> <pre><code>datapool = DataPool(pathutils.nsp_dir, pathutils.cam_recording_dir)\n\nif not datapool.verify_integrity():\n    logger.error(\n        \"File integrity check failed: Missing or duplicate NSP files detected. \"\n        \"Please verify the directory structure and try again. Returning to the initial screen.\"\n    )\n    return\n</code></pre>"},{"location":"developer-guide/program-flow/#3-extracting-neural-data-nev","title":"3. Extracting Neural Data (NEV)","text":"<p>This step extracts and reconstructs chunk serial data in the format of a dataframe from the stitched NSP-1 <code>.nev</code> file, which contains event-based neural data. The chunk serial values are reconstructed by combining five split chunks of serial communication sent from an Arduino.</p> <pre><code># 1. Get NEV serial start and end\nnsp1_nev_path = datapool.get_nsp1_nev_path()\nnev = Nev(nsp1_nev_path)\nnev_chunk_serial_df = nev.get_chunk_serial_df()\nlogger.info(f\"NEV dataframe\\n: {nev_chunk_serial_df}\")\nnev_start_serial, nev_end_serial = get_column_min_max(\n    nev_chunk_serial_df, \"chunk_serial\"\n)\nlogger.info(f\"Start serial: {nev_start_serial}, End serial: {nev_end_serial}\")\n</code></pre> <p>The script first retrieves the NSP-1 <code>.nev</code> file path and initializes an instance of the Nev class to parse its contents. It then calls <code>get_chunk_serial_df()</code>, which reconstructs the chunk serials by combining the five split parts. This reconstructed DataFrame provides a sequential timeline of neural events, essential for aligning with video data.</p> <p>To establish the valid time range for synchronization, the script determines the earliest and latest chunk serial values from the NEV data using <code>get_column_min_max()</code>. These values define the window in which video frames should be extracted later to ensure proper alignment.</p>"},{"location":"developer-guide/program-flow/#4-identifying-relevant-video-and-metadata-files-for-synchronization","title":"4. Identifying Relevant Video and Metadata Files for Synchronization","text":"<p>To ensure proper alignment between neural and video data, the script identifies which camera recordings overlap with the neural event (NEV) time range. Each video recording has an associated JSON metadata file containing start and end chunk serials, which are extracted and compared against the NEV serial range.</p> <p>To optimize performance, the script first checks if previously computed timestamps exist in <code>timestamps.json</code>. If found, these timestamps are used directly to skip redundant processing. Otherwise, the script iterates through all available JSON metadata files, extracting their chunk serials and determining whether they overlap with the neural recording. If a video\u2019s serial range falls within the NEV range, its timestamp is added to the processing list.</p> <p>Once all relevant timestamps are identified, they are saved to <code>timestamps.json</code> for future runs and sorted to maintain chronological order. This approach ensures that only the necessary video files are processed, reducing computational overhead while maintaining precise synchronization.</p> <pre><code># 3. load camera serials from the config file\ncamera_serials = pathutils.cam_serial\nlogger.info(f\"Camera serials loaded from config: {camera_serials}\")\n\n# 4. Go through all JSON files and find the ones that\n# are within the NEV serial range\n# read timestamps if available\ntimestamps_path = os.path.join(pathutils.output_dir, \"timestamps.json\")\ntimestamps = load_timestamps(timestamps_path, logger)\nif timestamps:\n    logger.info(f\"Loaded timestamps: {timestamps}\")\nelse:\n    logger.info(\"No timestamps found\")\n    timestamps = []\n    for timestamp, camera_file_group in camera_files.items():\n\n        json_path = get_json_file(camera_file_group, pathutils)\n        if json_path is None:\n            logger.error(f\"No JSON file found in group {timestamp}\")\n            continue\n        videojson = Videojson(json_path)\n        start_serial, end_serial = videojson.get_min_max_chunk_serial()\n        if start_serial is None or end_serial is None:\n            logger.error(f\"No chunk serials found in JSON file: {json_path}\")\n            continue\n\n        if end_serial &lt; nev_start_serial:\n            logger.info(f\"No overlap found: {timestamp}\")\n            continue\n\n        elif start_serial &lt;= nev_end_serial:\n            logger.info(f\"Overlap found, timestamp: {timestamp}\")\n            timestamps.append(timestamp)\n\n        else:\n            logger.info(f\"Break: {timestamp}\")\n            break\n    logger.info(f\"timestamps: {timestamps}\")\n    save_timestamps(timestamps_path, timestamps)\n\nsorted_timestamps = sort_timestamps(timestamps)\n</code></pre>"},{"location":"developer-guide/program-flow/#5-processing-videos-for-synchronization","title":"5. Processing Videos for Synchronization","text":"<p>Once the relevant timestamps are identified, this part of the script processes video files to align them with neural data. The workflow can be divided into two phases:</p> <ul> <li>Before Processing Videos \u2192 Extract and merge neural and video data.</li> <li>After Processing Videos \u2192 Generate subclips, add synchronized audio, and export the final video.</li> </ul>"},{"location":"developer-guide/program-flow/#before-processing-videos-extracting-and-merging-data","title":"Before Processing Videos: Extracting and Merging Data","text":"<p>The script iterates over each camera serial number and processes the corresponding video recordings. For each timestamp, it loads the associated camera metadata JSON file, extracts frame information, and filters the frames that overlap with the NEV chunk serial range.</p> <pre><code># 5. Go through the timestamps and process the videos\nfor camera_serial in camera_serials:\n    all_merged_list = []\n\n    for i, timestamp in enumerate(sorted_timestamps):\n        camera_file_group = camera_files[timestamp]\n\n        json_path = get_json_file(camera_file_group, pathutils)\n        if json_path is None:\n            logger.error(f\"No JSON file found in group {timestamp}\")\n            continue\n\n        videojson = Videojson(json_path)\n        camera_df = videojson.get_camera_df(camera_serial)\n        camera_df[\"frame_ids_relative\"] = (\n            camera_df[\"frame_ids_reconstructed\"]\n            - camera_df[\"frame_ids_reconstructed\"].iloc[0]\n            + 1\n        )\n\n        camera_df = camera_df.loc[\n            (camera_df[\"chunk_serial_data\"] &gt;= nev_start_serial)\n            &amp; (camera_df[\"chunk_serial_data\"] &lt;= nev_end_serial)\n        ]\n</code></pre> <p>The filtered camera frame data is then merged with the NEV chunk serials on serial to create a synchronized dataset. Next, the script extracts continuous neural/audio signals (NS5 data) for the same time window and merges them with the existing dataset. This results in a combined DataFrame containing timestamped neural/audio data, camera frame IDs, and amplitudes from the NS5 file.</p> <p>Note: the audio signal and the neural data are all arrays in the same format in NS5, so they can be processed in the same way.</p> <pre><code>        chunk_serial_joined = nev_chunk_serial_df.merge(\n            camera_df,\n            left_on=\"chunk_serial\",\n            right_on=\"chunk_serial_data\",\n            how=\"inner\",\n        )\n\n        logger.info(\"Processing ns5 filtered channel df...\")\n        ns5_slice = ns5.get_filtered_channel_df(\n            pathutils.ns5_channel,\n            chunk_serial_joined.iloc[0][\"TimeStamps\"],\n            chunk_serial_joined.iloc[-1][\"TimeStamps\"],\n        )\n\n        logger.info(\"Merging ns5 and chunk serial df...\")\n        all_merged = ns5_slice.merge(\n            chunk_serial_joined,\n            left_on=\"TimeStamp\",\n            right_on=\"TimeStamps\",\n            how=\"left\",\n        )\n\n        all_merged = all_merged[\n            [\n                \"TimeStamp\",\n                \"Amplitude\",\n                \"chunk_serial\",\n                \"frame_id\",\n                \"frame_ids_reconstructed\",\n                \"frame_ids_relative\",\n            ]\n        ]\n</code></pre> <p>If a matching MP4 file is found for the timestamp, it is added to the processing list. After iterating through all timestamps, the merged data for the camera serial is stored and logged for validation.</p> <pre><code>        mp4_path = get_mp4_file(camera_file_group, camera_serial, pathutils)\n        if mp4_path is None:\n            logger.error(f\"No MP4 file found in group {timestamp}\")\n            continue\n\n        all_merged[\"mp4_file\"] = mp4_path\n        all_merged_list.append(all_merged)\n\n    if not all_merged_list:\n        logger.warning(f\"No valid merged data for {camera_serial}\")\n        continue\n\n    all_merged_df = pd.concat(all_merged_list, ignore_index=True)\n    logger.info(\n        f\"Final merged DataFrame for {camera_serial} head:\\n{all_merged_df.head()}\"\n    )\n    logger.info(\n        f\"Final merged DataFrame for {camera_serial} tail:\\n{all_merged_df.tail()}\"\n    )\n</code></pre>"},{"location":"developer-guide/program-flow/#after-processing-videos-synchronizing-and-exporting","title":"After Processing Videos: Synchronizing and Exporting","text":"<p>With the synchronized DataFrame ready, the script processes the corresponding video files. It iterates through each unique MP4 file and extracts relevant frames based on the filtered timestamps. Using <code>make_synced_subclip_ffmpeg()</code>, the script generates subclips, attaching the audio data at 30 kHz.</p> <pre><code>for camera_serial in camera_serials:\n    ...\n    # process the videos\n    video_output_dir = os.path.join(pathutils.output_dir, camera_serial)\n    os.makedirs(video_output_dir, exist_ok=True)\n    video_output_path = os.path.join(video_output_dir, \"output.mp4\")\n\n    subclip_paths = []\n    for mp4_path in all_merged_df[\"mp4_file\"].unique():\n        df_sub = all_merged_df[all_merged_df[\"mp4_file\"] == mp4_path]\n\n        # Build a subclip from the relevant frames, attach audio\n        subclip = make_synced_subclip_ffmpeg(\n            df_sub,\n            mp4_path,\n            fps_audio=30000,  # 30kHz\n            out_dir=os.path.join(pathutils.output_dir, camera_serial),\n        )\n        subclip_paths.append(subclip)\n</code></pre> <p>If multiple subclips are generated, they are concatenated into a single video using <code>ffmpeg_concat_mp4s()</code>. Finally, the fully synchronized video is saved to the output directory, completing the alignment process. This ensures that the final exported video is precisely synchronized with continuous audio amplitude signals.</p> <pre><code>    # Now 'subclip_paths' has each final MP4 subclip\n    # If we have only one, just rename or copy it\n    if len(subclip_paths) == 1:\n        final_path = subclip_paths[0]\n    else:\n        final_path = os.path.join(\n            pathutils.output_dir, camera_serial, f\"stitched_{camera_serial}.mp4\"\n        )\n        ffmpeg_concat_mp4s(subclip_paths, final_path)\n\n    logger.info(f\"Saved {camera_serial} to {video_output_path}\")\n</code></pre>"},{"location":"edge-cases/","title":"Edge Cases &amp; Real Examples","text":"<p>This section highlights real-world edge cases encountered when synchronizing video and neural data. Each example provides a problem description, a sample JSON snippet, analysis, and a possible solution.</p> <ul> <li>Frame Counter Anomalies</li> <li>Chunk Serial Discontinuities in JSON</li> <li>Frame ID Discontinuities in JSON</li> <li>JSON File Anomalies</li> </ul>"},{"location":"edge-cases/chunk_serial_json_discontinuity/","title":"2. Chunk Serial Discontinuities in JSON","text":""},{"location":"edge-cases/chunk_serial_json_discontinuity/#the-problem","title":"The Problem","text":"<p>When analyzing chunk serial data in the JSON files, discontinuities can occur in different forms. These discontinuities can affect downstream processing, requiring careful identification and handling. The types of discontinuities observed in the dataset are:</p> <ul> <li>Type I Discontinuity: The number drops to zero and then increases to a value greater than 1.</li> <li>Type II Discontinuity: The number resets from zero to 1.</li> <li>Type III Discontinuity: The difference between consecutive numbers is greater than 1.</li> <li>Type IV Discontinuity: The number hits -1.</li> </ul> <p>These discontinuities can impact the integrity of data streams, requiring careful detection and mitigation strategies.</p>"},{"location":"edge-cases/chunk_serial_json_discontinuity/#examples","title":"Examples","text":"<p>Type I discontinuity (YFC_20240719_091054.json)</p> <pre><code>\"chunk_serial_data\": [\n        20323583,\n        20323583,\n        20323583,\n        20323583,\n        20323583\n    ],\n    [\n        0,\n        0,\n        0,\n        0,\n        0\n    ],\n    [\n        20323585,\n        20323585,\n        20323585,\n        20323585,\n        20323585\n    ],\n</code></pre> <p>Type II discontinuity (YFC_20240719_091054.json)</p> <pre><code>\"chunk_serial_data\": [\n        20332543,\n        20332543,\n        20332543,\n        20332543,\n        20332543\n    ],\n    [\n        0,\n        0,\n        0,\n        0,\n        0\n    ],\n    [\n        1,\n        1,\n        1,\n        1,\n        1\n    ],\n    ...\n    [\n        127,\n        127,\n        127,\n        127,\n        127\n    ],\n    [\n        0,\n        0,\n        0,\n        0,\n        0\n    ],\n    [\n        20332673,\n        20332673,\n        20332673,\n        20332673,\n        20332673\n    ],\n</code></pre> <p>Type III discontinuities (YFB_20240504_052717.json)</p> <pre><code>\"chunk_serial_data\": [\n        30133802,\n        30133802,\n        30133803,\n        30133803,\n        30133803\n    ],\n    [\n        30133804,\n        30133804,\n        30133804,\n        30133804,\n        30133804\n    ],\n</code></pre> <p>Type IV discontinuities (YFC_20240719_091054.json)</p> <pre><code>\"chunk_serial_data\": [\n        20323571,\n        20323571,\n        -1,\n        -1,\n        -1\n    ],\n    [\n        20323572,\n        20323572,\n        20323572,\n        20323572,\n        20323572\n    ],\n</code></pre>"},{"location":"edge-cases/chunk_serial_json_discontinuity/#observation","title":"Observation","text":"<p>Running the script to detect discontinuities on YFC_20240719_091054.json, we get 2024-07-19_09:10:54_chunk_discontinuities.json</p> <p>By observation along with other json files:</p> <ul> <li>Type I discontinuities occur the most frequent with almost all JSON, with a structured gap pattern of 128 occurring repeatedly.</li> <li>Type II discontinuities occur usually once in each JSON. The chunk serial usually drops to 0, and gradually increases to 127, before dropping to 0 again and resuming from where it left off as if all those data were not missing.</li> <li>Type III discontinuities are rare but they do occur</li> <li>Type IV discontinuities often appear in the very beginning of file, suggesting its relation with booting up.</li> </ul>"},{"location":"edge-cases/chunk_serial_json_discontinuity/#conclusion","title":"Conclusion","text":"<p>The analysis highlights a strong pattern of Type I discontinuities, suggesting a structured reset mechanism rather than random errors. The gaps of 128 could indicate an intentional segmentation in the data stream, possibly due to a recording or transmission mechanism. Extra attention is required when handling chunk serial stream in JSON files.</p>"},{"location":"edge-cases/chunk_serial_json_discontinuity/#download-example-data","title":"Download Example Data","text":"<ul> <li>2024-07-19_09:10:54_chunk_discontinuities.json</li> <li>YFC_20240719_091054.json</li> <li>Jupyter Notebook to Reproduce Results</li> </ul>"},{"location":"edge-cases/frame_counter_anomalies/","title":"1. Frame Counter Anomalies","text":""},{"location":"edge-cases/frame_counter_anomalies/#the-problem","title":"The Problem","text":"<p>In some cases, the JSON metadata file may show frame counters that jump unexpectedly. However, the total frame count remains consistent across multiple verification methods, suggesting that frames are not actually lost.</p>"},{"location":"edge-cases/frame_counter_anomalies/#example-json-incorrect-frame-sequence","title":"Example JSON (Incorrect Frame Sequence)","text":"<pre><code>{\n  \"frames\": [\n    [19393, 19393, 19392, 19393, 19392],\n    [19400, 19400, 19399, 19400, 19399]\n  ]\n}\n</code></pre> <p>In this case, the frame IDs jump from 19392 to 19399, which may indicate a counter issue.</p>"},{"location":"edge-cases/frame_counter_anomalies/#observations","title":"Observations","text":"<p>The JSON metadata suggests that frames skip from 19392 \u2192 19399.</p> <p>However, when checking the total number of frames in the JSON:</p> <pre><code>&gt;&gt;&gt; len(yfb_json[\"frame_id\"])\n18000\n</code></pre> <p>it still returns 18000 frames, meaning that the frames exist but their numbering is inconsistent.</p> <p>Similarly, probing the corresponding MP4 video file also returns 18000 frames, further confirming that no frames are missing.</p>"},{"location":"edge-cases/frame_counter_anomalies/#conclusion","title":"Conclusion","text":"<p>This issue is not a missing frame problem, but rather a frame counter anomaly.</p>"},{"location":"edge-cases/frame_counter_anomalies/#download-reference-json-file","title":"Download Reference JSON File","text":"<ul> <li>YFB_20240505_133351.json</li> </ul>"},{"location":"edge-cases/frame_json_discontinuity/","title":"3. Frame ID Discontinuities in JSON","text":""},{"location":"edge-cases/frame_json_discontinuity/#the-problem","title":"The Problem","text":"<p>The 4 types of discontinuities can also occur in JSON frame ID</p>"},{"location":"edge-cases/frame_json_discontinuity/#examples","title":"Examples","text":"<p>Type III discontinuities (YFB_20240504_052717.json)</p> <pre><code>\"frame_id\": [\n        12656,\n        12656,\n        12657,\n        12657,\n        12657\n    ],\n    [\n        12658,\n        12658,\n        12658,\n        12658,\n        12658\n    ],\n</code></pre>"},{"location":"edge-cases/frame_json_discontinuity/#observation","title":"Observation","text":"<ul> <li>Type I discontinuities - no incidents detected so far</li> <li>Type II discontinuities - no incidents detected so far</li> <li>Type III discontinuities - typically occur infrequently among JSON frame id</li> <li>Type IV discontinuities - no incidents detected so far</li> </ul>"},{"location":"edge-cases/json_anomalies/","title":"4. JSON Anomalies","text":""},{"location":"edge-cases/json_anomalies/#the-problem","title":"The Problem","text":"<p>In some cases, JSON files within a group of camera recordings may have a slightly different timestamp than the corresponding MP4 files. In other cases, the JSON file may be missing entirely. This inconsistency can create issues when synchronizing video and metadata.</p>"},{"location":"edge-cases/json_anomalies/#examples","title":"Examples","text":""},{"location":"edge-cases/json_anomalies/#example-1-json-timestamp-mismatch","title":"Example 1: JSON Timestamp Mismatch","text":"<p>In this example, the JSON file has a different timestamp (163417), whereas all the MP4 files share another timestamp (163418). However, they should belong to the same recording session, as each recording spans 10 minutes.</p> <pre><code>[auto@elias video-sync]$ ls /mnt/datalake/data/emu/YFCDatafile/VIDEO/20240719/ | grep 20240719_1634\nYFC_20240719_163417.json\nYFC_20240719_163418.18486634.mp4\nYFC_20240719_163418.18486638.mp4\nYFC_20240719_163418.23512014.mp4\nYFC_20240719_163418.23512906.mp4\n</code></pre> <p>In this case, the JSON file appears to have been created slightly earlier than the MP4 files, likely due to the way timestamps are assigned during recording.</p>"},{"location":"edge-cases/json_anomalies/#example-2-missing-json-file","title":"Example 2: Missing JSON File","text":"<p>In this case, there is no JSON file for the group of MP4 files recorded at 153936.</p> <pre><code>[auto@elias video-sync]$ ls /mnt/datalake/data/emu/YFCDatafile/VIDEO/20240719/ | grep 20240726_1539\nYFC_20240726_153936.18486634.mp4\nYFC_20240726_153936.18486638.mp4\nYFC_20240726_153936.23512014.mp4\nYFC_20240726_153936.23512906.mp4\n</code></pre> <p>This suggests that the metadata file (JSON) was not generated, potentially due to an issue during recording or a forceful program exit.</p>"},{"location":"edge-cases/json_anomalies/#observations-and-possible-causes","title":"Observations and Possible Causes","text":"<ul> <li> <p>Program Termination: A forceful exit (e.g., process killed, unexpected shutdown) may prevent the JSON file from being written properly or fully.</p> </li> <li> <p>File Write Order: The JSON metadata file may be generated slightly before or after the video files, leading to a minor timestamp discrepancy.</p> </li> </ul>"},{"location":"edge-cases/json_anomalies/#potential-solutions","title":"Potential Solutions","text":"<ul> <li> <p>Post-Processing Fix: Implement a script to detect mismatched JSON files and reassign them to the correct MP4 group based on proximity.</p> </li> <li> <p>Ignore if the json files are missing.</p> </li> </ul>"},{"location":"understanding-data/understanding-nev/","title":"Understanding NEV","text":"<p>This guide provides a detailed explanation of the NEV (Neural Event) file structure, focusing on decoding digital event streams (<code>UnparsedData</code>) and aligning them with external triggers such as camera recordings. It is intended for researchers and engineers using Blackrock systems in multimodal experimental setups.</p>"},{"location":"understanding-data/understanding-nev/#nev-file-structure","title":"NEV File Structure","text":"<p>The <code>.nev</code> file format, developed by BlackRock Microsystems, is designed to store timestamped neural events such as spikes, TTL pulses, and serial messages. It supports up to 10,000 electrodes and includes both metadata and raw event data, structured to balance flexibility, efficiency, and ease of parsing.</p> <p>A NEV file consists of three main components:</p> <ul> <li>Basic Header</li> <li>Extended Headers</li> <li>Data Packets</li> </ul>"},{"location":"understanding-data/understanding-nev/#basic-header","title":"Basic Header","text":"<p>The basic header contains global metadata about the recording session, including the timestamp resolution and the origin of time. You can inspect it using the following method. Key fields:</p> <ul> <li>TimeStampResolution: Number of timestamp ticks per second (e.g., 30,000 means 1 tick = 33.3 \u03bcs).</li> <li>SampleTimeResolution: Used for waveform sampling resolution, often the same as TimeStampResolution.</li> <li>TimeOrigin: The UTC time when the recording started.</li> </ul> <pre><code>&gt;&gt;&gt; nev.get_basic_header()\n{'FileTypeID': 'BREVENTS',\n 'FileSpec': '3.0',\n 'AddFlags': 1,\n 'BytesInHeader': 26512,\n 'BytesInDataPackets': 108,\n 'TimeStampResolution': 30000,\n 'SampleTimeResolution': 30000,\n 'TimeOrigin': datetime.datetime(2024, 7, 17, 11, 55, 38, 670000),\n 'CreatingApplication': 'File Dialog v7.6.1',\n 'Comment': '',\n 'NumExtendedHeaders': 818}\n</code></pre>"},{"location":"understanding-data/understanding-nev/#extended-headers","title":"Extended Headers","text":"<p>Extended headers contain channel-specific metadata, such as labeling, filtering, and electrode properties. They are stored as a list of dictionaries. Each PacketID identifies the type of metadata (e.g., waveform parameters, labels, filter settings).</p> <pre><code>&gt;&gt;&gt; nev.get_extended_headers()\n[{'PacketID': 'NEUEVWAV',\n  'ElectrodeID': 1,\n  'PhysicalConnector': 1,\n  'ConnectorPin': 1,\n  'DigitizationFactor': 250,\n  'EnergyThreshold': 0,\n  'HighThreshold': 0,\n  'LowThreshold': -255,\n  'NumSortedUnits': 0,\n  'BytesPerWaveform': 2,\n  'SpikeWidthSamples': 48,\n  'EmptyBytes': b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'},\n {'PacketID': 'NEUEVLBL',\n  'ElectrodeID': 1,\n  'Label': 'LdPF-mPF01-001',\n  'EmptyBytes': b'\\x00\\x00\\x00\\x00\\x00\\x00'},\n {'PacketID': 'NEUEVFLT',\n  'ElectrodeID': 1,\n  'HighFreqCorner': '250.0 Hz',\n  'HighFreqOrder': 4,\n  'HighFreqType': 'butterworth',\n  'LowFreqCorner': '7500.0 Hz',\n  'LowFreqOrder': 3,\n  'LowFreqType': 'butterworth',\n  'EmptyBytes': b'\\x00\\x00'},\n  ...,\n]\n</code></pre>"},{"location":"understanding-data/understanding-nev/#data-packets","title":"Data Packets","text":"<p>The <code>.nev</code> file also contains the actual timestamped events, such as TTL pulses or encoded serial data.</p> <pre><code>nev = Nev(nev_path)\nnev_digital_events_df = nev.get_digital_events_df()\n</code></pre> InsertionReason TimeStamps UnparsedData 1 4.05746e+09 60414 1 4.05746e+09 60415 129 4.05746e+09 114 129 4.05746e+09 104 129 4.05746e+09 42 129 4.05746e+09 1 129 4.05746e+09 0 1 4.05746e+09 60414 1 4.05746e+09 60415 129 4.05746e+09 115 129 4.05746e+09 104 129 4.05746e+09 42 129 4.05746e+09 1 129 4.05746e+09 0 1 4.05746e+09 60414 1 4.05746e+09 60415 129 4.05746e+09 116 129 4.05746e+09 104 129 4.05746e+09 42 129 4.05746e+09 1 <p>Key columns:</p> <ul> <li>TimeStamps: Integer-based timestamps with TimeStampResolution frequency (e.g., 30,000 Hz).</li> <li>InsertionReason: An 8-bit flag indicating why the event was inserted (e.g., digital change, serial input).</li> <li>UnparsedData: The raw 16-bit digital input or 7-bit serial payload (depending on the flag).</li> </ul>"},{"location":"understanding-data/understanding-nev/#understanding-insertionreason","title":"Understanding <code>InsertionReason</code>","text":"<p>The <code>InsertionReason</code> field in the NEV file is a bitwise flag that encodes the reason a particular event was recorded. Each bit represents a specific condition or source for the event. Multiple bits may be set simultaneously.</p>"},{"location":"understanding-data/understanding-nev/#relevant-values","title":"Relevant Values","text":"Value Binary (8-bit) Meaning <code>1</code> <code>00000001</code> A digital channel changed state (e.g., camera trigger line toggled) <code>129</code> <code>10000001</code> A serial channel changed, and a digital change occurred (i.e., a serial byte was received)"},{"location":"understanding-data/understanding-nev/#bitwise-breakdown-from-blackrock-nev-specification","title":"Bitwise Breakdown (from BlackRock NEV Specification)","text":"<p>The <code>InsertionReason</code> byte is defined as follows:</p> Bit Meaning 0 Digital channel changed (e.g., rising/falling edge on a trigger line) 1 Event is from a strobed input 2\u20136 Reserved (unused) 7 Serial channel changed (must be set alongside Bit 0)"},{"location":"understanding-data/understanding-nev/#interpretation","title":"Interpretation","text":"<ul> <li><code>0b00000001</code> (decimal <code>1</code>) \u2192 Digital event only  </li> <li><code>0b10000001</code> (decimal <code>129</code>) \u2192 Serial event (with digital change)</li> </ul>"},{"location":"understanding-data/understanding-nev/#usage","title":"Usage","text":"<ul> <li>Use rows where <code>InsertionReason == 1</code> to extract digital trigger events (e.g., camera exposure).</li> <li>Use rows where <code>InsertionReason == 129</code> to reconstruct serial counter values sent from Arduino in 5-byte chunks.</li> </ul> <p>This distinction is crucial for synchronizing camera frames with external signals such as triggers or Arduino-based serial counters.</p>"},{"location":"understanding-data/understanding-nev/#understanding-unparseddata","title":"Understanding <code>UnparsedData</code>","text":""},{"location":"understanding-data/understanding-nev/#how-trigger-and-serial-are-sent-from-arduino","title":"How Trigger and Serial are sent from Arduino","text":"<p>The Arduino transmits both a trigger pulse and a serial-encoded counter value every frame:</p> <p>The trigger pulse is sent via a digital output pin (e.g., pin 13) to the camera to initiate image capture. Simultaneously, the counter value is incremented and transmitted over three serial ports \u2014 in the current setup, two going to the PCBs (used by the cameras) and one to the audio interface.</p> <p>To ensure compatibility with the camera's serial input requirements, the counter value (a 32-bit unsigned integer) is split into 5 separate bytes, with each byte containing 7 bits of actual data. This is done because:</p> <ul> <li>The camera expects each byte to begin with a start bit</li> <li>It can only receive one byte at a time, and only uses the lower 7 bits of each byte for data.</li> <li>Therefore, transmitting the full 32-bit value requires 5 bytes (since 5 \u00d7 7 = 35 bits), with the upper 3 bits simply unused if not needed.</li> </ul> <p>The splitting and transmission are handled by this Arduino function:</p> <pre><code>bool send_trigger_sync_to_pcb(void *) {\n  digitalWrite(trigger_pin, HIGH);\n\n  byte bytesToSend[5]; // Create an array to store the 5 bytes\n  // Split the 32-bit integer into 5 bytes, each carrying 7 bits\n  for (int i = 0; i &lt; 5; i++) {\n    // Shift right by 7 bits times the index and mask out the lower 7 bits\n    bytesToSend[i] = (count &gt;&gt; (7 * i)) &amp; 0x7F;\n  }\n  //Send each byte over serial\n  for (int i = 0; i &lt; 5; i++) {\n    Serial1.write(bytesToSend[i]);\n    Serial2.write(bytesToSend[i]);\n    Serial3.write(bytesToSend[i]);\n    Serial1.flush();\n    Serial2.flush();\n    Serial3.flush();\n  }\n  digitalWrite(trigger_pin, LOW);\n  count = count + 1;\n  return true;\n}\n</code></pre>"},{"location":"understanding-data/understanding-nev/#how-trigger-and-serial-are-encoded-in-unparseddata","title":"How Trigger and Serial are encoded in <code>UnparsedData</code>","text":"<p>The UnparsedData field in the NEV digital events dataframe holds different meanings depending on the value of the InsertionReason flag.</p> <p>For <code>InsertionReason == 1</code>, the <code>UnparsedData</code> value represents a 16-bit unsigned integer encoding the state of all 16 digital input lines. Each bit corresponds to a specific digital channel\u2014commonly used for camera exposure signals or TTL triggers. Since the NEV file only records changes in digital line state (i.e., rising or falling edges), the data is sparse in time. To reconstruct the square waveforms of digital signals (such as camera exposure or external triggers), the following steps are required:</p> <ul> <li>Filter digital events where <code>InsertionReason == 1</code>.</li> <li>Convert UnparsedData into a 16-bit binary representation.</li> <li>Fill in gaps between timestamps to create a continuous time axis, since intermediate bit states are not recorded.</li> <li>Extract the state of the specific bit corresponding to the channel of interest.</li> <li>Plot the decoded signal as a square wave.</li> </ul> <p>This process is encapsulated in the <code>nev.plot_cam_exposure_all</code> function, which generates clear, continuous exposure traces:</p> <p></p> <p>When <code>InsertionReason</code> is <code>129</code>, the UnparsedData value represents a 7-bit payload from serial communication. The Arduino transmits a 32-bit integer over serial by encoding it into five 7-bit bytes, each stored as a separate event with <code>InsertionReason == 129</code>.</p> <p>To decode the full serial message:</p> <ul> <li>Group every 5 consecutive rows with <code>InsertionReason == 129</code>.</li> <li>Extract the 7-bit data values from UnparsedData.</li> <li>Reconstruct the original 32-bit integer by reversing the bit-shifting process.</li> </ul> <p>Each such 5-row group represents a complete serial counter value synchronized with a trigger pulse. This data can then be used for precise alignment between camera frames and neural events.</p>"},{"location":"understanding-data/understanding-nev/#how-serial-number-is-reconstructed-from-5-bytes","title":"How Serial Number is reconstructed from 5 Bytes","text":"<p>In this system, a 32-bit integer counter is transmitted from the Arduino in the form of 5 bytes, where each byte encodes 7 bits of actual data. Both the camera system and the BlackRock recording system receive this data and must reconstruct the original 32-bit counter value.</p>"},{"location":"understanding-data/understanding-nev/#from-the-camera-side","title":"From the Camera Side","text":"<p>In the camera recording software, each byte is masked with 0x7F to retain only the lower 7 bits. The bytes are then shifted and combined to reconstruct the full 32-bit counter value.</p> <pre><code>def process_serial_data(self, c):\n    serial_msg = []\n    frame_count = -1\n    if self.gpio_settings['line3'] == 'SerialOn':\n        # We expect only 5 bytes to be sent\n        if c.ChunkSerialDataLength == 5:\n            chunk_serial_data = c.ChunkSerialData\n            serial_msg = chunk_serial_data\n            split_chunk = [ord(c) for c in chunk_serial_data]\n\n            frame_count = 0\n            for i, b in enumerate(split_chunk):\n                frame_count |= (b &amp; 0x7F) &lt;&lt; (7 * i)\n</code></pre>"},{"location":"understanding-data/understanding-nev/#from-the-nev-blackrock-side","title":"From the NEV (BlackRock) Side","text":"<p>On the BlackRock system, serial data is captured within the <code>.nev</code> file and appears in rows where <code>InsertionReason == 129</code>. Each complete serial transmission occupies 5 consecutive rows in the event stream.</p> <p>The NEV processing workflow involves:</p> <ul> <li>Filtering valid groups of 5 rows.</li> <li>Extracting the UnparsedData field from each row.</li> <li>Converting the 5 \u00d7 7-bit chunks back into a single integer.</li> <li>Apply fix-anomaly scripts to fill in the gaps of the serial stream (Refer to edge cases documentation).</li> </ul> <pre><code>def get_cleaned_digital_events_df(self):\n    \"\"\"\n    only keep the rows which satisfy\n    1. InsertionReason == 129\n    2. the length of such group is 5\n    3. 0 &lt;= UnparsedData &lt;= 127 (should be true enforced by hardware)\n\n    Returns\n        InsertionReason     TimeStamps  UnparsedData\n    2   129                 1345819     40\n    3   129                 1345822     76\n    4   129                 1345825     35\n    5   129                 1345828     0\n    6   129                 1345831     0\n    \"\"\"\n    digital_events_df = self.get_digital_events_df()\n    # True indicates a change from 1 -&gt; 129 or 129 -&gt; 1\n    digital_events_df[\"group\"] = (\n        digital_events_df[\"InsertionReason\"]\n        != digital_events_df[\"InsertionReason\"].shift(1)\n    ).cumsum()\n    # Count the size of each group and assign True where the group size\n    # is 5 and the reason is 129\n    digital_events_df[\"keeprows\"] = digital_events_df.groupby(\"group\")[\n        \"InsertionReason\"\n    ].transform(lambda x: (x == 129) &amp; (x.size == 5))\n    digital_events_df = digital_events_df[digital_events_df[\"keeprows\"] == True]\n    digital_events_df = digital_events_df.drop([\"group\", \"keeprows\"], axis=1)\n    return digital_events_df\n\ndef bits_to_decimal(self, nums: list) -&gt; int:\n    \"\"\"\n    nums: [19, 101, 37, 0, 0]\n\n    Returns:\n    619155\n    \"\"\"\n    # Convert each number to a 7-bit binary string with leading zeros\n    binary_strings = [format(num, \"07b\") for num in nums][::-1]\n    # Concatenate all binary strings into one long binary string\n    full_binary_string = \"\".join(binary_strings)\n    # Convert the concatenated binary string to a decimal number\n    return int(full_binary_string, 2)\n\ndef get_chunk_serial_df(self, timestamp_byte: str = \"first\"):\n    \"\"\"Reconstruct chunk serial numbers from grouped digital events.\n\n    Processes the cleaned digital events DataFrame by grouping every five consecutive rows,\n    reconstructing each chunk serial number from the grouped 7-bit encoded values, and\n    associating it with a corresponding timestamp. The timestamp used for each group\n    can be explicitly selected as either the first or last byte in the group.\n\n    Args:\n        timestamp_byte (str, optional): Which byte's timestamp to use ('first' or 'last').\n            Defaults to 'first'. Use 'last' if you want the timestamp representing\n            the full completion of the serial transmission (recommended for accurate synchronization).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing:\n            - `TimeStamps`: Timestamp from the NEV data (based on selected byte).\n            - `chunk_serial`: Reconstructed chunk serial number.\n            - `UTCTimeStamp`: Human-readable UTC timestamp.\n\n    Raises:\n        AssertionError: If unparsed data is unavailable or timestamp_byte parameter is invalid.\n\n    Example:\n        &gt;&gt;&gt; nev.get_chunk_serial_df(timestamp_byte='last')\n                TimeStamps  chunk_serial              UTCTimeStamp\n        0         1345819       583208  2024-04-16 21:48:17.195433\n        1         1346821       583209  2024-04-16 21:48:17.228833\n    \"\"\"\n    assert self.has_unparsed_data(), \"No unparsed data available.\"\n    assert timestamp_byte in [\n        \"first\",\n        \"last\",\n    ], \"timestamp_byte must be either 'first' or 'last'\"\n\n    df = self.get_cleaned_digital_events_df()\n    results = []\n\n    for i in range(0, len(df), 5):\n        group = df.iloc[i : i + 5]\n        if len(group) == 5:\n            nums = group[\"UnparsedData\"].tolist()\n            decimal_number = self.bits_to_decimal(nums)\n\n            # explicitly choose which byte's timestamp to use\n            if timestamp_byte == \"first\":\n                timestamp = group[\"TimeStamps\"].iloc[0]\n            else:  # timestamp_byte == 'last'\n                timestamp = group[\"TimeStamps\"].iloc[-1]\n\n            unix_time = ts2unix(\n                self.timeOrigin, self.timestampResolution, timestamp\n            )\n            results.append((timestamp, decimal_number, unix_time))\n\n    # Explicitly fill missing serials if necessary\n    results = fill_missing_serials_with_gap(results)\n\n    return pd.DataFrame.from_records(\n        results, columns=[\"TimeStamps\", \"chunk_serial\", \"UTCTimeStamp\"]\n    )\n</code></pre>"},{"location":"understanding-data/understanding-nev/#timing-between-trigger-and-serial","title":"Timing Between Trigger and Serial","text":"<p>According to the Arduino code, the trigger pulse is initiated immediately before the serial data transmission begins:</p> <pre><code>bool send_trigger_sync_to_pcb(void *) {\n  digitalWrite(trigger_pin, HIGH);\n\n  // code to sent serial...\n\n  digitalWrite(trigger_pin, LOW);\n  count = count + 1;\n  return true;\n}\n</code></pre> <p>When observing the NEV recording, we find that the trigger pulse is captured exactly one timestamp before the first byte of the 5-byte serial transmission. This confirms the ordering and tight timing between the two signals. This relationship is visualized in the following figure:</p> <p></p> <p>This precise sequencing is essential for synchronizing camera frames with neural recordings. The reliable 1-timestamp offset can be leveraged during analysis to align data streams accurately.</p>"}]}