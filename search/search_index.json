{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfa5 Video Sync","text":"<p>A Python tool to synchronize NSP data with camera recordings.</p> <p>Welcome to the official documentation for <code>video-sync</code>. This tool allows you to synchronize Neural Signal Processing (NSP) data with camera recordings. It processes NEV and NS5 files, aligns timestamps with camera JSON metadata, slices video based on valid frames, and synchronizes audio with video.</p>"},{"location":"#documentation-overview","title":"\ud83d\udcd6 Documentation Overview","text":"<ul> <li>Installation</li> <li>Configuration</li> <li>Usage Guide</li> <li>Features</li> <li>EMU Camera Serials</li> <li>License</li> </ul>"},{"location":"configuration/","title":"\u2699\ufe0f Configuration","text":"<p>Before running <code>video-sync</code>, you need to configure it using a YAML configuration file.</p> <p>A sample configuration file is provided in the repo and is also available to download below. Rename the example template to <code>config.yaml</code> and replace the paths to your desired paths</p> <p>The following command renames the example config file to <code>config.yaml</code> <pre><code>cp config.example.yaml config.yaml\n</code></pre></p> <p>\ud83d\udce5 Download config.example.yaml</p>"},{"location":"emu-cameras/","title":"\ud83d\udcf7 EMU Camera Serials","text":"<p>List of supported EMU camera serial numbers:</p> Camera Serial Position 18486634 F1 23512908 F2 18486644 F3 18486638 B1 23512014 B2 23512906 R1 23512012 R2 23505577 R3 <p>Use these serials when mapping cameras for video synchronization.</p>"},{"location":"features/","title":"\ud83c\udfd7\ufe0f Features","text":"<p><code>video-sync</code> provides the following features:</p> <p>\u2714\ufe0f Synchronizes NEV and NS5 files with camera recordings \u2714\ufe0f Slices video based on valid frames \u2714\ufe0f Aligns audio with video for precise synchronization \u2714\ufe0f Supports configurable processing options \u2714\ufe0f Efficient and scalable for large datasets  </p>"},{"location":"installation/","title":"\ud83d\udce5 Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing <code>video-sync</code>, ensure you have the following dependencies:</p> <ul> <li>Conda (for environment management)</li> <li>FFmpeg (for video and audio processing)</li> </ul>"},{"location":"installation/#installing-miniconda","title":"Installing Miniconda","text":"<p>Miniconda is the light-weight version of Conda, it's recommended for all OS. Visit the official Miniconda download page to download the installer for your OS.</p>"},{"location":"installation/#installing-ffmpeg","title":"Installing FFmpeg","text":"<p>For Ubuntu/Debian: <pre><code>sudo apt update\nsudo apt install ffmpeg\n</code></pre></p> <p>For RHEL (with EPEL enabled) <pre><code>sudo yum install epel-release\nsudo yum install ffmpeg\n</code></pre></p> <p>For MacOS</p> <p>Ensure Homebrew is installed, then run: <pre><code>brew install ffmpeg\n</code></pre></p> <p>For Windows</p> <p>Use Chocolatey (recommended): <pre><code>choco install ffmpeg\n</code></pre></p>"},{"location":"installation/#installing-video-sync","title":"Installing video-sync","text":"<p>Clone the repository:</p> <pre><code>git clone git@github.com:BCM-Neurosurgery/video-sync.git\ncd video-sync\n</code></pre> <p>Note: if you don't have permission, reach out to Yewen</p> <p>For EMU tasks, check out branch <code>stitch-videos-to-neuro</code> <pre><code>git checkout stitch-videos-to-neuro\n</code></pre></p> <p>For TRD tasks, checkout branch <code>trd</code> <pre><code>git checkout trd\n</code></pre></p> <p>Create and activate the Conda environment, then install dependencies:</p> <pre><code>conda env create -f environment.yml\nconda activate videosync\npip install .\n</code></pre>"},{"location":"license/","title":"\ud83d\udcdc License","text":"<p>This project is licensed under the BSD-3-Clause License.  </p>"},{"location":"usage/","title":"\ud83d\ude80 Usage Guide","text":""},{"location":"usage/#running-video-sync","title":"Running <code>video-sync</code>","text":"<p>Make sure the conda environment <code>videosync</code> is activated and run <pre><code>stitch-videos --config path/to/config.yaml\n</code></pre></p> <p>Example Command</p> <pre><code>stitch-videos --config /home/auto/CODE/utils/video-sync/Testing/YFITesting/config.yaml\n</code></pre>"},{"location":"api/","title":"\ud83d\udee0 API Reference","text":"<p>This section provides documentation for all scripts used in <code>video-sync</code>.</p> <ul> <li>DataPoool Module</li> <li>Nev Module</li> <li>Nsx Module</li> <li>Nsx Module</li> <li>Video Module</li> <li>Videojson Module</li> <li>Pathutils Module</li> <li>Utils Module</li> </ul>"},{"location":"api/datapool/","title":"\ud83d\udcc4 DataPool API Documentation","text":"<p>This section provides detailed documentation for <code>pyvideosync.data_pool</code>.</p>"},{"location":"api/datapool/#datapool-class","title":"\ud83d\udccc DataPool Class","text":"<p>Manages NSP and video data for integrity verification and statistics.</p> <p>Attributes:</p> Name Type Description <code>nsp_dir</code> <code>str</code> <p>Directory containing NSP files.</p> <code>cam_recording_dir</code> <code>str</code> <p>Directory containing camera recordings.</p> <code>nev_pool</code> <code>NevPool</code> <p>Stores NEV files.</p> <code>nsx_pool</code> <code>NsxPool</code> <p>Stores NS5/NS3 files.</p> <code>video_pool</code> <code>VideoPool</code> <p>Stores video files.</p> <code>video_json_pool</code> <code>VideoJsonPool</code> <p>Stores video metadata.</p> <code>video_file_pool</code> <code>VideoFilesPool</code> <p>Stores all video-related files.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class DataPool:\n    \"\"\"Manages NSP and video data for integrity verification and statistics.\n\n    Attributes:\n        nsp_dir (str): Directory containing NSP files.\n        cam_recording_dir (str): Directory containing camera recordings.\n        nev_pool (NevPool): Stores NEV files.\n        nsx_pool (NsxPool): Stores NS5/NS3 files.\n        video_pool (VideoPool): Stores video files.\n        video_json_pool (VideoJsonPool): Stores video metadata.\n        video_file_pool (VideoFilesPool): Stores all video-related files.\n    \"\"\"\n\n    def __init__(self, nsp_dir: str, cam_recording_dir: str) -&gt; None:\n        \"\"\"Initializes the DataPool class.\n\n        Args:\n            nsp_dir (str): Path to the NSP directory.\n            cam_recording_dir (str): Path to the camera recording directory.\n        \"\"\"\n        self.nsp_dir = nsp_dir\n        self.cam_recording_dir = cam_recording_dir\n        self.nev_pool = NevPool()\n        self.nsx_pool = NsxPool()\n        self.video_pool = VideoPool()\n        self.video_json_pool = VideoJsonPool()\n        self.video_file_pool = VideoFilesPool()\n        self.init_pools()\n\n    def init_pools(self):\n        \"\"\"Initializes the pools by:\n\n        Grouping the files in the video pool by timestamp.\n        \"\"\"\n        for taskfolder_path in Path(self.cam_recording_dir).iterdir():\n            if taskfolder_path.is_dir():\n                for datefolder_path in taskfolder_path.iterdir():\n                    if datefolder_path.is_dir():\n                        for file_path in datefolder_path.iterdir():\n                            self.video_file_pool.add_file(str(file_path.resolve()))\n\n    def verify_integrity(self) -&gt; bool:\n        \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one `.nev` file and one `.ns5` file.\n\n        Returns:\n            bool: True if exactly one `.nev` file and exactly one `.ns5` file are found, otherwise False.\n        \"\"\"\n        nev_count = 0\n        ns5_count = 0\n\n        for file in os.listdir(self.nsp_dir):\n            if fnmatch.fnmatch(file, \"*.nev\"):\n                nev_count += 1\n            elif fnmatch.fnmatch(file, \"*.ns5\"):\n                ns5_count += 1\n\n        return nev_count == 1 and ns5_count == 1\n\n    def get_nev_path(self) -&gt; str:\n        \"\"\"Finds the first NEV file in the directory.\n\n        Returns:\n            str: The full path of the first matching `.nev` file if found, otherwise an empty string.\n        \"\"\"\n        for file in os.listdir(self.nsp_dir):\n            if fnmatch.fnmatch(file, \"*.nev\"):\n                return os.path.join(self.nsp_dir, file)\n\n        return \"\"\n\n    def get_ns5_path(self) -&gt; str:\n        \"\"\"Finds the first NS5 file in the directory.\n\n        Returns:\n            str: The full path of the first matching `.ns5` file if found, otherwise an empty string.\n        \"\"\"\n        for file in os.listdir(self.nsp_dir):\n            if fnmatch.fnmatch(file, \"*.ns5\"):\n                return os.path.join(self.nsp_dir, file)\n\n        return \"\"\n\n    def get_video_file_pool(self) -&gt; \"VideoFilesPool\":\n        \"\"\"Retrieves the video file pool.\n\n        Returns:\n            VideoFilesPool: The video file pool object.\n        \"\"\"\n        return self.video_file_pool\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.cam_recording_dir","title":"<code>cam_recording_dir = cam_recording_dir</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nev_pool","title":"<code>nev_pool = NevPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nsp_dir","title":"<code>nsp_dir = nsp_dir</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.nsx_pool","title":"<code>nsx_pool = NsxPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_file_pool","title":"<code>video_file_pool = VideoFilesPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_json_pool","title":"<code>video_json_pool = VideoJsonPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.video_pool","title":"<code>video_pool = VideoPool()</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.__init__","title":"<code>__init__(nsp_dir: str, cam_recording_dir: str) -&gt; None</code>","text":"<p>Initializes the DataPool class.</p> <p>Parameters:</p> Name Type Description Default <code>nsp_dir</code> <code>str</code> <p>Path to the NSP directory.</p> required <code>cam_recording_dir</code> <code>str</code> <p>Path to the camera recording directory.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self, nsp_dir: str, cam_recording_dir: str) -&gt; None:\n    \"\"\"Initializes the DataPool class.\n\n    Args:\n        nsp_dir (str): Path to the NSP directory.\n        cam_recording_dir (str): Path to the camera recording directory.\n    \"\"\"\n    self.nsp_dir = nsp_dir\n    self.cam_recording_dir = cam_recording_dir\n    self.nev_pool = NevPool()\n    self.nsx_pool = NsxPool()\n    self.video_pool = VideoPool()\n    self.video_json_pool = VideoJsonPool()\n    self.video_file_pool = VideoFilesPool()\n    self.init_pools()\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_nev_path","title":"<code>get_nev_path() -&gt; str</code>","text":"<p>Finds the first NEV file in the directory.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full path of the first matching <code>.nev</code> file if found, otherwise an empty string.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_nev_path(self) -&gt; str:\n    \"\"\"Finds the first NEV file in the directory.\n\n    Returns:\n        str: The full path of the first matching `.nev` file if found, otherwise an empty string.\n    \"\"\"\n    for file in os.listdir(self.nsp_dir):\n        if fnmatch.fnmatch(file, \"*.nev\"):\n            return os.path.join(self.nsp_dir, file)\n\n    return \"\"\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_ns5_path","title":"<code>get_ns5_path() -&gt; str</code>","text":"<p>Finds the first NS5 file in the directory.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full path of the first matching <code>.ns5</code> file if found, otherwise an empty string.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_ns5_path(self) -&gt; str:\n    \"\"\"Finds the first NS5 file in the directory.\n\n    Returns:\n        str: The full path of the first matching `.ns5` file if found, otherwise an empty string.\n    \"\"\"\n    for file in os.listdir(self.nsp_dir):\n        if fnmatch.fnmatch(file, \"*.ns5\"):\n            return os.path.join(self.nsp_dir, file)\n\n    return \"\"\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.get_video_file_pool","title":"<code>get_video_file_pool() -&gt; 'VideoFilesPool'</code>","text":"<p>Retrieves the video file pool.</p> <p>Returns:</p> Name Type Description <code>VideoFilesPool</code> <code>'VideoFilesPool'</code> <p>The video file pool object.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_video_file_pool(self) -&gt; \"VideoFilesPool\":\n    \"\"\"Retrieves the video file pool.\n\n    Returns:\n        VideoFilesPool: The video file pool object.\n    \"\"\"\n    return self.video_file_pool\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.init_pools","title":"<code>init_pools()</code>","text":"<p>Initializes the pools by:</p> <p>Grouping the files in the video pool by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def init_pools(self):\n    \"\"\"Initializes the pools by:\n\n    Grouping the files in the video pool by timestamp.\n    \"\"\"\n    for taskfolder_path in Path(self.cam_recording_dir).iterdir():\n        if taskfolder_path.is_dir():\n            for datefolder_path in taskfolder_path.iterdir():\n                if datefolder_path.is_dir():\n                    for file_path in datefolder_path.iterdir():\n                        self.video_file_pool.add_file(str(file_path.resolve()))\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.DataPool.verify_integrity","title":"<code>verify_integrity() -&gt; bool</code>","text":"<p>Verifies the integrity of the directory by ensuring it contains exactly one <code>.nev</code> file and one <code>.ns5</code> file.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if exactly one <code>.nev</code> file and exactly one <code>.ns5</code> file are found, otherwise False.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def verify_integrity(self) -&gt; bool:\n    \"\"\"Verifies the integrity of the directory by ensuring it contains exactly one `.nev` file and one `.ns5` file.\n\n    Returns:\n        bool: True if exactly one `.nev` file and exactly one `.ns5` file are found, otherwise False.\n    \"\"\"\n    nev_count = 0\n    ns5_count = 0\n\n    for file in os.listdir(self.nsp_dir):\n        if fnmatch.fnmatch(file, \"*.nev\"):\n            nev_count += 1\n        elif fnmatch.fnmatch(file, \"*.ns5\"):\n            ns5_count += 1\n\n    return nev_count == 1 and ns5_count == 1\n</code></pre>"},{"location":"api/datapool/#supporting-classes","title":"\ud83d\udccc Supporting Classes","text":""},{"location":"api/datapool/#nevpool","title":"NevPool","text":"<p>Stores NEV files grouped by suffix.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class NevPool:\n    \"\"\"Stores NEV files grouped by suffix.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds a NEV file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        suffix = file.split(\"-\")[-1]\n        self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NevPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a NEV file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds a NEV file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    suffix = file.split(\"-\")[-1]\n    self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#nsxpool","title":"NsxPool","text":"<p>Stores NS5 and NS3 files grouped by suffix.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class NsxPool:\n    \"\"\"Stores NS5 and NS3 files grouped by suffix.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds an NS5/NS3 file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        suffix = file.split(\"-\")[-1]\n        self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.NsxPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds an NS5/NS3 file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds an NS5/NS3 file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    suffix = file.split(\"-\")[-1]\n    self.files[suffix].append(file)\n</code></pre>"},{"location":"api/datapool/#videopool","title":"VideoPool","text":"<p>Stores video files grouped by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class VideoPool:\n    \"\"\"Stores video files grouped by timestamp.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds a video file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        timestamp = file.split(\"_\")[-1].split(\".\")[0]\n        self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a video file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds a video file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    timestamp = file.split(\"_\")[-1].split(\".\")[0]\n    self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#videojsonpool","title":"VideoJsonPool","text":"<p>Stores video metadata JSON files grouped by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class VideoJsonPool:\n    \"\"\"Stores video metadata JSON files grouped by timestamp.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"\n        Adds a file to the internal dictionary, grouping by its timestamp.\n\n        The timestamp is extracted from the file name as the portion after the last underscore (`_`)\n        and before the file extension (`.`).\n\n        Example:\n            Given file names:\n            - \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n            - \"utsw_TRD011_day_1_20240716_152736.json\"\n\n            The extracted timestamps would be:\n            - \"154737\" from \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n            - \"152736\" from \"utsw_TRD011_day_1_20240716_152736.json\"\n\n            These files are then grouped by their extracted timestamp.\n\n        Args:\n            file (str): The file name including its extension.\n\n        \"\"\"\n        timestamp = file.split(\"_\")[-1].split(\".\")[0]\n        self.files[timestamp].append(file)\n\n    def list_groups(self) -&gt; dict[str, list[str]]:\n        \"\"\"Lists all groups of video metadata files.\n\n        Returns:\n            dict[str, list[str]]: A dictionary where keys are timestamps (str)\n            and values are lists of file names (str).\n        \"\"\"\n        return {timestamp: files for timestamp, files in self.files.items()}\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a file to the internal dictionary, grouping by its timestamp.</p> <p>The timestamp is extracted from the file name as the portion after the last underscore (<code>_</code>) and before the file extension (<code>.</code>).</p> Example <p>Given file names: - \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\" - \"utsw_TRD011_day_1_20240716_152736.json\"</p> <p>The extracted timestamps would be: - \"154737\" from \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\" - \"152736\" from \"utsw_TRD011_day_1_20240716_152736.json\"</p> <p>These files are then grouped by their extracted timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file name including its extension.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"\n    Adds a file to the internal dictionary, grouping by its timestamp.\n\n    The timestamp is extracted from the file name as the portion after the last underscore (`_`)\n    and before the file extension (`.`).\n\n    Example:\n        Given file names:\n        - \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n        - \"utsw_TRD011_day_1_20240716_152736.json\"\n\n        The extracted timestamps would be:\n        - \"154737\" from \"utsw_TRD011_day_1_20240716_154737.23512011.mp4\"\n        - \"152736\" from \"utsw_TRD011_day_1_20240716_152736.json\"\n\n        These files are then grouped by their extracted timestamp.\n\n    Args:\n        file (str): The file name including its extension.\n\n    \"\"\"\n    timestamp = file.split(\"_\")[-1].split(\".\")[0]\n    self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoJsonPool.list_groups","title":"<code>list_groups() -&gt; dict[str, list[str]]</code>","text":"<p>Lists all groups of video metadata files.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: A dictionary where keys are timestamps (str)</p> <code>dict[str, list[str]]</code> <p>and values are lists of file names (str).</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def list_groups(self) -&gt; dict[str, list[str]]:\n    \"\"\"Lists all groups of video metadata files.\n\n    Returns:\n        dict[str, list[str]]: A dictionary where keys are timestamps (str)\n        and values are lists of file names (str).\n    \"\"\"\n    return {timestamp: files for timestamp, files in self.files.items()}\n</code></pre>"},{"location":"api/datapool/#videofilespool","title":"VideoFilesPool","text":"<p>Stores all video-related files grouped by timestamp.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>class VideoFilesPool:\n    \"\"\"Stores all video-related files grouped by timestamp.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self.files = defaultdict(list)\n\n    def add_file(self, file: str):\n        \"\"\"Adds a video-related file to the pool.\n\n        Args:\n            file (str): File name to be added.\n        \"\"\"\n        timestamp = extract_timestamp(file)\n        self.files[timestamp].append(file)\n\n    def list_groups(self) -&gt; dict[str, list[str]]:\n        \"\"\"Lists groups of files sorted by timestamp.\n\n        Returns:\n            dict[str, list[str]]: A dictionary where keys are timestamps (str)\n            and values are lists of file names (str).\n        \"\"\"\n        return {timestamp: self.files[timestamp] for timestamp in sorted(self.files)}\n\n    def find_one_random_json(self) -&gt; str | None:\n        \"\"\"Finds a random JSON file in the pool.\n\n        Returns:\n            str: A JSON file name if found, otherwise None.\n        \"\"\"\n        for files in self.files.values():\n            for file in files:\n                if file.endswith(\".json\"):\n                    return file\n        return None\n\n    def get_unique_cam_serials(self) -&gt; set[str]:\n        \"\"\"\n        Returns a set of all unique camera serial numbers found in the filenames.\n\n        Returns:\n            set[str]: A set of unique camera serial numbers.\n        \"\"\"\n        serials = set()\n        for files in self.files.values():\n            for file in files:\n                if file.endswith(\".mp4\"):\n                    serial = extract_cam_serial(file)\n                if serial:\n                    serials.add(serial)\n        return serials\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.files","title":"<code>files = defaultdict(list)</code>  <code>instance-attribute</code>","text":""},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.__init__","title":"<code>__init__() -&gt; None</code>","text":"Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.files = defaultdict(list)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.add_file","title":"<code>add_file(file: str)</code>","text":"<p>Adds a video-related file to the pool.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>File name to be added.</p> required Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def add_file(self, file: str):\n    \"\"\"Adds a video-related file to the pool.\n\n    Args:\n        file (str): File name to be added.\n    \"\"\"\n    timestamp = extract_timestamp(file)\n    self.files[timestamp].append(file)\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.find_one_random_json","title":"<code>find_one_random_json() -&gt; str | None</code>","text":"<p>Finds a random JSON file in the pool.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>A JSON file name if found, otherwise None.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def find_one_random_json(self) -&gt; str | None:\n    \"\"\"Finds a random JSON file in the pool.\n\n    Returns:\n        str: A JSON file name if found, otherwise None.\n    \"\"\"\n    for files in self.files.values():\n        for file in files:\n            if file.endswith(\".json\"):\n                return file\n    return None\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.get_unique_cam_serials","title":"<code>get_unique_cam_serials() -&gt; set[str]</code>","text":"<p>Returns a set of all unique camera serial numbers found in the filenames.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>set[str]: A set of unique camera serial numbers.</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def get_unique_cam_serials(self) -&gt; set[str]:\n    \"\"\"\n    Returns a set of all unique camera serial numbers found in the filenames.\n\n    Returns:\n        set[str]: A set of unique camera serial numbers.\n    \"\"\"\n    serials = set()\n    for files in self.files.values():\n        for file in files:\n            if file.endswith(\".mp4\"):\n                serial = extract_cam_serial(file)\n            if serial:\n                serials.add(serial)\n    return serials\n</code></pre>"},{"location":"api/datapool/#pyvideosync.data_pool.VideoFilesPool.list_groups","title":"<code>list_groups() -&gt; dict[str, list[str]]</code>","text":"<p>Lists groups of files sorted by timestamp.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: A dictionary where keys are timestamps (str)</p> <code>dict[str, list[str]]</code> <p>and values are lists of file names (str).</p> Source code in <code>pyvideosync/data_pool.py</code> <pre><code>def list_groups(self) -&gt; dict[str, list[str]]:\n    \"\"\"Lists groups of files sorted by timestamp.\n\n    Returns:\n        dict[str, list[str]]: A dictionary where keys are timestamps (str)\n        and values are lists of file names (str).\n    \"\"\"\n    return {timestamp: self.files[timestamp] for timestamp in sorted(self.files)}\n</code></pre>"},{"location":"developer-guide/","title":"\ud83d\udee0\ufe0f Developer Guide","text":"<p>Welcome to the Developer Guide for <code>video-sync</code>. This section provides a deep dive into the internal workings of the project, covering:</p> <ul> <li>Program Flow: How the tool processes and synchronizes data.</li> <li>Contributing: How developers can contribute to the project.</li> </ul>"},{"location":"developer-guide/contributing/","title":"\ud83e\udd1d Contributing Guide","text":"<p>We welcome contributions to improve video-sync! Follow these steps to contribute:</p> <p>Note: This guide assumes basic familiarity with Git, Python, and command-line environments. </p>"},{"location":"developer-guide/contributing/#1-fork-clone-the-repository","title":"1: Fork &amp; Clone the Repository","text":"<p>Click the Fork button at the top right of the GitHub repository</p> <p>Clone your fork <pre><code>git clone https://github.com/YOUR_USERNAME/video-sync.git\ncd video-sync\n</code></pre></p> <p>Set the upstream remote</p> <pre><code>git remote add upstream https://github.com/bcm-neurosurgery/video-sync.git\n</code></pre>"},{"location":"developer-guide/contributing/#2-set-up-your-environment","title":"2: Set Up Your Environment","text":"<p>Please refer to installations guide</p> <p>Verify installation <pre><code>stitch-videos --help\n</code></pre></p>"},{"location":"developer-guide/contributing/#3-create-a-branch","title":"3: Create a Branch","text":"<p>Before making any changes, create a new branch:</p> <pre><code>git checkout -b feature/my-awesome-feature\n</code></pre> <p>For bug fixes, use:</p> <pre><code>git checkout -b fix/issue-123\n</code></pre> <p>Replace my-awesome-feature or issue-123 with a meaningful name.</p>"},{"location":"developer-guide/contributing/#4-make-changes-test","title":"4: Make Changes &amp; Test","text":"<p>Write your code and make changes. Run tests to ensure everything works:</p> <p>Format your code with: <pre><code>black .\n</code></pre></p>"},{"location":"developer-guide/contributing/#5-commit-and-push","title":"5: Commit and Push","text":"<p>Stage and commit your changes</p> <pre><code>git add .\ngit commit -m \"Add feature: my awesome feature\"\n</code></pre> <p>Push your branch to GitHub</p> <pre><code>git push origin feature/my-awesome-feature\n</code></pre>"},{"location":"developer-guide/contributing/#6-submit-a-pull-request","title":"6: Submit a Pull Request","text":"<p>Go to your fork on GitHub. Click the \"New Pull Request\" button. Select your branch and compare it with the main branch. Write a clear description of your changes. Click \"Create Pull Request\".</p> <p>Yewen will review your code and provide feedback if needed.</p>"},{"location":"developer-guide/contributing/#7-keep-your-fork-updated","title":"7: Keep Your Fork Updated","text":"<p>To avoid merge conflicts, keep your local repository updated with the latest changes:</p> <pre><code>git checkout main\ngit pull upstream main\ngit checkout feature/my-awesome-feature\ngit merge main\n</code></pre>"},{"location":"developer-guide/contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<ul> <li>Follow PEP 8 for Python code style.</li> <li>Write clear commit messages.</li> <li>Keep pull requests focused\u2014one feature or bug fix per PR.</li> <li>Add comments and docstrings where necessary.</li> <li>If modifying behavior, update documentation and tests.</li> </ul> <p>Happy coding! Thank you for improving video-sync!</p>"},{"location":"developer-guide/program-flow/","title":"\ud83d\udd04 Program Flow","text":"<p>This section explains the data processing flow of <code>video-sync</code>.</p>"},{"location":"developer-guide/program-flow/#1-configuration","title":"1. Configuration","text":"<p>In main.py, we load and validate the configuration using the <code>PathUtils</code> class.</p> <pre><code>timestamp = get_current_ts()\n\npathutils = PathUtils(config_path, timestamp)\nlogger = configure_logging(pathutils.output_dir)\n\nif not pathutils.is_config_valid():\n    logger.error(\"Config not valid, exiting to inital screen...\")\n    return\n</code></pre> <p>This ensures the configuration is valid before proceeding with synchronization.</p>"},{"location":"developer-guide/program-flow/#2-data-integrity-check","title":"2. Data Integrity Check","text":"<p>The DataPool class ensures all required neural and camera files are present.</p> <p>If files are missing, the process stops.</p> <pre><code>datapool = DataPool(pathutils.nsp_dir, pathutils.cam_recording_dir)\n\nif not datapool.verify_integrity():\n    logger.error(\n        \"File integrity check failed: Missing or duplicate NSP files detected. \"\n        \"Please verify the directory structure and try again. Returning to the initial screen.\"\n    )\n    return\n</code></pre>"},{"location":"developer-guide/program-flow/#3-extracting-neural-data-nev","title":"3. Extracting Neural Data (NEV)","text":"<p>This step extracts and reconstructs chunk serial data in the format of a dataframe from the stitched NSP-1 <code>.nev</code> file, which contains event-based neural data. The chunk serial values are reconstructed by combining five split chunks of serial communication sent from an Arduino.</p> <pre><code># 1. Get NEV serial start and end\nnsp1_nev_path = datapool.get_nsp1_nev_path()\nnev = Nev(nsp1_nev_path)\nnev_chunk_serial_df = nev.get_chunk_serial_df()\nlogger.info(f\"NEV dataframe\\n: {nev_chunk_serial_df}\")\nnev_start_serial, nev_end_serial = get_column_min_max(\n    nev_chunk_serial_df, \"chunk_serial\"\n)\nlogger.info(f\"Start serial: {nev_start_serial}, End serial: {nev_end_serial}\")\n</code></pre> <p>The script first retrieves the NSP-1 <code>.nev</code> file path and initializes an instance of the Nev class to parse its contents. It then calls <code>get_chunk_serial_df()</code>, which reconstructs the chunk serials by combining the five split parts. This reconstructed DataFrame provides a sequential timeline of neural events, essential for aligning with video data.</p> <p>To establish the valid time range for synchronization, the script determines the earliest and latest chunk serial values from the NEV data using <code>get_column_min_max()</code>. These values define the window in which video frames should be extracted later to ensure proper alignment.</p>"},{"location":"developer-guide/program-flow/#4-identifying-relevant-video-and-metadata-files-for-synchronization","title":"4. Identifying Relevant Video and Metadata Files for Synchronization","text":"<p>To ensure proper alignment between neural and video data, the script identifies which camera recordings overlap with the neural event (NEV) time range. Each video recording has an associated JSON metadata file containing start and end chunk serials, which are extracted and compared against the NEV serial range.</p> <p>To optimize performance, the script first checks if previously computed timestamps exist in <code>timestamps.json</code>. If found, these timestamps are used directly to skip redundant processing. Otherwise, the script iterates through all available JSON metadata files, extracting their chunk serials and determining whether they overlap with the neural recording. If a video\u2019s serial range falls within the NEV range, its timestamp is added to the processing list.</p> <p>Once all relevant timestamps are identified, they are saved to <code>timestamps.json</code> for future runs and sorted to maintain chronological order. This approach ensures that only the necessary video files are processed, reducing computational overhead while maintaining precise synchronization.</p> <pre><code># 3. load camera serials from the config file\ncamera_serials = pathutils.cam_serial\nlogger.info(f\"Camera serials loaded from config: {camera_serials}\")\n\n# 4. Go through all JSON files and find the ones that\n# are within the NEV serial range\n# read timestamps if available\ntimestamps_path = os.path.join(pathutils.output_dir, \"timestamps.json\")\ntimestamps = load_timestamps(timestamps_path, logger)\nif timestamps:\n    logger.info(f\"Loaded timestamps: {timestamps}\")\nelse:\n    logger.info(\"No timestamps found\")\n    timestamps = []\n    for timestamp, camera_file_group in camera_files.items():\n\n        json_path = get_json_file(camera_file_group, pathutils)\n        if json_path is None:\n            logger.error(f\"No JSON file found in group {timestamp}\")\n            continue\n        videojson = Videojson(json_path)\n        start_serial, end_serial = videojson.get_min_max_chunk_serial()\n        if start_serial is None or end_serial is None:\n            logger.error(f\"No chunk serials found in JSON file: {json_path}\")\n            continue\n\n        if end_serial &lt; nev_start_serial:\n            logger.info(f\"No overlap found: {timestamp}\")\n            continue\n\n        elif start_serial &lt;= nev_end_serial:\n            logger.info(f\"Overlap found, timestamp: {timestamp}\")\n            timestamps.append(timestamp)\n\n        else:\n            logger.info(f\"Break: {timestamp}\")\n            break\n    logger.info(f\"timestamps: {timestamps}\")\n    save_timestamps(timestamps_path, timestamps)\n\nsorted_timestamps = sort_timestamps(timestamps)\n</code></pre>"},{"location":"developer-guide/program-flow/#5-processing-videos-for-synchronization","title":"5. Processing Videos for Synchronization","text":"<p>Once the relevant timestamps are identified, this part of the script processes video files to align them with neural data. The workflow can be divided into two phases:</p> <ul> <li>Before Processing Videos \u2192 Extract and merge neural and video data.</li> <li>After Processing Videos \u2192 Generate subclips, add synchronized audio, and export the final video.</li> </ul>"},{"location":"developer-guide/program-flow/#before-processing-videos-extracting-and-merging-data","title":"Before Processing Videos: Extracting and Merging Data","text":"<p>The script iterates over each camera serial number and processes the corresponding video recordings. For each timestamp, it loads the associated camera metadata JSON file, extracts frame information, and filters the frames that overlap with the NEV chunk serial range.</p> <pre><code># 5. Go through the timestamps and process the videos\nfor camera_serial in camera_serials:\n    all_merged_list = []\n\n    for i, timestamp in enumerate(sorted_timestamps):\n        camera_file_group = camera_files[timestamp]\n\n        json_path = get_json_file(camera_file_group, pathutils)\n        if json_path is None:\n            logger.error(f\"No JSON file found in group {timestamp}\")\n            continue\n\n        videojson = Videojson(json_path)\n        camera_df = videojson.get_camera_df(camera_serial)\n        camera_df[\"frame_ids_relative\"] = (\n            camera_df[\"frame_ids_reconstructed\"]\n            - camera_df[\"frame_ids_reconstructed\"].iloc[0]\n            + 1\n        )\n\n        camera_df = camera_df.loc[\n            (camera_df[\"chunk_serial_data\"] &gt;= nev_start_serial)\n            &amp; (camera_df[\"chunk_serial_data\"] &lt;= nev_end_serial)\n        ]\n</code></pre> <p>The filtered camera frame data is then merged with the NEV chunk serials on serial to create a synchronized dataset. Next, the script extracts continuous neural/audio signals (NS5 data) for the same time window and merges them with the existing dataset. This results in a combined DataFrame containing timestamped neural/audio data, camera frame IDs, and amplitudes from the NS5 file.</p> <p>Note: the audio signal and the neural data are all arrays in the same format in NS5, so they can be processed in the same way.</p> <pre><code>        chunk_serial_joined = nev_chunk_serial_df.merge(\n            camera_df,\n            left_on=\"chunk_serial\",\n            right_on=\"chunk_serial_data\",\n            how=\"inner\",\n        )\n\n        logger.info(\"Processing ns5 filtered channel df...\")\n        ns5_slice = ns5.get_filtered_channel_df(\n            pathutils.ns5_channel,\n            chunk_serial_joined.iloc[0][\"TimeStamps\"],\n            chunk_serial_joined.iloc[-1][\"TimeStamps\"],\n        )\n\n        logger.info(\"Merging ns5 and chunk serial df...\")\n        all_merged = ns5_slice.merge(\n            chunk_serial_joined,\n            left_on=\"TimeStamp\",\n            right_on=\"TimeStamps\",\n            how=\"left\",\n        )\n\n        all_merged = all_merged[\n            [\n                \"TimeStamp\",\n                \"Amplitude\",\n                \"chunk_serial\",\n                \"frame_id\",\n                \"frame_ids_reconstructed\",\n                \"frame_ids_relative\",\n            ]\n        ]\n</code></pre> <p>If a matching MP4 file is found for the timestamp, it is added to the processing list. After iterating through all timestamps, the merged data for the camera serial is stored and logged for validation.</p> <pre><code>        mp4_path = get_mp4_file(camera_file_group, camera_serial, pathutils)\n        if mp4_path is None:\n            logger.error(f\"No MP4 file found in group {timestamp}\")\n            continue\n\n        all_merged[\"mp4_file\"] = mp4_path\n        all_merged_list.append(all_merged)\n\n    if not all_merged_list:\n        logger.warning(f\"No valid merged data for {camera_serial}\")\n        continue\n\n    all_merged_df = pd.concat(all_merged_list, ignore_index=True)\n    logger.info(\n        f\"Final merged DataFrame for {camera_serial} head:\\n{all_merged_df.head()}\"\n    )\n    logger.info(\n        f\"Final merged DataFrame for {camera_serial} tail:\\n{all_merged_df.tail()}\"\n    )\n</code></pre>"},{"location":"developer-guide/program-flow/#after-processing-videos-synchronizing-and-exporting","title":"After Processing Videos: Synchronizing and Exporting","text":"<p>With the synchronized DataFrame ready, the script processes the corresponding video files. It iterates through each unique MP4 file and extracts relevant frames based on the filtered timestamps. Using <code>make_synced_subclip_ffmpeg()</code>, the script generates subclips, attaching the audio data at 30 kHz.</p> <pre><code>for camera_serial in camera_serials:\n    ...\n    # process the videos\n    video_output_dir = os.path.join(pathutils.output_dir, camera_serial)\n    os.makedirs(video_output_dir, exist_ok=True)\n    video_output_path = os.path.join(video_output_dir, \"output.mp4\")\n\n    subclip_paths = []\n    for mp4_path in all_merged_df[\"mp4_file\"].unique():\n        df_sub = all_merged_df[all_merged_df[\"mp4_file\"] == mp4_path]\n\n        # Build a subclip from the relevant frames, attach audio\n        subclip = make_synced_subclip_ffmpeg(\n            df_sub,\n            mp4_path,\n            fps_audio=30000,  # 30kHz\n            out_dir=os.path.join(pathutils.output_dir, camera_serial),\n        )\n        subclip_paths.append(subclip)\n</code></pre> <p>If multiple subclips are generated, they are concatenated into a single video using <code>ffmpeg_concat_mp4s()</code>. Finally, the fully synchronized video is saved to the output directory, completing the alignment process. This ensures that the final exported video is precisely synchronized with continuous audio amplitude signals.</p> <pre><code>    # Now 'subclip_paths' has each final MP4 subclip\n    # If we have only one, just rename or copy it\n    if len(subclip_paths) == 1:\n        final_path = subclip_paths[0]\n    else:\n        final_path = os.path.join(\n            pathutils.output_dir, camera_serial, f\"stitched_{camera_serial}.mp4\"\n        )\n        ffmpeg_concat_mp4s(subclip_paths, final_path)\n\n    logger.info(f\"Saved {camera_serial} to {video_output_path}\")\n</code></pre>"},{"location":"understanding-anomalies/discontinuity-in-json/","title":"Discontinuity in Json","text":""},{"location":"understanding-anomalies/discontinuity-in-json/#discontinuity-of-serial","title":"Discontinuity of Serial","text":""},{"location":"understanding-anomalies/discontinuity-in-json/#the-problem","title":"The Problem","text":"<p>When analyzing <code>chunk_serial_data</code> from JSON metadata files, various forms of discontinuities may appear. These discontinuities can impact synchronization and downstream processing, requiring explicit handling. We categorize them as follows:</p> <ul> <li>Type I Discontinuity: Serial number drops to 0 and resumes from a value greater than 1.</li> <li>Type II Discontinuity: Serial resets from 0 to 1 and counts up (e.g., 1 \u2192 127 \u2192 0), then jumps back to original stream.</li> <li>Type III Discontinuity: Consecutive serial numbers differ by more than 1 (i.e., skipped values).</li> <li>Type IV Discontinuity: Serial number is <code>-1</code>, indicating a failed read or missing chunk.</li> </ul> <p>These patterns are artifacts of the encoding and buffering system and do not necessarily indicate data loss.</p>"},{"location":"understanding-anomalies/discontinuity-in-json/#examples","title":"Examples","text":""},{"location":"understanding-anomalies/discontinuity-in-json/#type-i-discontinuity","title":"Type I Discontinuity","text":"<p>YFC_20240719_091054.json</p> <pre><code>\"chunk_serial_data\": [\n        20323583,\n        20323583,\n        20323583,\n        20323583,\n        20323583\n    ], \n    [\n        0,\n        0,\n        0,\n        0,\n        0\n    ],\n    [\n        20323585,\n        20323585,\n        20323585,\n        20323585,\n        20323585\n    ],\n</code></pre>"},{"location":"understanding-anomalies/discontinuity-in-json/#type-ii-discontinuity","title":"Type II Discontinuity","text":"<p>YFC_20240719_091054.json</p> <pre><code>\"chunk_serial_data\": [\n        20332543,\n        20332543,\n        20332543,\n        20332543,\n        20332543\n    ],\n    [\n        0,\n        0,\n        0,\n        0,\n        0\n    ],\n    [\n        1,\n        1,\n        1,\n        1,\n        1\n    ],\n    ...\n    [\n        127,\n        127,\n        127,\n        127,\n        127\n    ],\n    [\n        0,\n        0,\n        0,\n        0,\n        0\n    ],\n    [\n        20332673,\n        20332673,\n        20332673,\n        20332673,\n        20332673\n    ],\n</code></pre>"},{"location":"understanding-anomalies/discontinuity-in-json/#type-iii-discontinuity","title":"Type III Discontinuity","text":"<p>YFB_20240504_052717.json</p> <pre><code>\"chunk_serial_data\": [\n        30133802,\n        30133802,\n        30133803,\n        30133803,\n        30133803\n    ],\n    [\n        30133804,\n        30133804,\n        30133804,\n        30133804,\n        30133804\n    ],\n</code></pre>"},{"location":"understanding-anomalies/discontinuity-in-json/#type-iv-discontinuity","title":"Type IV Discontinuity","text":"<p>YFC_20240719_091054.json</p> <pre><code>\"chunk_serial_data\": [\n        20323571,\n        20323571,\n        -1,\n        -1,\n        -1\n    ],\n    [\n        20323572,\n        20323572,\n        20323572,\n        20323572,\n        20323572\n    ],\n</code></pre> <p>When <code>chunk_serial_data == -1</code>, it is explicitly set by the acquisition software when serial data cannot be decoded correctly. See:</p> <pre><code>frame_count = -1\nif self.gpio_settings['line3'] == 'SerialOn':\n    # We expect only 5 bytes to be sent\n    if c.ChunkSerialDataLength == 5:\n        chunk_serial_data = c.ChunkSerialData\n        serial_msg = chunk_serial_data\n        split_chunk = [ord(c) for c in chunk_serial_data]\n\n        # Reconstruct the current count from the chunk serial data\n        frame_count = 0\n        for i, b in enumerate(split_chunk):\n            frame_count |= (b &amp; 0x7F) &lt;&lt; (7 * i)\n\nframe_metadata[\"chunk_serial_data\"].append(frame_count)\n</code></pre>"},{"location":"understanding-anomalies/discontinuity-in-json/#observation","title":"Observation","text":"<ul> <li>Type I discontinuities appear frequently and follow a repeated gap pattern of 128, possibly due to buffer handling or chunk boundaries.</li> <li>Type II often occurs once per JSON, where a separate counting sequence briefly replaces the main stream.</li> <li>Type III skips are rare but detectable.</li> <li>Type IV values (-1) are often present at the beginning of the file or during transient serial glitches.</li> </ul> <p>Despite these anomalies, the total number of chunk serial entries matches the duration of the recording, indicating no frames are lost:</p> <pre><code>&gt;&gt;&gt; with open(json_path, \"r\") as f:\n&gt;&gt;&gt;     json_dic = json.load(f)\n&gt;&gt;&gt; len(json_dic[\"chunk_serial_data\"])\n36000\n</code></pre>"},{"location":"understanding-anomalies/discontinuity-in-json/#conclusion","title":"Conclusion","text":"<p>The analysis highlights a strong pattern of Type I discontinuities, suggesting a structured reset mechanism rather than random errors. The gaps of 128 could indicate an intentional segmentation in the data stream, possibly due to a recording or transmission mechanism.</p> <p>In my implementation, I used the following function in <code>pyvideosync.fixanomaly</code> to correct the stream and produce continuous chunk serials and frame IDs:</p> <pre><code>df = pd.DataFrame.from_records(res)\ndf = self.reconstruct_frame_id(df)\n\n# Fix Type I and Type II discontinuities\nchunk_serial_fixed = fix_discontinuities(df[\"chunk_serial_data\"].tolist())\nframe_id_fixed = fix_discontinuities(df[\"frame_ids_reconstructed\"].tolist())\n\n# Fix Type III jumps and Type IV -1s\ncontinuous_chunk_serial = fill_array_gaps(chunk_serial_fixed)\ncontinuous_frame_ids = fill_array_gaps(frame_id_fixed)\n</code></pre>"},{"location":"understanding-anomalies/discontinuity-in-json/#download-example-data","title":"Download Example Data","text":"<ul> <li>2024-07-19_09:10:54_chunk_discontinuities.json</li> <li>YFC_20240719_091054.json</li> <li>Jupyter Notebook to Reproduce Results</li> </ul>"},{"location":"understanding-anomalies/discontinuity-in-json/#discontinuity-of-frame-id","title":"Discontinuity of Frame ID","text":""},{"location":"understanding-anomalies/discontinuity-in-json/#the-problem_1","title":"The Problem","text":"<p>In some cases, the JSON metadata file shows frame counters that jump unexpectedly. However, the total frame count remains consistent across multiple verification methods, suggesting that frames are not actually lost, but rather the frame ID counter has anomalies.</p>"},{"location":"understanding-anomalies/discontinuity-in-json/#examples_1","title":"Examples","text":""},{"location":"understanding-anomalies/discontinuity-in-json/#type-iii-discontinuity_1","title":"Type III Discontinuity","text":"<p><code>YFB_20240504_052717.json</code></p> <pre><code>{\n  \"frames\": [\n    [19393, 19393, 19392, 19393, 19392],\n    [19400, 19400, 19399, 19400, 19399]\n  ]\n}\n</code></pre> <p>In this case, the frame IDs jump from <code>19392 \u2192 19399</code>, indicating a possible counter skip.</p>"},{"location":"understanding-anomalies/discontinuity-in-json/#special-discontinuity-overflow","title":"Special Discontinuity (Overflow)","text":"<p><code>YFB_20240504_052717.json</code></p> <pre><code>[\n    65534, 65534, 65535, 65535, 65535\n],\n[\n    65535, 65535, 1, 1, 1\n],\n[\n    1, 1, 2, 2, 2\n],\n[\n    2, 2, 3, 3, 3\n]\n</code></pre> <p>This suggests a 16-bit counter overflow where the frame ID resets after reaching <code>65535</code>.</p>"},{"location":"understanding-anomalies/discontinuity-in-json/#observations","title":"Observations","text":"<p>Even when frame IDs show discontinuities:</p> <pre><code>&gt;&gt;&gt; len(yfb_json[\"frame_id\"])\n18000\n</code></pre> <p>The count still returns <code>18000</code> frames, indicating no data loss.</p> <p>Verifying the video file with <code>ffmpeg</code> confirms the full frame count:</p> <pre><code>import subprocess\n\ndef count_frames_ffmpeg(video_path: str) -&gt; int:\n    cmd = [\n        \"ffprobe\", \"-v\", \"error\", \"-select_streams\", \"v:0\",\n        \"-count_packets\", \"-show_entries\", \"stream=nb_read_packets\",\n        \"-of\", \"csv=p=0\", video_path\n    ]\n    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n    return int(result.stdout.strip())\n</code></pre> <p>Alternatively, with OpenCV:</p> <pre><code>self.frame_count = int(self.capture.get(cv2.CAP_PROP_FRAME_COUNT))\n</code></pre> <p>Both methods confirm the actual number of frames rendered in the MP4 file matches expectations.</p>"},{"location":"understanding-anomalies/discontinuity-in-json/#conclusion_1","title":"Conclusion","text":"<p>The irregularities in <code>frame_id</code> (such as jumps or resets) are best understood as counter inconsistencies, not actual frame drops.</p> <ul> <li>Total frame count is preserved in both metadata and video.</li> <li>Frame ID jumps (e.g., <code>19392 \u2192 19399</code>) or overflows (e.g., <code>65535 \u2192 1</code>) are internal to the acquisition system or serialization process.</li> <li>This issue is a metadata anomaly, not a data loss or sync error.</li> </ul> <p>These anomalies do not affect the video content and can be corrected or ignored during post-processing.</p>"},{"location":"understanding-anomalies/discontinuity-in-json/#download-reference-json-file","title":"Download Reference JSON File","text":"<ul> <li><code>YFB_20240505_133351.json</code></li> </ul>"},{"location":"understanding-anomalies/json-diff-timestamp/","title":"Discontinuity in Json","text":""},{"location":"understanding-anomalies/json-diff-timestamp/#the-problem","title":"The Problem","text":"<p>In some cases, JSON files within a group of camera recordings may have a slightly different timestamp than the corresponding MP4 files. In other cases, the JSON file may be missing entirely. This inconsistency can create issues when synchronizing video and metadata.</p>"},{"location":"understanding-anomalies/json-diff-timestamp/#examples","title":"Examples","text":""},{"location":"understanding-anomalies/json-diff-timestamp/#example-1-json-timestamp-mismatch","title":"Example 1: JSON Timestamp Mismatch","text":"<p>In this example, the JSON file has a different timestamp (163417), whereas all the MP4 files share another timestamp (163418). However, they should belong to the same recording session, as each recording spans 10 minutes.</p> <pre><code>[auto@elias video-sync]$ ls /mnt/datalake/data/emu/YFCDatafile/VIDEO/20240719/ | grep 20240719_1634\nYFC_20240719_163417.json\nYFC_20240719_163418.18486634.mp4\nYFC_20240719_163418.18486638.mp4\nYFC_20240719_163418.23512014.mp4\nYFC_20240719_163418.23512906.mp4\n</code></pre> <p>In this case, the JSON file appears to have been created slightly earlier than the MP4 files, likely due to the way timestamps are assigned during recording.</p>"},{"location":"understanding-anomalies/json-diff-timestamp/#example-2-missing-json-file","title":"Example 2: Missing JSON File","text":"<p>In this case, there is no JSON file for the group of MP4 files recorded at 153936.</p> <pre><code>[auto@elias video-sync]$ ls /mnt/datalake/data/emu/YFCDatafile/VIDEO/20240719/ | grep 20240726_1539\nYFC_20240726_153936.18486634.mp4\nYFC_20240726_153936.18486638.mp4\nYFC_20240726_153936.23512014.mp4\nYFC_20240726_153936.23512906.mp4\n</code></pre> <p>This suggests that the metadata file (JSON) was not generated, potentially due to an issue during recording or a forceful program exit.</p>"},{"location":"understanding-anomalies/json-diff-timestamp/#observations-and-possible-causes","title":"Observations and Possible Causes","text":"<ul> <li> <p>Program Termination: A forceful exit (e.g., process killed, unexpected shutdown) may prevent the JSON file from being written properly or fully.</p> </li> <li> <p>File Write Order: The JSON metadata file may be generated slightly before or after the video files, leading to a minor timestamp discrepancy.</p> </li> </ul>"},{"location":"understanding-anomalies/json-diff-timestamp/#potential-solutions","title":"Potential Solutions","text":"<ul> <li> <p>Post-Processing Fix: Implement a script to detect mismatched JSON files and reassign them to the correct MP4 group based on proximity.</p> </li> <li> <p>Ignore if the json files are missing.</p> </li> </ul>"},{"location":"understanding-data/understanding-json/","title":"Understanding JSON","text":"<p>This section explains how JSON metadata is generated and saved during acquisition using the multi-camera recording system.</p>"},{"location":"understanding-data/understanding-json/#how-json-is-saved","title":"How JSON is Saved","text":"<p>The JSON metadata is assembled during each recording loop iteration and written asynchronously to disk via a dedicated thread. This ensures that JSON saving does not block the main acquisition loop.</p> <p>Relevant code:</p> <pre><code>frame_metadata = {\"real_times\": real_time, \"local_times\": local_time, \"base_filename\": self.video_base_file}\n\nframe_metadata[\"timestamps\"] = []\nframe_metadata[\"frame_id\"] = []\nframe_metadata[\"frame_id_abs\"] = []\nframe_metadata[\"chunk_serial_data\"] = []\nframe_metadata[\"serial_msg\"] = []\nframe_metadata[\"camera_serials\"] = []\nframe_metadata[\"exposure_times\"] = []\nframe_metadata[\"frame_rates_requested\"] = []\nframe_metadata[\"frame_rates_binning\"] = []\n</code></pre> <p>Each frame's metadata is appended to this dictionary. After all cameras are processed for a single frame, the following call queues the metadata to be written:</p> <pre><code>self.json_queue.put(frame_metadata)\n</code></pre> <p>Later, a thread runs <code>write_metadata_queue(...)</code> to write this metadata to JSON file(s).</p>"},{"location":"understanding-data/understanding-json/#how-json-is-grouped-with-mp4-files","title":"How JSON is Grouped with MP4 Files","text":"<p>During recording, both video and JSON metadata files are saved in segments (or chunks) to prevent excessive memory usage and to maintain system stability over long recordings.</p> <p>For example, a 10-minute recording may result in multiple <code>.mp4</code> files per each camera and a corresponding <code>.json</code> metadata file:</p> <pre><code>-r--r-----+ 1 root datalake 441M Mar 18 11:30 YFMDatafile_20250318_110436.18486638.mp4\n-r--r-----+ 1 root datalake 323M Mar 18 11:30 YFMDatafile_20250318_110436.18486644.mp4\n-r--r-----+ 1 root datalake 216M Mar 18 11:30 YFMDatafile_20250318_110436.23505577.mp4\n-r--r-----+ 1 root datalake 717M Mar 18 11:30 YFMDatafile_20250318_110436.23512012.mp4\n-r--r-----+ 1 root datalake 233M Mar 18 11:30 YFMDatafile_20250318_110436.23512013.mp4\n-r--r-----+ 1 root datalake 232M Mar 18 11:30 YFMDatafile_20250318_110436.23512014.mp4\n-r--r-----+ 1 root datalake 362M Mar 18 11:30 YFMDatafile_20250318_110436.23512906.mp4\n-r--r-----+ 1 root datalake 623M Mar 18 11:30 YFMDatafile_20250318_110436.23512908.mp4\n-r--r-----+ 1 root datalake 9.4M Mar 18 11:30 YFMDatafile_20250318_110436.json\n</code></pre>"},{"location":"understanding-data/understanding-json/#filename-format","title":"Filename Format","text":"<p>The filenames follow the pattern:</p> <pre><code>&lt;base_name&gt;_&lt;YYYYMMDD_HHMMSS&gt;.&lt;camera_serial&gt;.mp4\n</code></pre> <ul> <li><code>base_name</code> is derived from the <code>recording_path</code> set at the beginning of acquisition.</li> <li><code>YYYYMMDD_HHMMSS</code> represents the date and time at which the segment began recording.</li> <li>Each camera serial is appended as a suffix to distinguish between files from multiple cameras.</li> <li>A single <code>.json</code> file is generated for each chunk and contains metadata for all cameras in that segment.</li> </ul> <p>The JSON filename does not have a camera serial suffix because it aggregates metadata across all cameras.</p>"},{"location":"understanding-data/understanding-json/#chunking-mechanism","title":"Chunking Mechanism","text":"<p>The system is configured to split files every N frames using the <code>video_segment_len</code> field from the camera config:</p> <pre><code>video_segment_len = self.camera_config[\"acquisition-settings\"][\"video_segment_len\"]\n</code></pre> <p>When this number of frames is reached:</p> <ul> <li>A new filename is generated using the current timestamp.</li> <li>New threads continue writing to the newly created files.</li> <li>JSON and video saving continue uninterrupted.</li> </ul> <pre><code>if frame_idx % total_frames == 0:\n    prog = tqdm(total=total_frames)\n    frame_idx = 0\n\n    if self.video_base_file is not None:\n        # Create a new video_base_filename for the new video segment\n        now = datetime.now()\n        time_str = now.strftime(\"%Y%m%d_%H%M%S\")\n\n        # Update base name and file path\n        self.video_base_name = \"_\".join([self.video_root, time_str])\n        self.video_base_file = os.path.join(self.video_path, self.video_base_name)\n</code></pre>"},{"location":"understanding-data/understanding-json/#how-json-is-structured","title":"How JSON is Structured","text":"<p>For each recorded frame (per camera), the following fields are recorded:</p> <ul> <li><code>timestamps</code>: Hardware timestamps from the camera.</li> <li><code>frame_id</code>: The local frame number (may reset per segment).</li> <li><code>frame_id_abs</code>: The absolute frame ID from chunk metadata.</li> <li><code>chunk_serial_data</code>: Frame counter reconstructed from 5-byte serial message (if <code>SerialOn</code> is enabled).</li> <li><code>serial_msg</code>: The raw 5-byte message received over serial.</li> <li><code>camera_serials</code>: Camera serial number (device ID).</li> <li><code>exposure_times</code>: Exposure time for each frame.</li> <li><code>frame_rates_requested</code>: The requested acquisition frame rate.</li> <li><code>frame_rates_binning</code>: The effective frame rate after binning.</li> </ul> <p>These fields are updated per camera inside a loop:</p> <pre><code>for c in self.cams:\n    im_ref = c.get_image()\n    timestamp = im_ref.GetTimeStamp()\n    chunk_data = im_ref.GetChunkData()\n    frame_id = im_ref.GetFrameID()\n    frame_id_abs = chunk_data.GetFrameID()\n\n    # Serial decoding\n    serial_msg = []\n    frame_count = -1\n    if self.gpio_settings['line3'] == 'SerialOn':\n        if c.ChunkSerialDataLength == 5:\n            chunk_serial_data = c.ChunkSerialData\n            serial_msg = chunk_serial_data\n            split_chunk = [ord(c) for c in chunk_serial_data]\n\n            frame_count = 0\n            for i, b in enumerate(split_chunk):\n                frame_count |= (b &amp; 0x7F) &lt;&lt; (7 * i)\n\n    frame_metadata[\"timestamps\"].append(timestamp)\n    frame_metadata[\"frame_id\"].append(frame_id)\n    frame_metadata[\"frame_id_abs\"].append(frame_id_abs)\n    frame_metadata[\"chunk_serial_data\"].append(frame_count)\n    frame_metadata[\"serial_msg\"].append(serial_msg)\n    frame_metadata[\"camera_serials\"].append(c.DeviceSerialNumber)\n    frame_metadata[\"exposure_times\"].append(c.ExposureTime)\n    frame_metadata[\"frame_rates_binning\"].append(c.BinningHorizontal * 30)\n    frame_metadata[\"frame_rates_requested\"].append(c.AcquisitionFrameRate)\n</code></pre> <p>This loop ensures that the metadata per frame and per camera is stored in a synchronized fashion and saved with high fidelity.</p>"},{"location":"understanding-data/understanding-nev/","title":"Understanding NEV","text":"<p>This guide provides a detailed explanation of the NEV (Neural Event) file structure, focusing on decoding digital event streams (<code>UnparsedData</code>) and aligning them with external triggers such as camera recordings. It is intended for researchers and engineers using Blackrock systems in multimodal experimental setups.</p>"},{"location":"understanding-data/understanding-nev/#nev-file-structure","title":"NEV File Structure","text":"<p>The <code>.nev</code> file format, developed by BlackRock Microsystems, is designed to store timestamped neural events such as spikes, TTL pulses, and serial messages. It supports up to 10,000 electrodes and includes both metadata and raw event data, structured to balance flexibility, efficiency, and ease of parsing.</p> <p>A NEV file consists of three main components:</p> <ul> <li>Basic Header</li> <li>Extended Headers</li> <li>Data Packets</li> </ul>"},{"location":"understanding-data/understanding-nev/#basic-header","title":"Basic Header","text":"<p>The basic header contains global metadata about the recording session, including the timestamp resolution and the origin of time. You can inspect it using the following method. Key fields:</p> <ul> <li>TimeStampResolution: Number of timestamp ticks per second (e.g., 30,000 means 1 tick = 33.3 \u03bcs).</li> <li>SampleTimeResolution: Used for waveform sampling resolution, often the same as TimeStampResolution.</li> <li>TimeOrigin: The UTC time when the recording started.</li> </ul> <pre><code>&gt;&gt;&gt; nev.get_basic_header()\n{'FileTypeID': 'BREVENTS',\n 'FileSpec': '3.0',\n 'AddFlags': 1,\n 'BytesInHeader': 26512,\n 'BytesInDataPackets': 108,\n 'TimeStampResolution': 30000,\n 'SampleTimeResolution': 30000,\n 'TimeOrigin': datetime.datetime(2024, 7, 17, 11, 55, 38, 670000),\n 'CreatingApplication': 'File Dialog v7.6.1',\n 'Comment': '',\n 'NumExtendedHeaders': 818}\n</code></pre>"},{"location":"understanding-data/understanding-nev/#extended-headers","title":"Extended Headers","text":"<p>Extended headers contain channel-specific metadata, such as labeling, filtering, and electrode properties. They are stored as a list of dictionaries. Each PacketID identifies the type of metadata (e.g., waveform parameters, labels, filter settings).</p> <pre><code>&gt;&gt;&gt; nev.get_extended_headers()\n[{'PacketID': 'NEUEVWAV',\n  'ElectrodeID': 1,\n  'PhysicalConnector': 1,\n  'ConnectorPin': 1,\n  'DigitizationFactor': 250,\n  'EnergyThreshold': 0,\n  'HighThreshold': 0,\n  'LowThreshold': -255,\n  'NumSortedUnits': 0,\n  'BytesPerWaveform': 2,\n  'SpikeWidthSamples': 48,\n  'EmptyBytes': b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'},\n {'PacketID': 'NEUEVLBL',\n  'ElectrodeID': 1,\n  'Label': 'LdPF-mPF01-001',\n  'EmptyBytes': b'\\x00\\x00\\x00\\x00\\x00\\x00'},\n {'PacketID': 'NEUEVFLT',\n  'ElectrodeID': 1,\n  'HighFreqCorner': '250.0 Hz',\n  'HighFreqOrder': 4,\n  'HighFreqType': 'butterworth',\n  'LowFreqCorner': '7500.0 Hz',\n  'LowFreqOrder': 3,\n  'LowFreqType': 'butterworth',\n  'EmptyBytes': b'\\x00\\x00'},\n  ...,\n]\n</code></pre>"},{"location":"understanding-data/understanding-nev/#data-packets","title":"Data Packets","text":"<p>The <code>.nev</code> file also contains the actual timestamped events, such as TTL pulses or encoded serial data.</p> <pre><code>nev = Nev(nev_path)\nnev_digital_events_df = nev.get_digital_events_df()\n</code></pre> InsertionReason TimeStamps UnparsedData 1 4.05746e+09 60414 1 4.05746e+09 60415 129 4.05746e+09 114 129 4.05746e+09 104 129 4.05746e+09 42 129 4.05746e+09 1 129 4.05746e+09 0 1 4.05746e+09 60414 1 4.05746e+09 60415 129 4.05746e+09 115 129 4.05746e+09 104 129 4.05746e+09 42 129 4.05746e+09 1 129 4.05746e+09 0 1 4.05746e+09 60414 1 4.05746e+09 60415 129 4.05746e+09 116 129 4.05746e+09 104 129 4.05746e+09 42 129 4.05746e+09 1 <p>Key columns:</p> <ul> <li>TimeStamps: Integer-based timestamps with TimeStampResolution frequency (e.g., 30,000 Hz).</li> <li>InsertionReason: An 8-bit flag indicating why the event was inserted (e.g., digital change, serial input).</li> <li>UnparsedData: The raw 16-bit digital input or 7-bit serial payload (depending on the flag).</li> </ul>"},{"location":"understanding-data/understanding-nev/#understanding-insertionreason","title":"Understanding <code>InsertionReason</code>","text":"<p>The <code>InsertionReason</code> field in the NEV file is a bitwise flag that encodes the reason a particular event was recorded. Each bit represents a specific condition or source for the event. Multiple bits may be set simultaneously.</p>"},{"location":"understanding-data/understanding-nev/#relevant-values","title":"Relevant Values","text":"Value Binary (8-bit) Meaning <code>1</code> <code>00000001</code> A digital channel changed state (e.g., camera trigger line toggled) <code>129</code> <code>10000001</code> A serial channel changed, and a digital change occurred (i.e., a serial byte was received)"},{"location":"understanding-data/understanding-nev/#bitwise-breakdown-from-blackrock-nev-specification","title":"Bitwise Breakdown (from BlackRock NEV Specification)","text":"<p>The <code>InsertionReason</code> byte is defined as follows:</p> Bit Meaning 0 Digital channel changed (e.g., rising/falling edge on a trigger line) 1 Event is from a strobed input 2\u20136 Reserved (unused) 7 Serial channel changed (must be set alongside Bit 0)"},{"location":"understanding-data/understanding-nev/#interpretation","title":"Interpretation","text":"<ul> <li><code>0b00000001</code> (decimal <code>1</code>) \u2192 Digital event only  </li> <li><code>0b10000001</code> (decimal <code>129</code>) \u2192 Serial event (with digital change)</li> </ul>"},{"location":"understanding-data/understanding-nev/#usage","title":"Usage","text":"<ul> <li>Use rows where <code>InsertionReason == 1</code> to extract digital trigger events (e.g., camera exposure).</li> <li>Use rows where <code>InsertionReason == 129</code> to reconstruct serial counter values sent from Arduino in 5-byte chunks.</li> </ul> <p>This distinction is crucial for synchronizing camera frames with external signals such as triggers or Arduino-based serial counters.</p>"},{"location":"understanding-data/understanding-nev/#understanding-unparseddata","title":"Understanding <code>UnparsedData</code>","text":""},{"location":"understanding-data/understanding-nev/#how-trigger-and-serial-are-sent-from-arduino","title":"How Trigger and Serial are sent from Arduino","text":"<p>The Arduino transmits both a trigger pulse and a serial-encoded counter value every frame:</p> <p>The trigger pulse is sent via a digital output pin (e.g., pin 13) to the camera to initiate image capture. Simultaneously, the counter value is incremented and transmitted over three serial ports \u2014 in the current setup, two going to the PCBs (used by the cameras) and one to the audio interface.</p> <p>To ensure compatibility with the camera's serial input requirements, the counter value (a 32-bit unsigned integer) is split into 5 separate bytes, with each byte containing 7 bits of actual data. This is done because:</p> <ul> <li>The camera expects each byte to begin with a start bit</li> <li>It can only receive one byte at a time, and only uses the lower 7 bits of each byte for data.</li> <li>Therefore, transmitting the full 32-bit value requires 5 bytes (since 5 \u00d7 7 = 35 bits), with the upper 3 bits simply unused if not needed.</li> </ul> <p>The splitting and transmission are handled by this Arduino function:</p> <pre><code>bool send_trigger_sync_to_pcb(void *) {\n  digitalWrite(trigger_pin, HIGH);\n\n  byte bytesToSend[5]; // Create an array to store the 5 bytes\n  // Split the 32-bit integer into 5 bytes, each carrying 7 bits\n  for (int i = 0; i &lt; 5; i++) {\n    // Shift right by 7 bits times the index and mask out the lower 7 bits\n    bytesToSend[i] = (count &gt;&gt; (7 * i)) &amp; 0x7F;\n  }\n  //Send each byte over serial\n  for (int i = 0; i &lt; 5; i++) {\n    Serial1.write(bytesToSend[i]);\n    Serial2.write(bytesToSend[i]);\n    Serial3.write(bytesToSend[i]);\n    Serial1.flush();\n    Serial2.flush();\n    Serial3.flush();\n  }\n  digitalWrite(trigger_pin, LOW);\n  count = count + 1;\n  return true;\n}\n</code></pre>"},{"location":"understanding-data/understanding-nev/#how-trigger-and-serial-are-encoded-in-unparseddata","title":"How Trigger and Serial are encoded in <code>UnparsedData</code>","text":"<p>The UnparsedData field in the NEV digital events dataframe holds different meanings depending on the value of the InsertionReason flag.</p> <p>For <code>InsertionReason == 1</code>, the <code>UnparsedData</code> value represents a 16-bit unsigned integer encoding the state of all 16 digital input lines. Each bit corresponds to a specific digital channel\u2014commonly used for camera exposure signals or TTL triggers. Since the NEV file only records changes in digital line state (i.e., rising or falling edges), the data is sparse in time. To reconstruct the square waveforms of digital signals (such as camera exposure or external triggers), the following steps are required:</p> <ul> <li>Filter digital events where <code>InsertionReason == 1</code>.</li> <li>Convert UnparsedData into a 16-bit binary representation.</li> <li>Fill in gaps between timestamps to create a continuous time axis, since intermediate bit states are not recorded.</li> <li>Extract the state of the specific bit corresponding to the channel of interest.</li> <li>Plot the decoded signal as a square wave.</li> </ul> <p>This process is encapsulated in the <code>nev.plot_cam_exposure_all</code> function, which generates clear, continuous exposure traces:</p> <p></p> <p>When <code>InsertionReason</code> is <code>129</code>, the UnparsedData value represents a 7-bit payload from serial communication. The Arduino transmits a 32-bit integer over serial by encoding it into five 7-bit bytes, each stored as a separate event with <code>InsertionReason == 129</code>.</p> <p>To decode the full serial message:</p> <ul> <li>Group every 5 consecutive rows with <code>InsertionReason == 129</code>.</li> <li>Extract the 7-bit data values from UnparsedData.</li> <li>Reconstruct the original 32-bit integer by reversing the bit-shifting process.</li> </ul> <p>Each such 5-row group represents a complete serial counter value synchronized with a trigger pulse. This data can then be used for precise alignment between camera frames and neural events.</p>"},{"location":"understanding-data/understanding-nev/#how-serial-number-is-reconstructed-from-5-bytes","title":"How Serial Number is reconstructed from 5 Bytes","text":"<p>In this system, a 32-bit integer counter is transmitted from the Arduino in the form of 5 bytes, where each byte encodes 7 bits of actual data. Both the camera system and the BlackRock recording system receive this data and must reconstruct the original 32-bit counter value.</p>"},{"location":"understanding-data/understanding-nev/#from-the-camera-side","title":"From the Camera Side","text":"<p>In the camera recording software, each byte is masked with 0x7F to retain only the lower 7 bits. The bytes are then shifted and combined to reconstruct the full 32-bit counter value.</p> <pre><code>def process_serial_data(self, c):\n    serial_msg = []\n    frame_count = -1\n    if self.gpio_settings['line3'] == 'SerialOn':\n        # We expect only 5 bytes to be sent\n        if c.ChunkSerialDataLength == 5:\n            chunk_serial_data = c.ChunkSerialData\n            serial_msg = chunk_serial_data\n            split_chunk = [ord(c) for c in chunk_serial_data]\n\n            frame_count = 0\n            for i, b in enumerate(split_chunk):\n                frame_count |= (b &amp; 0x7F) &lt;&lt; (7 * i)\n</code></pre>"},{"location":"understanding-data/understanding-nev/#from-the-nev-blackrock-side","title":"From the NEV (BlackRock) Side","text":"<p>On the BlackRock system, serial data is captured within the <code>.nev</code> file and appears in rows where <code>InsertionReason == 129</code>. Each complete serial transmission occupies 5 consecutive rows in the event stream.</p> <p>The NEV processing workflow involves:</p> <ul> <li>Filtering valid groups of 5 rows.</li> <li>Extracting the UnparsedData field from each row.</li> <li>Converting the 5 \u00d7 7-bit chunks back into a single integer.</li> <li>Apply fix-anomaly scripts to fill in the gaps of the serial stream (Refer to edge cases documentation).</li> </ul> <pre><code>def get_cleaned_digital_events_df(self):\n    \"\"\"\n    only keep the rows which satisfy\n    1. InsertionReason == 129\n    2. the length of such group is 5\n    3. 0 &lt;= UnparsedData &lt;= 127 (should be true enforced by hardware)\n\n    Returns\n        InsertionReason     TimeStamps  UnparsedData\n    2   129                 1345819     40\n    3   129                 1345822     76\n    4   129                 1345825     35\n    5   129                 1345828     0\n    6   129                 1345831     0\n    \"\"\"\n    digital_events_df = self.get_digital_events_df()\n    # True indicates a change from 1 -&gt; 129 or 129 -&gt; 1\n    digital_events_df[\"group\"] = (\n        digital_events_df[\"InsertionReason\"]\n        != digital_events_df[\"InsertionReason\"].shift(1)\n    ).cumsum()\n    # Count the size of each group and assign True where the group size\n    # is 5 and the reason is 129\n    digital_events_df[\"keeprows\"] = digital_events_df.groupby(\"group\")[\n        \"InsertionReason\"\n    ].transform(lambda x: (x == 129) &amp; (x.size == 5))\n    digital_events_df = digital_events_df[digital_events_df[\"keeprows\"] == True]\n    digital_events_df = digital_events_df.drop([\"group\", \"keeprows\"], axis=1)\n    return digital_events_df\n\ndef bits_to_decimal(self, nums: list) -&gt; int:\n    \"\"\"\n    nums: [19, 101, 37, 0, 0]\n\n    Returns:\n    619155\n    \"\"\"\n    # Convert each number to a 7-bit binary string with leading zeros\n    binary_strings = [format(num, \"07b\") for num in nums][::-1]\n    # Concatenate all binary strings into one long binary string\n    full_binary_string = \"\".join(binary_strings)\n    # Convert the concatenated binary string to a decimal number\n    return int(full_binary_string, 2)\n\ndef get_chunk_serial_df(self, timestamp_byte: str = \"first\"):\n    \"\"\"Reconstruct chunk serial numbers from grouped digital events.\n\n    Processes the cleaned digital events DataFrame by grouping every five consecutive rows,\n    reconstructing each chunk serial number from the grouped 7-bit encoded values, and\n    associating it with a corresponding timestamp. The timestamp used for each group\n    can be explicitly selected as either the first or last byte in the group.\n\n    Args:\n        timestamp_byte (str, optional): Which byte's timestamp to use ('first' or 'last').\n            Defaults to 'first'. Use 'last' if you want the timestamp representing\n            the full completion of the serial transmission (recommended for accurate synchronization).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing:\n            - `TimeStamps`: Timestamp from the NEV data (based on selected byte).\n            - `chunk_serial`: Reconstructed chunk serial number.\n            - `UTCTimeStamp`: Human-readable UTC timestamp.\n\n    Raises:\n        AssertionError: If unparsed data is unavailable or timestamp_byte parameter is invalid.\n\n    Example:\n        &gt;&gt;&gt; nev.get_chunk_serial_df(timestamp_byte='last')\n                TimeStamps  chunk_serial              UTCTimeStamp\n        0         1345819       583208  2024-04-16 21:48:17.195433\n        1         1346821       583209  2024-04-16 21:48:17.228833\n    \"\"\"\n    assert self.has_unparsed_data(), \"No unparsed data available.\"\n    assert timestamp_byte in [\n        \"first\",\n        \"last\",\n    ], \"timestamp_byte must be either 'first' or 'last'\"\n\n    df = self.get_cleaned_digital_events_df()\n    results = []\n\n    for i in range(0, len(df), 5):\n        group = df.iloc[i : i + 5]\n        if len(group) == 5:\n            nums = group[\"UnparsedData\"].tolist()\n            decimal_number = self.bits_to_decimal(nums)\n\n            # explicitly choose which byte's timestamp to use\n            if timestamp_byte == \"first\":\n                timestamp = group[\"TimeStamps\"].iloc[0]\n            else:  # timestamp_byte == 'last'\n                timestamp = group[\"TimeStamps\"].iloc[-1]\n\n            unix_time = ts2unix(\n                self.timeOrigin, self.timestampResolution, timestamp\n            )\n            results.append((timestamp, decimal_number, unix_time))\n\n    # Explicitly fill missing serials if necessary\n    results = fill_missing_serials_with_gap(results)\n\n    return pd.DataFrame.from_records(\n        results, columns=[\"TimeStamps\", \"chunk_serial\", \"UTCTimeStamp\"]\n    )\n</code></pre>"},{"location":"understanding-data/understanding-nev/#timing-between-trigger-and-serial","title":"Timing Between Trigger and Serial","text":"<p>According to the Arduino code, the trigger pulse is initiated immediately before the serial data transmission begins:</p> <pre><code>bool send_trigger_sync_to_pcb(void *) {\n  digitalWrite(trigger_pin, HIGH);\n\n  // code to sent serial...\n\n  digitalWrite(trigger_pin, LOW);\n  count = count + 1;\n  return true;\n}\n</code></pre> <p>When observing the NEV recording, we find that the trigger pulse is captured exactly one timestamp before the first byte of the 5-byte serial transmission. This confirms the ordering and tight timing between the two signals. This relationship is visualized in the following figure:</p> <p></p> <p>This precise sequencing is essential for synchronizing camera frames with neural recordings. The reliable 1-timestamp offset can be leveraged during analysis to align data streams accurately.</p>"},{"location":"understanding-data/understanding-ns5/","title":"Understanding NS5","text":"<p>BlackRock NSx files (<code>.ns1</code> through <code>.ns9</code>) contain continuously sampled data from electrophysiological recordings, such as raw voltage traces, local field potentials (LFP), or EMG signals. The number <code>x</code> in the file extension corresponds to the sampling rate tier \u2014 for example, <code>.ns1</code> might store lower-rate LFP data (e.g., 500 Hz), while <code>.ns5</code> typically stores high-resolution raw data sampled at 30,000 Hz.</p> <p>These files are written in a time non-decreasing manner and are typically paired with a <code>.nev</code> file that contains spike events and digital triggers. For instance, the files <code>experiment.nev</code> and <code>experiment.ns5</code> together represent the full set of spike events and high-resolution continuous signals for a recording session.</p> <p>It is important to note that: - An NSx file can exist independently of a NEV file, and vice versa. - Multiple NSx files (e.g., <code>.ns2</code>, <code>.ns6</code>) may exist in the same recording session if continuous sampling was performed at multiple rates. - The <code>.ns5</code> file is most commonly used when raw neural data is acquired at the highest available sampling rate (30 kHz).</p> <p>This section provides guidance on how to interpret and process <code>.ns5</code> files, including reading the header structure, decoding the data blocks, and aligning NS5 signals with NEV-based events like triggers or serial pulses.</p>"},{"location":"understanding-data/understanding-ns5/#ns5-file-structure","title":"NS5 File Structure","text":"<p>Ns5 is...</p> <p>A Ns5 file consists of three main components:</p> <ul> <li>Basic Header</li> <li>Extended Headers</li> <li>Data Packets</li> </ul> <p>You can load a <code>.ns5</code> file by</p> <pre><code>from pyvideosync.nsx import Nsx\nns5_path = \"/path/to/ns5\"\nns5 = Nsx(ns5_path)\n</code></pre>"},{"location":"understanding-data/understanding-ns5/#basic-header","title":"Basic Header","text":"<p>The Basic Header provides essential metadata about the <code>.ns5</code> file and its structure. It is always located at the beginning of the file and defines how to interpret the rest of the data. All multi-byte values are stored in little-endian format, and character arrays may not be null-terminated unless they are shorter than the maximum allowed length.</p> <p>Below is an example output of the parsed NS5 basic header using the <code>get_basic_header()</code> method:</p> <pre><code>&gt;&gt;&gt; ns5.get_basic_header()\n{'FileTypeID': 'BRSMPGRP',\n 'SampleResolution': 30000,\n 'FileSpec': '3.0',\n 'BytesInHeader': 710,\n 'Label': '30 kS/s',\n 'Comment': '',\n 'Period': 1,\n 'TimeStampResolution': 30000,\n 'TimeOrigin': datetime.datetime(2024, 7, 17, 13, 35, 39, 30000),\n 'ChannelCount': 6}\n</code></pre> <p>Explanation of fields</p> Field Description <code>FileTypeID</code> Always set to <code>\"BRSMPGRP\"</code>, indicating a Neural Continuous Data file. Earlier versions may use <code>\"NEURALSG\"</code> or <code>\"NEURALCD\"</code>. <code>FileSpec</code> File specification version, represented as major and minor (e.g., <code>'3.0'</code> corresponds to version 3.0). <code>BytesInHeader</code> Total number of bytes in the combined standard and extended headers. Also serves as the byte offset to the first data packet. <code>Label</code> A human-readable label for the sampling group (e.g., <code>\"30 kS/s\"</code> or <code>\"LFP Low\"</code>). <code>Comment</code> Optional descriptive comment about the file or recording session. <code>Period</code> Number of 1/30,000 second intervals between data points. A value of <code>1</code> implies a 30,000 Hz sampling rate. A value of <code>3</code> would indicate 10,000 Hz. <code>TimeStampResolution</code> Frequency of the global time base (in counts per second). Typically <code>30,000</code>, indicating that each timestamp unit equals 1/30,000 seconds. <code>TimeOrigin</code> The UTC time at which the recording began. This is considered time zero for the timestamp values. <code>ChannelCount</code> Total number of continuously sampled channels in this file. This should match the number of extended headers that follow."},{"location":"understanding-data/understanding-ns5/#extended-headers","title":"Extended Headers","text":"<p>The NS5 file includes one extended header per continuously sampled channel. These headers describe how each electrode channel was configured during acquisition. The number of extended headers corresponds to the ChannelCount specified in the basic header.</p> <p>The standard format for each extended header includes:</p> Field Description <code>Type</code> Always set to \"CC\" (Continuous Channel), indicating this is an extended header for continuous data. <code>ElectrodeID</code> Unique identifier for the electrode. This ID matches the electrode numbers in the corresponding NEV file. <code>ElectrodeLabel</code> A human-readable label or name for the electrode (e.g., \"elec1\"). <code>PhysicalConnector</code> The physical bank or system connector to which the electrode is attached. For example: 1 = Bank A, 2 = Bank B, etc. <code>ConnectorPin</code> The specific pin number on the connector where the electrode is connected (typically 1\u201337). <code>MinDigitalValue</code> Minimum digital value of the signal (e.g., -8192). <code>MaxDigitalValue</code> Maximum digital value of the signal (e.g., 8192). <code>MinAnalogValue</code> Minimum analog value after scaling, typically in microvolts or millivolts (e.g., -5000). <code>MaxAnalogValue</code> Maximum analog value after scaling (e.g., 5000). <code>Units</code> Unit of measurement for the analog values (e.g., \"\u03bcV\" or \"mV\"). <code>HighFreqCorner</code> The upper cutoff frequency of the signal filter in millihertz. <code>HighFreqOrder</code> Order of the high-pass filter applied (e.g., 4 for a 4th-order filter). <code>HighFilterType</code> Type of high-pass filter used. 0 = NONE, 1 = Butterworth. <code>LowFreqCorner</code> The lower cutoff frequency of the signal filter in millihertz. <code>LowFreqOrder</code> Order of the low-pass filter applied. <code>LowFilterType</code> Type of low-pass filter used. 0 = NONE, 1 = Butterworth. <p>Each of these entries helps interpret the scale and filtering applied to the raw neural signals stored in the NS5 file.</p> <p>You can also render the extended headers using:</p> <pre><code>&gt;&gt;&gt; ns5.get_extended_headers_df()\n</code></pre> Type ElectrodeID ElectrodeLabel PhysicalConnector ConnectorPin MinDigitalValue MaxDigitalValue MinAnalogValue MaxAnalogValue Units HighFreqCorner HighFreqOrder HighFreqType LowFreqCorner LowFreqOrder LowFreqType CC 257 Photodiode 9 1 -32767 32767 -5000 5000 mV 0.0 Hz 0 none 0.0 Hz 0 none CC 258 Audio 9 2 -32767 32767 -5000 5000 mV 0.0 Hz 0 none 0.0 Hz 0 none CC 259 RoomMic1 9 3 -32767 32767 -5000 5000 mV 0.0 Hz 0 none 0.0 Hz 0 none CC 260 StimSync 9 4 -32767 32767 -5000 5000 mV 0.0 Hz 0 none 0.0 Hz 0 none CC 261 GSR 9 5 -32767 32767 -5000 5000 mV 0.0 Hz 0 none 0.0 Hz 0 none CC 262 LegaMemorySync 9 6 -32767 32767 -5000 5000 mV 0.0 Hz 0 none 0.0 Hz 0 none"},{"location":"understanding-data/understanding-ns5/#data-packets","title":"Data Packets","text":"<p>The third section of an NS5 file contains the actual recorded data: time-stamped packets of continuously sampled neural signals. Each data packet begins with a small header, followed by a timestamp, the number of data points, and the data values themselves.</p> <p>According to Blackrock\u2019s NSx file specification (Rev 7.00, Page 18):</p> Field Type Length (Bytes) Description <code>Header</code> <code>Byte</code> 1 Always set to <code>0x01</code> to indicate the start of a data packet. <code>Timestamp</code> <code>Unsigned int64</code> 8 The global timestamp for when this block of data begins. The unit is determined by <code>TimeStampResolution</code> in the basic header. <code>NumDataPoints</code> <code>Unsigned int32</code> 4 The number of data points that follow this header. <code>DataPoints</code> <code>Array of int16</code> Variable The sampled voltage values. There are <code>ChannelCount</code> values for each time point, one per channel, repeated for <code>NumDataPoints</code>. <p>All data points are stored in channel-major order (i.e., samples from all channels at the same time step are grouped together), and the timestamps always increase monotonically.</p> <p>These packets can also be used to indicate acquisition gaps or pauses in recording \u2014 if multiple packets appear, it's usually because the file was paused or segmented.</p> <p>You can access the continuous data using the <code>get_data()</code> method:</p> <pre><code>{\n  'start_time_s': 0.0,\n  'data_time_s': 'all',\n  'downsample': 1,\n  'elec_ids': [257, 258, 259, 260, 261, 262],\n  'data_headers': [{\n      'Timestamp': 4057455182,\n      'NumDataPoints': 38332687,\n      'data_time_s': 1277.756\n  }],\n  'data': [memmap(..., dtype=int16)],\n  'samp_per_s': 30000.0\n}\n</code></pre> <ul> <li>start_time_s: Time in seconds corresponding to the first timestamp (usually 0).</li> <li>elec_ids: List of electrode (channel) IDs in the order of the data.</li> <li>data_headers: Contains metadata like the starting timestamp and the number of data points in each segment.</li> <li>data: A NumPy <code>memmap</code> array representing the signal data. Each row corresponds to a different channel.</li> <li>samp_per_s: The sampling rate, typically 30,000 Hz.</li> </ul>"},{"location":"understanding-data/understanding-ns5/#how-audioneuro-data-is-encoded","title":"How Audio/Neuro Data is Encoded","text":"<p>In the NS5 file format, audio and neural data are stored as continuous analog signals recorded from multiple channels at high sampling rates (e.g., 30,000 Hz). These signals are organized in data packets, each containing:</p> <ul> <li>A global timestamp indicating when the data block begins.</li> <li>A fixed number of data points sampled uniformly over time.</li> <li>One sample per channel per timestamp, stored as 16-bit signed integers.</li> </ul> <p>This layout ensures accurate alignment between channels and enables consistent decoding of the analog waveform over time.</p> <p>Each channel (e.g., a microphone or electrode) has metadata stored in the extended headers, such as the channel label, pin number, analog range, and filter settings.</p> <p>To access the raw signal from a particular channel, the following function retrieves the corresponding row in the <code>memmap</code> array:</p> <pre><code>def get_channel_array(self, channel: str):\n    \"\"\"\n    Retrieve the raw analog signal array from a specific channel.\n\n    Args:\n        channel (str): The electrode or microphone label (e.g., \"RoomMic2\").\n\n    Returns:\n        np.ndarray: 1D array of raw amplitude values.\n    \"\"\"\n    row_index = self.extended_headers_df[\n        self.extended_headers_df[\"ElectrodeLabel\"] == channel\n    ].index.item()\n    return self.memmapData[row_index]\n</code></pre> <p>To convert the raw array into a time-aligned DataFrame:</p> <pre><code>def get_channel_df(self, channel: str):\n    \"\"\"\n    Construct a DataFrame for a specific channel with timestamp and amplitude.\n\n    Returns:\n        pd.DataFrame: DataFrame containing:\n            - TimeStamp: Sample index based on global timestamp.\n            - Amplitude: Raw signal values (int16).\n            - UTCTimeStamp: Human-readable UTC timestamp.\n    \"\"\"\n    channel_data = self.get_channel_array(channel)\n    num_samples = len(channel_data)\n    channel_df = pd.DataFrame(channel_data, columns=[\"Amplitude\"])\n    channel_df[\"TimeStamp\"] = np.arange(\n        self.timeStamp, self.timeStamp + num_samples\n    )\n    channel_df[\"UTCTimeStamp\"] = channel_df[\"TimeStamp\"].apply(\n        lambda x: utils.ts2unix(self.timeOrigin, self.timestampResolution, x)\n    )\n    return channel_df[[\"TimeStamp\", \"Amplitude\", \"UTCTimeStamp\"]]\n</code></pre> <p>Each sample in the array is aligned using:</p> <ul> <li><code>self.timeStamp</code>: The timestamp of the first recorded data point.</li> <li><code>self.timeOrigin</code>: The actual UTC time corresponding to timestamp 0.</li> <li><code>self.timestampResolution</code>: The number of counts per second (usually 30,000).</li> </ul> <p>This ensures that the amplitude at each point is properly aligned in real time, allowing precise synchronization with other data streams such as video or triggers.</p>"},{"location":"understanding-data/understanding-ns5/#how-to-decode-audio","title":"How to Decode Audio","text":"<p>To export microphone data from NS5 to a <code>.wav</code> file:</p> <ol> <li> <p>Extract the audio channel: <pre><code>df_sub = ns5.get_channel_df(\"RoomMic2\")\naudio_samples = df_sub[\"Amplitude\"].values.astype(np.int16)\n</code></pre></p> </li> <li> <p>Compute dynamic sample rate: Although the NS5 file uses a nominal timestamp resolution of 30,000 Hz, the actual spacing between video frames (based on <code>frame_id</code> vs. <code>TimeStamp</code>) is not exactly 1000 ticks per frame. This small variation (e.g., 998, 1001) accumulates over time and can lead to noticeable drift. To ensure audio and video remain aligned, we compute the true sample rate dynamically: <pre><code>exported_video_duration_s = (max_frame - min_frame + 1) / 30  # 30 fps\nfps_audio = int(len(audio_samples) / exported_video_duration_s)\n</code></pre></p> </li> <li> <p>Export to WAV: <pre><code>from scipy.io.wavfile import write as wav_write\nwav_write(audio_wav_path, fps_audio, audio_samples)\n</code></pre></p> </li> </ol> <p>This ensures the audio duration matches the actual video length, avoiding sync issues due to small timing deviations in frame acquisition.</p>"}]}